{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc",
   "metadata": {
    "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc"
   },
   "source": [
    "# Deep Reinforcement Learning Laboratory\n",
    "\n",
    "This laboratory is focused on getting more advanced versions of Deep Reinforcement Learning algorithms up and running. Deep Reinforcement Learning is **hard**, and getting agents to stably train can be frustrating and requires quite a bit of subtlety in analysis of intermediate results. We will start by implementing `REINFORCE` on the [Cartpole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uldpcHx0OHyx",
   "metadata": {
    "id": "uldpcHx0OHyx"
   },
   "source": [
    "## Imports and weights and biases login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9689f766-b099-4948-a1b2-37e92975178f",
   "metadata": {
    "id": "9689f766-b099-4948-a1b2-37e92975178f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Using weights and biases\n",
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241e9bdc-7aa9-4a12-a57f-088fdc87eab6",
   "metadata": {
    "id": "241e9bdc-7aa9-4a12-a57f-088fdc87eab6"
   },
   "outputs": [],
   "source": [
    "# Standard imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Plus one non standard one -- we need this to sample from policies.\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e806e9d4-bb04-40ed-9cd4-f016823b100c",
   "metadata": {
    "id": "e806e9d4-bb04-40ed-9cd4-f016823b100c",
    "outputId": "753a212b-355e-4133-f7e1-53e9b5895040"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarco-chisci\u001b[0m (\u001b[33mmarcouni\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login to weights and biases account\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f",
   "metadata": {
    "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f"
   },
   "source": [
    "# Exercise 1: `REINFORCE` Implementation (warm up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dk0EFuXUOpXz",
   "metadata": {
    "id": "Dk0EFuXUOpXz"
   },
   "source": [
    "## Policy Net\n",
    "A simple policy network with one hidden layer and a temperature parameter to smooth the output (the higher the temperature the smoother the output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ea9416-b07e-4354-bf36-1150394b0698",
   "metadata": {
    "id": "16ea9416-b07e-4354-bf36-1150394b0698"
   },
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, env, hidden_layers=32, temperature=1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], hidden_layers)\n",
    "        self.fc2 = nn.Linear(hidden_layers, env.action_space.n)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = F.softmax(self.fc2(s)/self.temperature, dim=-1)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uvEAh3bgOWTz",
   "metadata": {
    "id": "uvEAh3bgOWTz"
   },
   "source": [
    "## Episode Runner\n",
    "Class that runs the training episodes collecting all the useful info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd2dd9d-a60b-4892-878f-d83f64ac5f63",
   "metadata": {
    "id": "cbd2dd9d-a60b-4892-878f-d83f64ac5f63",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A class that , given an environment, a policy network and the max lenght of the episode is in charge\n",
    "# of running it\n",
    "class Episode_runner:\n",
    "    def __init__(self, env, policy, maxlen=500):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.maxlen= maxlen\n",
    "\n",
    "    # Given observation and the policy, sample from pi(a | obs). Returns the\n",
    "    # selected action and the log probability of that action (needed for policy gradient).\n",
    "    def select_action(self, obs):\n",
    "        dist = Categorical(self.policy(obs))\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return (action.item(), log_prob.reshape(1))\n",
    "\n",
    "    # Given the environment and the policy, run it up to the maximum number of steps.\n",
    "    def run_episode(self):\n",
    "        # Collect just about everything.\n",
    "        observations = []\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "\n",
    "        # Reset the environment and start the episode.\n",
    "        (obs, info) = self.env.reset() \n",
    "        for i in range(self.maxlen):\n",
    "            # Get the current observation, run the policy and select an action.\n",
    "            obs = torch.tensor(obs)\n",
    "            (action, log_prob) = self.select_action(obs)\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "            # Advance the episode by executing the selected action.\n",
    "            (obs, reward, term, trunc, info) = self.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if term or trunc:\n",
    "                break\n",
    "        return (observations, actions, torch.cat(log_probs), rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc5787-1c38-4f5c-b716-e40e900d00d2",
   "metadata": {},
   "source": [
    "## Deterministic Episode Runner\n",
    "Class that runs the testing episodes. The main difference with the training Episode Runner is that the action selected is always the one with the maximum probability, hence why it's called deterministic.\n",
    "\n",
    "To test the current best policy, the total average reward and episode length is calculated using the best policy in a deterministic manner on test_episodes episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f505e44c-0d5e-48f1-b874-d318424b3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class that , given an episode runner (with an environment and a policy network), an episode render to show the policy \n",
    "# and the max lenght of each episode evaluates the quality of the learned policy network, \n",
    "# always selecting the action with max probability \n",
    "class Determinist_Test_Episode_runner:\n",
    "    def __init__(self, episode_runner, episode_runner_render, maxlen=500):\n",
    "        self.ep_runner = episode_runner\n",
    "        self.ep_run_render = episode_runner_render\n",
    "        self.maxlen= maxlen\n",
    "\n",
    "    #select the most probable action given the policy and current observation\n",
    "    def select_action(self, obs):\n",
    "        dist = Categorical(self.ep_runner.policy(obs))\n",
    "        action= torch.argmax(dist.log_prob(dist.enumerate_support()))\n",
    "        return action.item()\n",
    "\n",
    "    # Given the environment and the policy, run it up to the maximum number of steps \n",
    "    def run_episode(self):\n",
    "        # Collect just about everything.\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        # Reset the environment and start the episode.\n",
    "        (obs, info) = self.ep_runner.env.reset() \n",
    "        for i in range(self.maxlen):\n",
    "            # Get the current observation, run the policy and select an action.\n",
    "            obs = torch.tensor(obs)\n",
    "            action = self.select_action(obs)\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "\n",
    "            # Advance the episode by executing the selected action.\n",
    "            (obs, reward, term, trunc, info) = self.ep_runner.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if term or trunc:\n",
    "                break\n",
    "        return (observations, actions, rewards)\n",
    "\n",
    "    def test(self, test_episodes):\n",
    "        print('Testing the best policy')\n",
    "        self.ep_runner.policy.eval()\n",
    "        total_reward = 0\n",
    "        episode_lengths = []\n",
    "        for _ in range(test_episodes):\n",
    "            (_, _, rewards) = self.run_episode()\n",
    "            total_reward += np.sum(rewards)\n",
    "            episode_lengths.append(len(rewards))\n",
    "        test_average_episode_len_metric = {\"test_average_episode_length\": np.mean(episode_lengths)}\n",
    "        test_average_rewards_metric = {\"test_average_total_reward\": total_reward / test_episodes}\n",
    "        wandb.log({**test_average_rewards_metric, **test_average_episode_len_metric})\n",
    "\n",
    "        (obs, _, _, _) = self.ep_run_render.run_episode()\n",
    "        self.ep_runner.policy.train()\n",
    "        print(f'Average Total reward: {total_reward / test_episodes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BhypW9ySOthT",
   "metadata": {
    "id": "BhypW9ySOthT"
   },
   "source": [
    "## Reinforce\n",
    "Implementation of REINFORCE policy gradient algorithm.\n",
    "\n",
    "It receives two \"training\" episode runners, one to train the policy and one to show the rendering of some episodes if display = True to see the progress.\n",
    "The training parameters are:\n",
    "- gamma: used to compute discounted total reward, balances the importance of immediate versus future reward. A higher value gives a higher weight to future rewards while a lower value prioritizes immediate rewards.\n",
    "- baseline: the type of baseline used to compute the value function. None (no baseline), std (standard baseline, standardize rewards within the episode) or a ValueNet (the value function is approximated by a Neural Network with the same architecture as the policy network)\n",
    "- num_episodes: number of training episodes to train the policy and baseline net\n",
    "- eval_every: after how many training episodes the policy is evaluated\n",
    "- eval_episodes: the number of episodes the policy is evaluated on\n",
    "- lr and lr_baseline: learning rates of the policy and baseline net\n",
    "\n",
    "Given all these inputs, for each episode, the discounted reward, running reward and episode length (in the cartpole case the episode length is equal to the reward of that episode) are computed. If there is a baseline, the policy loss, that is used to optimize the policy, is calculated with the target value. If the baseline is a Neural Network it is also optimized using its loss (how closely it approximates the value function).\n",
    "\n",
    "After eval_every episodes of training the policy is evaluated on eval_episodes episodes. Instead of using the running average the function calculates the average total reward of the episodes that were runned and their average length. As previously said, these two quantities are the same in Cartpole since the reward of each episode is its length. During this evaluation the best model (the one with the best average reward) is saved.\n",
    "\n",
    "I also chose to collect all the episodes lengths and losses, every 10 episodes are aggregated in a single value (the mean) to create graphs that are easier to read.\n",
    "\n",
    "Lastly, the function calculates the average episode length of the entire training to show a general trend, it can be used to investigate the stability of a training and how fast it is able to converge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a50f666-126a-4a16-9cbc-d9c846a1c3a7",
   "metadata": {
    "id": "2a50f666-126a-4a16-9cbc-d9c846a1c3a7"
   },
   "outputs": [],
   "source": [
    "# Utility to compute the discounted total reward. Torch doesn't like flipped arrays, so we need to\n",
    "# .copy() the final numpy array.\n",
    "def compute_returns(rewards, gamma):\n",
    "    return np.flip(np.cumsum([gamma**(i+1)*r for (i, r) in enumerate(rewards)][::-1]), 0).copy()\n",
    "\n",
    "# Implementation of the REINFORCE policy gradient algorithm.\n",
    "# It receives the episode runner, the wandb run to save the results, the episode runner render that is used to monitor training \n",
    "# when display = True, the gamma parameter, the number of episodes to train the policy and baseline net, the type of baseline used,\n",
    "# eval_every (after how many training steps we evaluate the policy), eval_episode (how many episodes we \n",
    "# evaluate the policy on) and the learning rates\n",
    "def reinforce(episode_runner, wandb, episode_runner_render=None, gamma=0.99, num_episodes=2000,\n",
    "              baseline=None, display=False, eval_every=100, eval_episodes=100, lr= 1e-2, lr_baseline = 1e-3 ):\n",
    "    # We use Adam instead of SGD.\n",
    "    opt = torch.optim.Adam(episode_runner.policy.parameters(), lr= lr)\n",
    "\n",
    "    # If we have a baseline network, create the optimizer.\n",
    "    if isinstance(baseline, nn.Module):\n",
    "        opt_baseline = torch.optim.Adam(baseline.parameters(), lr= lr_baseline)  \n",
    "        baseline.train()\n",
    "        print('Training agent with baseline value network.')\n",
    "    elif baseline == 'std':\n",
    "        print('Training agent with standardization baseline.')\n",
    "    else:\n",
    "        print('Training agent with no baseline.')\n",
    "\n",
    "    #Collect running rewards, all the episodes lengths and training loss\n",
    "    running_rewards = [0.0]\n",
    "    all_episodes_lenghts = []\n",
    "    training_losses = []\n",
    "    value_losses = []\n",
    "\n",
    "    #save the latest policy with the greatest average totale reward\n",
    "    best_model_state_dict = None\n",
    "    best_avg_tot_rew = 0\n",
    "    \n",
    "    # The main training loop.\n",
    "    episode_runner.policy.train()\n",
    "    for episode in range(1, num_episodes+1):\n",
    "        # Run an episode of the environment, collect everything needed for policy update.\n",
    "        (observations, actions, log_probs, rewards) = episode_runner.run_episode()\n",
    "\n",
    "        # Compute the discounted reward for every step of the episode.\n",
    "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
    "        # Keep a running average of total discounted rewards for the whole episode.\n",
    "        running_rewards.append(0.05 * returns[0].item() + 0.95 * running_rewards[-1])\n",
    "        running_rewards_metric = {\"running_reward\": running_rewards[-1]}\n",
    "\n",
    "        # Handle baseline.\n",
    "        if isinstance(baseline, nn.Module):\n",
    "            with torch.no_grad():\n",
    "                target = returns - baseline(torch.stack(observations))\n",
    "        elif baseline == 'std':                                       #Standardize returns\n",
    "            target = (returns - returns.mean()) / returns.std()\n",
    "        else:\n",
    "            target = returns\n",
    "\n",
    "        # Make an optimization step\n",
    "        opt.zero_grad()\n",
    "        loss = (-log_probs * target).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Collect each episode length and training loss\n",
    "        all_episodes_lenghts.append(len(returns))\n",
    "        training_losses.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "        # Log only the mean training loss, episode lenght and running reward of 10 episode to make the graphs cleaner\n",
    "        if episode % 10 == 0:\n",
    "            loss_policy_metric = {\"loss_policy\": np.mean(training_losses[-10:])}\n",
    "            episode_length_metric = {\"episode_length\": np.mean(all_episodes_lenghts[-10:])}\n",
    "            wandb.log({**loss_policy_metric, **episode_length_metric}, commit = False)\n",
    "\n",
    "        # Update baseline network.\n",
    "        if isinstance(baseline, nn.Module):\n",
    "            opt_baseline.zero_grad()\n",
    "            loss_baseline = ((returns - baseline(torch.stack(observations)))**2.0).mean()\n",
    "            loss_baseline.backward()\n",
    "            opt_baseline.step()\n",
    "            value_losses.append(loss_baseline.detach().cpu().numpy())\n",
    "            if episode % 10 == 0:\n",
    "                loss_value_metric = {\"loss_value\": np.mean(value_losses[-10:])}\n",
    "                wandb.log({**loss_value_metric}, commit = False)\n",
    "\n",
    "        # Render and evaluate the current policy after every \"eval_every\" policy updates.\n",
    "        if episode % eval_every == 0:\n",
    "            episode_runner.policy.eval()\n",
    "            total_reward = 0\n",
    "            episode_lengths = []\n",
    "            #evaluate on \"eval_episodes\" episodes the total reward and the episodes length\n",
    "            for _ in range(eval_episodes):\n",
    "                (_, _, _, rewards) = episode_runner.run_episode()\n",
    "                total_reward += np.sum(rewards)\n",
    "                episode_lengths.append(len(rewards))\n",
    "            average_episode_len_metric = {\"average_episode_length\": np.mean(episode_lengths)}\n",
    "            average_rewards_metric = {\"average_total_reward\": total_reward / eval_episodes}\n",
    "            wandb.log({**average_rewards_metric, **average_episode_len_metric}, commit = False)\n",
    "            # Update best policy, the one with the best average total reward at testing time\n",
    "            if  total_reward / eval_episodes >= best_avg_tot_rew:\n",
    "                best_avg_tot_rew = total_reward / eval_episodes\n",
    "                # save all the parameters of the best policy\n",
    "                best_model_state_dict = episode_runner.policy.state_dict()\n",
    "            if display:\n",
    "                (obs, _, _, _) = episode_runner_render.run_episode()\n",
    "            episode_runner.policy.train()\n",
    "            print(f'Running reward of episode {episode}/{num_episodes}: {running_rewards[-1]}')\n",
    "            print(f'Average Total reward: {total_reward / eval_episodes}')\n",
    "        \n",
    "        wandb.log({**running_rewards_metric})\n",
    "\n",
    "    # Lastly, calculate and print the average episode lenght of the entire training\n",
    "    print(f'Average length of all episodes: {np.mean(all_episodes_lenghts)}')\n",
    "    average_all_episodes_metric= {\"average_lenght_all_episodes\": np.mean(all_episodes_lenghts)}\n",
    "    wandb.log({**average_all_episodes_metric})\n",
    "    \n",
    "    episode_runner.policy.eval()\n",
    "    if isinstance(baseline, nn.Module):\n",
    "        baseline.eval()\n",
    "    return best_model_state_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea05c3",
   "metadata": {},
   "source": [
    "# Standard runs\n",
    "In the following section I tested `REINFORCE` on the Cartpole environment using the standard baseline while changing the parameters (temperature and gamma).\n",
    "\n",
    "Since training the policy is stochastic, for each setting I performed the policy training 5 times with 5 different seeds to be able to replicate the experiments.\n",
    "\n",
    "At the end of this section there will be a wandb report with the results, for every setting the 5 runs are aggregated showing their mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mkTymBWNPESI",
   "metadata": {
    "id": "mkTymBWNPESI"
   },
   "source": [
    "## Standard run\n",
    "Base parameters, temperature = 1 and gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b440c-2b1d-42db-81f2-9bdd6d036d3a",
   "metadata": {
    "id": "378b440c-2b1d-42db-81f2-9bdd6d036d3a",
    "outputId": "396212df-359e-4c94-c804-398648e3f57b"
   },
   "outputs": [],
   "source": [
    "# Random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard Run \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 1,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    # Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    # Set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b48689-6e55-4f7d-aa56-c4ce6459e765",
   "metadata": {},
   "source": [
    "## Standard run and higher temperature (5)\n",
    "Higher temperature, temperature = 5 and gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef897b43-445f-467b-8e2e-8bd9e30fc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard and temperature 5 \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98a63e-5e66-4ecc-98b7-ce1341a1e81a",
   "metadata": {},
   "source": [
    "## Standard run and higher temperature (10)\n",
    "Even higher temperature, temperature = 10 and gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578621a9-c76a-46c9-8a80-9b9411c7ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard and temperature 10 \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 10,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e2878-19d2-410b-bb75-e468082ad22b",
   "metadata": {},
   "source": [
    "## Standard run and lower gamma\n",
    "Higher temperature (since it achieved better results) and lower gamma, temperature = 5 and gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e338ef-b678-433f-8404-3ab28b34b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard and gamma 0.9 \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.9,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915a9b8",
   "metadata": {},
   "source": [
    "## Standard Runs Report\n",
    "This is the public report created with weights and biases: https://api.wandb.ai/links/marcouni/c8d6xyuv\n",
    "\n",
    "It is quite easy to see that the best performance is obtained when the temperature is 5 and gamma is 0.99. Its average total reward is almost always the highest and it is extremely fast to converge (highest average length of all episodes) to a very stable perfect policy (always able to reach the 500th step in each episode). Its test average reward is perfect, meaning that the final policy is one of the possible best policies for this environment.\n",
    "\n",
    "The standard run and the one with temperature = 10 are not that different, except that the latter seems to be a little slower to converge (lower average length of all episodes) but is more stable (perfect test average total reward).\n",
    "\n",
    "It is also clear that the worst run is the one with a lower gamma, being the worst by every metric. This result shows the importance of future steps (through the importance of future rewards) in Cartpole. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5adad7-759b-4000-925b-701f41fe6e97",
   "metadata": {
    "id": "2a5adad7-759b-4000-925b-701f41fe6e97"
   },
   "source": [
    "# Exercise 2: `REINFORCE` with a Value Baseline (warm up)\n",
    "\n",
    "In this exercise we will augment my implementation (or your own) of `REINFORCE` to subtract a baseline from the target in the update equation in order to stabilize (and hopefully speed-up) convergence. For now we will stick to the Cartpole environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07",
   "metadata": {
    "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07"
   },
   "source": [
    "**First Things First**: Recall from the slides on Deep Reinforcement Learning that we can **subtract** any function that doesn't depend on the current action from the q-value without changing the (maximum of our) objecttive function $J$:  \n",
    "\n",
    "$$ \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_{s} \\mu(s) \\sum_a \\left( q_{\\pi}(s, a) - b(s) \\right) \\nabla \\pi(a \\mid s, \\boldsymbol{\\theta}) $$\n",
    "\n",
    "In `REINFORCE` this means we can subtract from our target $G_t$:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - b(S_t)) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
    "\n",
    "Since we are only interested in the **maximum** of our objective, we can also **rescale** our target by any function that also doesn't depend on the action. A **simple baseline** which is even independent of the state -- that is, it is **constant** for each episode -- is to just **standardize rewards within the episode**. So, we **subtract** the average return and **divide** by the variance of returns:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha \\left(\\frac{G_t - \\bar{G}}{\\sigma_G}\\right) \\nabla  \\pi(A_t \\mid s, \\boldsymbol{\\theta}) $$\n",
    "\n",
    "This baseline is **already** implemented in my implementation of `REINFORCE`. Experiment with and without this standardization baseline and compare the performance. We are going to do something more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df",
   "metadata": {
    "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df"
   },
   "source": [
    "**The Real Exercise**: Standard practice is to use the state-value function $v(s)$ as a baseline. This is intuitively appealing -- we are more interested in updating out policy for returns that estimate the current **value** worse. Our new update becomes:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - \\tilde{v}(S_t \\mid \\mathbf{w})) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
    "\n",
    "where $\\tilde{v}(s \\mid \\mathbf{w})$ is a **deep neural network** with parameters $w$ that estimates $v_\\pi(s)$. What neural network? Typically, we use the **same** network architecture as that of the Policy.\n",
    "\n",
    "**Your Task**: Modify your implementation to fit a second, baseline network to estimate the value function and use it as **baseline**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed0e92",
   "metadata": {},
   "source": [
    "# Different baselines\n",
    "This section contains the results of the experiments with REINFORCE on Cartpole using different kinds of baselines (no baseline, standard baseline and a Value Net as a baseline).\n",
    "\n",
    "Since previously the best results were obtained with temperature = 5 and gamma = 0.99 these parameters will be used in all the following runs. I also experimented with changing the number of layers of the Policy and Value Nets.\n",
    "\n",
    "Once again, for each setting I performed the policy training 5 times with 5 different seeds to be able to replicate the experiments and at the end of this section there will be a wandb report with the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f97e3",
   "metadata": {},
   "source": [
    "## Without the baseline\n",
    "Since I already experimented with Reinforce with the standard baseline the next run will not cointain a baseline of any kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76680bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Without the baseline \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": None,\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ad67f-180b-4009-a6e7-752fe1e4bc58",
   "metadata": {},
   "source": [
    "## With Baseline network\n",
    "Using ValueNet, a simple Net with the same architecture as the Policy Net that has to learn to estimate the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6187b4f3-568a-49a5-93f6-cc7cccb8cc2f",
   "metadata": {
    "id": "6187b4f3-568a-49a5-93f6-cc7cccb8cc2f"
   },
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, env, hidden_layers=32):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], hidden_layers)\n",
    "        self.fc2 = nn.Linear(hidden_layers, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = self.fc2(s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f",
   "metadata": {
    "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f"
   },
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Baseline Network \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f186e88-9437-4453-b4f8-27c0d3c8e92f",
   "metadata": {},
   "source": [
    "## With baseline, 128 hidden layer\n",
    "Adding more hidden layers to the ValueNet and PolicyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22488e84-fadd-44e7-b9a8-4d3ba72f0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Baseline Network 128 layers temp 5\",\n",
    "          config={\n",
    "              \"hidden_layers\": 128,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac2ae4a-f4e8-432d-aa66-fa50cc891a8a",
   "metadata": {},
   "source": [
    "## With baseline, 16 hidden layer\n",
    "Lowering the number of hidden layers of the ValueNet and PolicyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0652b1-5cbf-4cc1-8e67-9d80ab45be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Baseline Network 16 layers temp 5\",\n",
    "          config={\n",
    "              \"hidden_layers\": 16,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c1e91",
   "metadata": {},
   "source": [
    "## Baselines Runs Report\n",
    "This is the public report created with weights and biases: https://api.wandb.ai/links/marcouni/0ksdyjpo\n",
    "\n",
    "Once again the best setting seems to be the one with a standard baseline and temperature = 5, closely followed by the settings with baseline networks with 32 (standard) and 128 hidden layers (more layers achieve a slightly better result). Given these results using a standard baseline and temperature = 5 seems like the best case scenario for Cartpole.\n",
    "This is probably due to the simplicity (the standardized mean of the discounted return seems like a very good approximation of the value function) of the problem since its convergence is incredibly fast and stable. \n",
    "\n",
    "Using a ValueNet also leads to a stable training and a perfect policy (the test average total reward is perfect with 32 and 128 hidden layers) but seems to slow down the convergence as it needs more time to learn to approximate the value function. \n",
    "\n",
    "When the layers of the two Networks are not enough (16 hidden layers) they are unable to correctly approximate the value function and the policy.\n",
    "\n",
    "As expected the setting with no baseline produces the worst results, the runs are much less stable, as it can be seen in average total reward and average length all episodes. The final policy is also extremely unstable, it can be perfect or horrible ( test average total reward has a very high variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc5eec-9ebc-4867-b698-cfefbe6387d8",
   "metadata": {},
   "source": [
    " # Longer runs\n",
    " Lastly I also experiments with much longer runs with two of the best settings, with standard baseline and ValueNet, to see if their performance gap was due to the number of training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a52700c-e8cd-4048-b567-bb885b9eea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Long Standard\",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 5000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 50,\n",
    "              \"test_episodes\" : 500,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf6d680-b49d-4b82-81e8-fb431e225e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Long Baseline Network \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 5000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 50,\n",
    "              \"test_episodes\" : 500,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c664f9",
   "metadata": {},
   "source": [
    "## Longer Runs Report\n",
    "This is the public report created with weights and biases: https://api.wandb.ai/links/marcouni/r8dqdyc5\n",
    "\n",
    "It is clear that increasing the number of training episodes deteriorates both settings' performances, making the training more unstable and the final policy less good. The one that suffers a worse decline in reward is the one with ValueNet, probably due to overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b3194",
   "metadata": {},
   "source": [
    "# Video recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0459791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "env_render = gymnasium.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "# wrap the env in the record video\n",
    "recorder = gymnasium.wrappers.RecordVideo(env=env_render, video_folder=\"/data01/dl24marchi/DLAProjects/videos\", name_prefix=\"test-video\", episode_trigger=lambda x: x%10 == 0)\n",
    "\n",
    "# env reset for a fresh start\n",
    "observation, info = recorder.reset()\n",
    "\n",
    "###\n",
    "# Start the recorder\n",
    "recorder.start_video_recorder()\n",
    "\n",
    "# And run the final agent for a few episodes.\n",
    "for _ in range(100):\n",
    "    run_episode(recorder, policy)\n",
    "\n",
    "####\n",
    "# Don't forget to close the video recorder before the env!\n",
    "recorder.close_video_recorder()\n",
    "\n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf1447-d222-4b24-a357-5b7f9824390c",
   "metadata": {
    "id": "64bf1447-d222-4b24-a357-5b7f9824390c"
   },
   "source": [
    "-----\n",
    "# Exercise 3: Going Deeper\n",
    "\n",
    "As usual, pick **AT LEAST ONE** of the following exercises to complete.\n",
    "\n",
    "### Exercise 3.1: Solving Lunar Lander with `REINFORCE` (easy)\n",
    "\n",
    "Use my (or even better, improve on my) implementation of `REINFORCE` to solve the [Lunar Lander Environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/). This environment is a little bit harder than Cartpole, but not much. Make sure you perform the same types of analyses we did during the lab session to quantify and qualify the performance of your agents.\n",
    "\n",
    "### Exercise 3.2: Solving Cartpole and Lunar Lander with `Deep Q-Learning` (harder)\n",
    "\n",
    "On policy Deep Reinforcement Learning tends to be **very unstable**. Write an implementation (or adapt an existing one) of `Deep Q-Learning` to solve our two environments (Cartpole and Lunar Lander). To do this you will need to implement a **Replay Buffer** and use a second, slow-moving **target Q-Network** to stabilize learning.\n",
    "\n",
    "### Exercise 3.3: Solving the OpenAI CarRacing environment (hardest)\n",
    "\n",
    "Use `Deep Q-Learning` -- or even better, an off-the-shelf implementation of **Proximal Policy Optimization (PPO)** -- to train an agent to solve the [OpenAI CarRacing](https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN) environment. This will be the most *fun*, but also the most *difficult*. Some tips:\n",
    "\n",
    "1. Make sure you use the `continuous=False` argument to the environment constructor. This ensures that the action space is **discrete** (we haven't seen how to work with continuous action spaces).\n",
    "2. Your Q-Network will need to be a CNN. A simple one should do, with two convolutional + maxpool layers, folowed by a two dense layers. You will **definitely** want to use a GPU to train your agents.\n",
    "3. The observation space of the environment is a single **color image** (a single frame of the game). Most implementations stack multiple frames (e.g. 3) after converting them to grayscale images as an observation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d99e29",
   "metadata": {},
   "source": [
    "## CarRacing-v2 environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a79147",
   "metadata": {},
   "source": [
    "The raw frames are preprocessed by cropping an 84  84 region of the image that roughly captures the playing area (every frame always contains a black area at the bottom of the frame, so we cut that black area) and converting their RGB representation to gray-scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb9fe414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess(img):\n",
    "    img = img[:84, 6:90] # CarRacing-v2-specific cropping\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd508d6c",
   "metadata": {},
   "source": [
    "The game screen gradually zooms in for the first 50 steps, thus, we will not use the first 50 steps of the game.\n",
    "\n",
    "The preprocessing is applied to the last 4 frames, they are stacked to produce the input to the Q-function.\n",
    "\n",
    "A simple frame-skipping technique is used: the agent sees and selects actions on every kth frame instead of every frame and its last action is repeated on skipped frames. Running the emulator forward for one step requires much less computation than having the agent select an action, this technique allows the agent to play roughly k times more games without significantly increasing the runtime.\n",
    "\n",
    "ImageEnv is a gymnasium wrapper that redefines reset() and step() following the previously defined changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "206ce119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEnv(gymnasium.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        skip_frames=4,         # How many frames are skipped\n",
    "        stack_frames=4,        # How many frames to stack\n",
    "        initial_no_op=50,      # No actions during the zoom in\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ImageEnv, self).__init__(env, **kwargs)\n",
    "        self.initial_no_op = initial_no_op\n",
    "        self.skip_frames = skip_frames\n",
    "        self.stack_frames = stack_frames\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the original environment.\n",
    "        state, info = self.env.reset()\n",
    "\n",
    "        # Do nothing for the first `self.initial_no_op` steps, zoom in\n",
    "        for _ in range(self.initial_no_op):\n",
    "            state, _, _, _, info = self.env.step(0)   # action 0 : do nothing\n",
    "        \n",
    "        # Convert a frame to 84 X 84 gray scale one\n",
    "        state = preprocess(state)\n",
    "\n",
    "        # The initial observation is simply a copy of the frame `s`, np.tile repeats s self.stack_frames times\n",
    "        self.stacked_state = np.tile(state, (self.stack_frames, 1, 1))  # [4, 84, 84]\n",
    "        return self.stacked_state, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # We take the same action for self.skip_frames steps\n",
    "        reward = 0\n",
    "        for _ in range(self.skip_frames):\n",
    "            state, r, terminated, truncated, info = self.env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        # Convert a frame to 84 X 84 gray scale one\n",
    "        state = preprocess(state)\n",
    "\n",
    "        # Push the current frame `state` at the end of self.stacked_state while removing the oldest frame\n",
    "        self.stacked_state = np.concatenate((self.stacked_state[1:], state[np.newaxis]), axis=0)\n",
    "\n",
    "        return self.stacked_state, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ce29bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of an observation:  (4, 84, 84)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Discrete action space: do nothing, steer left, steer right, gas, brake.\n",
    "env = gymnasium.make('CarRacing-v2', continuous=False)\n",
    "env = ImageEnv(env)\n",
    "\n",
    "# Check reset\n",
    "state, _ = env.reset()\n",
    "print(\"The shape of an observation: \", state.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for i in range(4):\n",
    "    axes[i].imshow(state[i], cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0497fa6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check step, gas action for 4 steps, check moving background\n",
    "for i in range(4):\n",
    "    state, r, terminated, truncated, info = env.step(3)  # 3rd action is `gas` action\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for i in range(4):\n",
    "    axes[i].imshow(state[i], cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ccdfb",
   "metadata": {},
   "source": [
    "## DQN\n",
    "A Neural Network that takes in n_observations frames and tries to predict the expected return of taking each possible action given the current input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62abd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NetDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(NetDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_observations, 16, kernel_size=7, stride=3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4)\n",
    "        # Image 5*5 after convolutions and max pooling\n",
    "        self.fc1 = nn.Linear(32*5*5, 256)  \n",
    "        self.fc2 = nn.Linear(256, n_actions)   \n",
    "        self.max_pool= nn.MaxPool2d(kernel_size= (2,2))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.max_pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "        x = torch.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477c8877",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "It stores the transitions (state, action, next state, reward) that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "682bf902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e5)):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0     # Pointer\n",
    "\t\tself.size = 0    # Current size\n",
    "\n",
    "\t\tself.state = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "\t\tself.action = np.zeros((max_size, *action_dim), dtype=np.int64)\n",
    "\t\tself.next_state = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "\t\tself.reward = np.zeros((max_size, 1), dtype=np.float32)\n",
    "\t\tself.terminated = np.zeros((max_size, 1), dtype=np.float32)\n",
    "\n",
    "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\tdef add(self, state, action, next_state, reward, terminated):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.not_done[self.ptr] = terminated\n",
    "\n",
    "        # Update pointer and current size\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size  \n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\t# Collect batch_size random integers from 0 to current size \n",
    "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.terminated[ind]).to(self.device)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04df5ab",
   "metadata": {},
   "source": [
    "## DQNAgent\n",
    "DQN agent:\n",
    "\n",
    "- act() takes as input one state and outputs an action by following an epsilon-greedy policy.\n",
    "\n",
    "- learn() samples a batch from replay buffer and trains the NetDQN with MSE loss using temporal difference\n",
    "\n",
    "- process() takes one transition (state, action, next state, reward) as input and decides if the agent can learn (after the warmup) and if the target network has to be updated, it also decays the current epsilon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b731ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=0.0005,\n",
    "        epsilon=1.0,\n",
    "        epsilon_min=0.1,\n",
    "        gamma=0.99,\n",
    "        batch_size=64,\n",
    "        warmup_steps=2500,\n",
    "        buffer_size=int(1e5),\n",
    "        target_update_interval=5000,\n",
    "    ):\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.target_update_interval = target_update_interval   # After how many steps update target net\n",
    "\n",
    "        self.network = NetDQN(state_dim[0], action_dim)\n",
    "        self.target_network = NetDQN(state_dim[0], action_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr)\n",
    "\n",
    "        self.buffer = ReplayBuffer(state_dim, (1, ), buffer_size)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "        self.network.to(self.device)\n",
    "        self.target_network.to(self.device)\n",
    "        \n",
    "        self.total_steps = 0\n",
    "        self.epsilon_decay = (epsilon - epsilon_min) / 5e5\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def act(self, state, training=True):\n",
    "        # Set network to train() mode if training\n",
    "        self.network.train(training)\n",
    "        # Random action if training and if randomly chosen with epsilon or we are in the first warmup steps\n",
    "        if training and ((np.random.rand() < self.epsilon) or (self.total_steps < self.warmup_steps)):\n",
    "            action = np.random.randint(0, self.action_dim)\n",
    "        # Follow the current policy, select best action given the approximated q value\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "            qvalue = self.network(state)\n",
    "            action = torch.argmax(qvalue).item()\n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        # Sample batch size transitions from replay buffer \n",
    "        state, action, next_state, reward, terminated = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Calculated with the target network, detach: do not train the target net\n",
    "        next_qvalue = self.target_network(next_state).detach()\n",
    "        temporal_difference_target = reward + (1. - terminated) * self.gamma * next_qvalue.max(dim=1, keepdim=True).values\n",
    "        # MSE loss between current q value (gather q value that corresponds to action) and temporal difference of the target\n",
    "        loss = F.mse_loss(self.network(state).gather(1, action.long()), temporal_difference_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        result = {\n",
    "            'reward': reward,\n",
    "            'training_loss': loss.item()\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process(self, transition):\n",
    "        result = {}\n",
    "        self.total_steps += 1\n",
    "        self.buffer.update(*transition)\n",
    "        \n",
    "        # Start learning after the warmup\n",
    "        if self.total_steps > self.warmup_steps:\n",
    "            result = self.learn()\n",
    "        \n",
    "        # Update the target network\n",
    "        if self.total_steps % self.target_update_interval == 0:\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "        # Epsilon decay\n",
    "        self.epsilon -= self.epsilon_decay\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93672c",
   "metadata": {},
   "source": [
    "## Training the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a567ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the agent \n",
    "def evaluate(eval_env, agent, n_evals=10):\n",
    "    scores = 0\n",
    "    episode_length = 0\n",
    "    for _ in range(n_evals):\n",
    "        (state, _), done, ret = eval_env.reset(), False, 0\n",
    "        while not done:\n",
    "            action = agent.act(state, training=False)\n",
    "            next_state, reward, terminated, truncated, info = eval_env.step(action)\n",
    "            state = next_state\n",
    "            ret += reward\n",
    "            episode_length += 1\n",
    "            done = terminated or truncated\n",
    "        scores += ret\n",
    "    result = {\n",
    "            'scores': np.round(scores / n_evals, 4),\n",
    "            'average_length': np.round(episode_length / n_evals, 4)\n",
    "        }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b68ddab",
   "metadata": {},
   "source": [
    "Train the DQN Agent for 1 million steps and evaluate it every 5000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5913b10",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m state_dim \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m84\u001b[39m)\n\u001b[1;32m      7\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[0;32m----> 9\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQNAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[0;34m(self, state_dim, action_dim, lr, epsilon, epsilon_min, gamma, batch_size, warmup_steps, buffer_size, target_update_interval)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m ReplayBuffer(state_dim, (\u001b[38;5;241m1\u001b[39m, ), buffer_size)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_network\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/DLAProjects/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DLAProjects/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/DLAProjects/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/DLAProjects/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/DLAProjects/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    292\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 293\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    297\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "run=wandb.init(\n",
    "          project=\"Lab3-DRL-DQN_Agent\",\n",
    "          name = \"Training DQN\",\n",
    "          config={\n",
    "            \"lr\" : 0.0005,\n",
    "            \"epsilon\" : 1.0,\n",
    "            \"epsilon_min\" : 0.1,\n",
    "            \"gamma\" : 0.99,\n",
    "            \"batch_size\" : 64,\n",
    "            \"warmup_steps\" : 2500,\n",
    "            \"buffer_size\" :int(1e5),\n",
    "            \"target_update_interval\" : 5000\n",
    "            \"max_steps\": int(1e6)\n",
    "            \"eval_interval\" : 5000\n",
    "              })\n",
    "    \n",
    "# Copy the configuration\n",
    "config = wandb.config\n",
    "\n",
    "# Create environments to train and evaluate\n",
    "env = gymnasium.make('CarRacing-v2', continuous=False)\n",
    "env = ImageEnv(env)\n",
    "\n",
    "eval_env = gymnasium.make('CarRacing-v2', continuous=False)\n",
    "eval_env = ImageEnv(eval_env)\n",
    "\n",
    "# General definitions\n",
    "state_dim = (4, 84, 84)\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_dim, action_dim, config.lr, config.epsilon, config.epsilon_min, config.gamma, config.batch_size,\n",
    "                 config.warmup_steps, config.buffer_size, config.target_update_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fde452",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "# To save the best agent, the one with the best eval avg score\n",
    "best_avg_score = 0\n",
    "while True:\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    result = agent.process((state, action, reward, next_state, terminated))  \n",
    "\n",
    "    state = next_state\n",
    "    if terminated or truncated:\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "    if agent.total_steps % config.eval_interval == 0:\n",
    "        ret = evaluate(eval_env, agent)\n",
    "\n",
    "        scores_metric = {\"Average Eval Scores\": ret['scores']}\n",
    "        episode_length_metric = {\"Average Episode Length\": ret['average_length']}\n",
    "        wandb.log({**scores_metric, **episode_length_metric}, commit = False)\n",
    "        \n",
    "        # Save the best agent\n",
    "        if best_avg_score < ret['scores']:\n",
    "            best_avg_score = ret['scores']\n",
    "            # Save all the parameters of the model\n",
    "            model_state_dict = agent.network.state_dict()\n",
    "    \n",
    "    loss_metric = {\"Trainin Loss\": result['training_loss']}\n",
    "    training_reward_metric = {\"Training Reward\": result['reward']}\n",
    "    wandb.log({**loss_metric, **training_reward_metric})\n",
    "    \n",
    "    if agent.total_steps > config.max_steps:\n",
    "         # Load saved weights of the best model\n",
    "        agent.network.load_state_dict(model_state_dict)\n",
    "\n",
    "        # Save the best model on weights and biases as an artifact\n",
    "        model_artifact = wandb.Artifact(\n",
    "                    \"DQN_Agent\" , type=\"model\",\n",
    "                    description=\"best model for Lab3-DRL\",\n",
    "                    metadata=dict(config))\n",
    "\n",
    "        torch.save(agent.network.state_dict(), \"best_model.pth\")\n",
    "        model_artifact.add_file(\"best_model.pth\")\n",
    "        wandb.save(\"best_model.pth\")\n",
    "        run.log_artifact(model_artifact)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb95146",
   "metadata": {},
   "source": [
    "## Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b2ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gymnasium.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "test_env = ImageEnv(test_env)\n",
    "\n",
    "frames = []\n",
    "scores = 0\n",
    "(state, _), done, ret = eval_env.reset(), False, 0\n",
    "while not done:\n",
    "    frames.append(test_env.render())\n",
    "    action = agent.act(state, training=False)\n",
    "    next_state, reward, terminated, truncated, info = test_env.step(action)\n",
    "    state = next_state\n",
    "    ret += reward\n",
    "    done = terminated or truncated\n",
    "scores += ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(imgs, video_name, _return=True):\n",
    "    import cv2\n",
    "    import os\n",
    "    import string\n",
    "    import random\n",
    "    \n",
    "    if video_name is None:\n",
    "        video_name = ''.join(random.choice(string.ascii_letters) for i in range(18)) + '.webm'\n",
    "    height, width, layers = imgs[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
    "    video = cv2.VideoWriter(video_name, fourcc, 10, (width, height))\n",
    "    \n",
    "    for img in imgs:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        video.write(img)\n",
    "    video.release()\n",
    "    if _return:\n",
    "        from IPython.display import Video\n",
    "        return Video(video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "animate(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e029510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
