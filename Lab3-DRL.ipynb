{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc",
   "metadata": {
    "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc"
   },
   "source": [
    "# Deep Reinforcement Learning Laboratory\n",
    "\n",
    "This laboratory is focused on getting more advanced versions of Deep Reinforcement Learning algorithms up and running. Deep Reinforcement Learning is **hard**, and getting agents to stably train can be frustrating and requires quite a bit of subtlety in analysis of intermediate results. We will start by implementing `REINFORCE` on the [Cartpole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uldpcHx0OHyx",
   "metadata": {
    "id": "uldpcHx0OHyx"
   },
   "source": [
    "## Imports and weights and biases login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689f766-b099-4948-a1b2-37e92975178f",
   "metadata": {
    "id": "9689f766-b099-4948-a1b2-37e92975178f"
   },
   "outputs": [],
   "source": [
    "# Using weights and biases\n",
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241e9bdc-7aa9-4a12-a57f-088fdc87eab6",
   "metadata": {
    "id": "241e9bdc-7aa9-4a12-a57f-088fdc87eab6"
   },
   "outputs": [],
   "source": [
    "# Standard imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Plus one non standard one -- we need this to sample from policies.\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806e9d4-bb04-40ed-9cd4-f016823b100c",
   "metadata": {
    "id": "e806e9d4-bb04-40ed-9cd4-f016823b100c",
    "outputId": "753a212b-355e-4133-f7e1-53e9b5895040"
   },
   "outputs": [],
   "source": [
    "# Login to weights and biases account\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f",
   "metadata": {
    "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f"
   },
   "source": [
    "# Exercise 1: `REINFORCE` Implementation (warm up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dk0EFuXUOpXz",
   "metadata": {
    "id": "Dk0EFuXUOpXz"
   },
   "source": [
    "## Policy Net\n",
    "A simple policy network with one hidden layer and a temperature parameter to smooth the output (the higher the temperature the smoother the output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ea9416-b07e-4354-bf36-1150394b0698",
   "metadata": {
    "id": "16ea9416-b07e-4354-bf36-1150394b0698"
   },
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, env, hidden_layers=32, temperature=1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], hidden_layers)\n",
    "        self.fc2 = nn.Linear(hidden_layers, env.action_space.n)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = F.softmax(self.fc2(s)/self.temperature, dim=-1)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uvEAh3bgOWTz",
   "metadata": {
    "id": "uvEAh3bgOWTz"
   },
   "source": [
    "## Episode Runner\n",
    "Class that runs the training episodes collecting all the useful info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd2dd9d-a60b-4892-878f-d83f64ac5f63",
   "metadata": {
    "id": "cbd2dd9d-a60b-4892-878f-d83f64ac5f63",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A class that , given an environment, a policy network and the max lenght of the episode is in charge\n",
    "# of running it\n",
    "class Episode_runner:\n",
    "    def __init__(self, env, policy, maxlen=500):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.maxlen= maxlen\n",
    "\n",
    "    # Given observation and the policy, sample from pi(a | obs). Returns the\n",
    "    # selected action and the log probability of that action (needed for policy gradient).\n",
    "    def select_action(self, obs):\n",
    "        dist = Categorical(self.policy(obs))\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return (action.item(), log_prob.reshape(1))\n",
    "\n",
    "    # Given the environment and the policy, run it up to the maximum number of steps.\n",
    "    def run_episode(self):\n",
    "        # Collect just about everything.\n",
    "        observations = []\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "\n",
    "        # Reset the environment and start the episode.\n",
    "        (obs, info) = self.env.reset() \n",
    "        for i in range(self.maxlen):\n",
    "            # Get the current observation, run the policy and select an action.\n",
    "            obs = torch.tensor(obs)\n",
    "            (action, log_prob) = self.select_action(obs)\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "            # Advance the episode by executing the selected action.\n",
    "            (obs, reward, term, trunc, info) = self.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if term or trunc:\n",
    "                break\n",
    "        return (observations, actions, torch.cat(log_probs), rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc5787-1c38-4f5c-b716-e40e900d00d2",
   "metadata": {},
   "source": [
    "## Deterministic Episode Runner\n",
    "Class that runs the testing episodes. The main difference with the training Episode Runner is that the action selected is always the one with the maximum probability, hence why it's called deterministic.\n",
    "\n",
    "To test the current best policy, the total average reward and episode length is calculated using the best policy in a deterministic manner on test_episodes episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f505e44c-0d5e-48f1-b874-d318424b3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class that , given an episode runner (with an environment and a policy network), an episode render to show the policy \n",
    "# and the max lenght of each episode evaluates the quality of the learned policy network, \n",
    "# always selecting the action with max probability \n",
    "class Determinist_Test_Episode_runner:\n",
    "    def __init__(self, episode_runner, episode_runner_render, maxlen=500):\n",
    "        self.ep_runner = episode_runner\n",
    "        self.ep_run_render = episode_runner_render\n",
    "        self.maxlen= maxlen\n",
    "\n",
    "    #select the most probable action given the policy and current observation\n",
    "    def select_action(self, obs):\n",
    "        dist = Categorical(self.ep_runner.policy(obs))\n",
    "        action= torch.argmax(dist.log_prob(dist.enumerate_support()))\n",
    "        return action.item()\n",
    "\n",
    "    # Given the environment and the policy, run it up to the maximum number of steps \n",
    "    def run_episode(self):\n",
    "        # Collect just about everything.\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        # Reset the environment and start the episode.\n",
    "        (obs, info) = self.ep_runner.env.reset() \n",
    "        for i in range(self.maxlen):\n",
    "            # Get the current observation, run the policy and select an action.\n",
    "            obs = torch.tensor(obs)\n",
    "            action = self.select_action(obs)\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "\n",
    "            # Advance the episode by executing the selected action.\n",
    "            (obs, reward, term, trunc, info) = self.ep_runner.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if term or trunc:\n",
    "                break\n",
    "        return (observations, actions, rewards)\n",
    "\n",
    "    def test(self, test_episodes):\n",
    "        print('Testing the best policy')\n",
    "        self.ep_runner.policy.eval()\n",
    "        total_reward = 0\n",
    "        episode_lengths = []\n",
    "        for _ in range(test_episodes):\n",
    "            (_, _, rewards) = self.run_episode()\n",
    "            total_reward += np.sum(rewards)\n",
    "            episode_lengths.append(len(rewards))\n",
    "        test_average_episode_len_metric = {\"test_average_episode_length\": np.mean(episode_lengths)}\n",
    "        test_average_rewards_metric = {\"test_average_total_reward\": total_reward / test_episodes}\n",
    "        wandb.log({**test_average_rewards_metric, **test_average_episode_len_metric})\n",
    "\n",
    "        (obs, _, _, _) = self.ep_run_render.run_episode()\n",
    "        self.ep_runner.policy.train()\n",
    "        print(f'Average Total reward: {total_reward / test_episodes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BhypW9ySOthT",
   "metadata": {
    "id": "BhypW9ySOthT"
   },
   "source": [
    "## Reinforce\n",
    "Implementation of REINFORCE policy gradient algorithm.\n",
    "\n",
    "It receives two \"training\" episode runners, one to train the policy and one to show the rendering of some episodes if display = True to see the progress.\n",
    "The training parameters are:\n",
    "- gamma: used to compute discounted total reward, balances the importance of immediate versus future reward. A higher value gives a higher weight to future rewards while a lower value prioritizes immediate rewards.\n",
    "- baseline: the type of baseline used to compute the value function. None (no baseline), std (standard baseline, standardize rewards within the episode) or a ValueNet (the value function is approximated by a Neural Network with the same architecture as the policy network)\n",
    "- num_episodes: number of training episodes to train the policy and baseline net\n",
    "- eval_every: after how many training episodes the policy is evaluated\n",
    "- eval_episodes: the number of episodes the policy is evaluated on\n",
    "- lr and lr_baseline: learning rates of the policy and baseline net\n",
    "\n",
    "Given all these inputs, for each episode, the discounted reward, running reward and episode length (in the cartpole case the episode length is equal to the reward of that episode) are computed. If there is a baseline, the policy loss, that is used to optimize the policy, is calculated with the target value. If the baseline is a Neural Network it is also optimized using its loss (how closely it approximates the value function).\n",
    "\n",
    "After eval_every episodes of training the policy is evaluated on eval_episodes episodes. Instead of using the running average the function calculates the average total reward of the episodes that were runned and their average length. As previously said, these two quantities are the same in Cartpole since the reward of each episode is its length. During this evaluation the best model (the one with the best average reward) is saved.\n",
    "\n",
    "I also chose to collect all the episodes lengths and losses, every 10 episodes are aggregated in a single value (the mean) to create graphs that are easier to read.\n",
    "\n",
    "Lastly, the function calculates the average episode length of the entire training to show a general trend, it can be used to investigate the stability of a training and how fast it is able to converge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a50f666-126a-4a16-9cbc-d9c846a1c3a7",
   "metadata": {
    "id": "2a50f666-126a-4a16-9cbc-d9c846a1c3a7"
   },
   "outputs": [],
   "source": [
    "# Utility to compute the discounted total reward. Torch doesn't like flipped arrays, so we need to\n",
    "# .copy() the final numpy array.\n",
    "def compute_returns(rewards, gamma):\n",
    "    return np.flip(np.cumsum([gamma**(i+1)*r for (i, r) in enumerate(rewards)][::-1]), 0).copy()\n",
    "\n",
    "# Implementation of the REINFORCE policy gradient algorithm.\n",
    "# It receives the episode runner, the wandb run to save the results, the episode runner render that is used to monitor training \n",
    "# when display = True, the gamma parameter, the number of episodes to train the policy and baseline net, the type of baseline used,\n",
    "# eval_every (after how many training steps we evaluate the policy), eval_episode (how many episodes we \n",
    "# evaluate the policy on) and the learning rates\n",
    "def reinforce(episode_runner, wandb, episode_runner_render=None, gamma=0.99, num_episodes=2000,\n",
    "              baseline=None, display=False, eval_every=100, eval_episodes=100, lr= 1e-2, lr_baseline = 1e-3 ):\n",
    "    # We use Adam instead of SGD.\n",
    "    opt = torch.optim.Adam(episode_runner.policy.parameters(), lr= lr)\n",
    "\n",
    "    # If we have a baseline network, create the optimizer.\n",
    "    if isinstance(baseline, nn.Module):\n",
    "        opt_baseline = torch.optim.Adam(baseline.parameters(), lr= lr_baseline)  \n",
    "        baseline.train()\n",
    "        print('Training agent with baseline value network.')\n",
    "    elif baseline == 'std':\n",
    "        print('Training agent with standardization baseline.')\n",
    "    else:\n",
    "        print('Training agent with no baseline.')\n",
    "\n",
    "    #Collect running rewards, all the episodes lengths and training loss\n",
    "    running_rewards = [0.0]\n",
    "    all_episodes_lenghts = []\n",
    "    training_losses = []\n",
    "    value_losses = []\n",
    "\n",
    "    #save the latest policy with the greatest average totale reward\n",
    "    best_model_state_dict = None\n",
    "    best_avg_tot_rew = 0\n",
    "    \n",
    "    # The main training loop.\n",
    "    episode_runner.policy.train()\n",
    "    for episode in range(1, num_episodes+1):\n",
    "        # Run an episode of the environment, collect everything needed for policy update.\n",
    "        (observations, actions, log_probs, rewards) = episode_runner.run_episode()\n",
    "\n",
    "        # Compute the discounted reward for every step of the episode.\n",
    "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
    "        # Keep a running average of total discounted rewards for the whole episode.\n",
    "        running_rewards.append(0.05 * returns[0].item() + 0.95 * running_rewards[-1])\n",
    "        running_rewards_metric = {\"running_reward\": running_rewards[-1]}\n",
    "\n",
    "        # Handle baseline.\n",
    "        if isinstance(baseline, nn.Module):\n",
    "            with torch.no_grad():\n",
    "                target = returns - baseline(torch.stack(observations))\n",
    "        elif baseline == 'std':                                       #Standardize returns\n",
    "            target = (returns - returns.mean()) / returns.std()\n",
    "        else:\n",
    "            target = returns\n",
    "\n",
    "        # Make an optimization step\n",
    "        opt.zero_grad()\n",
    "        loss = (-log_probs * target).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Collect each episode length and training loss\n",
    "        all_episodes_lenghts.append(len(returns))\n",
    "        training_losses.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "        # Log only the mean training loss, episode lenght and running reward of 10 episode to make the graphs cleaner\n",
    "        if episode % 10 == 0:\n",
    "            loss_policy_metric = {\"loss_policy\": np.mean(training_losses[-10:])}\n",
    "            episode_length_metric = {\"episode_length\": np.mean(all_episodes_lenghts[-10:])}\n",
    "            wandb.log({**loss_policy_metric, **episode_length_metric}, commit = False)\n",
    "\n",
    "        # Update baseline network.\n",
    "        if isinstance(baseline, nn.Module):\n",
    "            opt_baseline.zero_grad()\n",
    "            loss_baseline = ((returns - baseline(torch.stack(observations)))**2.0).mean()\n",
    "            loss_baseline.backward()\n",
    "            opt_baseline.step()\n",
    "            value_losses.append(loss_baseline.detach().cpu().numpy())\n",
    "            if episode % 10 == 0:\n",
    "                loss_value_metric = {\"loss_value\": np.mean(value_losses[-10:])}\n",
    "                wandb.log({**loss_value_metric}, commit = False)\n",
    "\n",
    "        # Render and evaluate the current policy after every \"eval_every\" policy updates.\n",
    "        if episode % eval_every == 0:\n",
    "            episode_runner.policy.eval()\n",
    "            total_reward = 0\n",
    "            episode_lengths = []\n",
    "            #evaluate on \"eval_episodes\" episodes the total reward and the episodes length\n",
    "            for _ in range(eval_episodes):\n",
    "                (_, _, _, rewards) = episode_runner.run_episode()\n",
    "                total_reward += np.sum(rewards)\n",
    "                episode_lengths.append(len(rewards))\n",
    "            average_episode_len_metric = {\"average_episode_length\": np.mean(episode_lengths)}\n",
    "            average_rewards_metric = {\"average_total_reward\": total_reward / eval_episodes}\n",
    "            wandb.log({**average_rewards_metric, **average_episode_len_metric}, commit = False)\n",
    "            # Update best policy, the one with the best average total reward at testing time\n",
    "            if  total_reward / eval_episodes >= best_avg_tot_rew:\n",
    "                best_avg_tot_rew = total_reward / eval_episodes\n",
    "                # save all the parameters of the best policy\n",
    "                best_model_state_dict = episode_runner.policy.state_dict()\n",
    "            if display:\n",
    "                (obs, _, _, _) = episode_runner_render.run_episode()\n",
    "            episode_runner.policy.train()\n",
    "            print(f'Running reward of episode {episode}/{num_episodes}: {running_rewards[-1]}')\n",
    "            print(f'Average Total reward: {total_reward / eval_episodes}')\n",
    "        \n",
    "        wandb.log({**running_rewards_metric})\n",
    "\n",
    "    # Lastly, calculate and print the average episode lenght of the entire training\n",
    "    print(f'Average length of all episodes: {np.mean(all_episodes_lenghts)}')\n",
    "    average_all_episodes_metric= {\"average_lenght_all_episodes\": np.mean(all_episodes_lenghts)}\n",
    "    wandb.log({**average_all_episodes_metric})\n",
    "    \n",
    "    episode_runner.policy.eval()\n",
    "    if isinstance(baseline, nn.Module):\n",
    "        baseline.eval()\n",
    "    return best_model_state_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea05c3",
   "metadata": {},
   "source": [
    "# Standard runs\n",
    "In the following section I tested `REINFORCE` on the Cartpole environment using the standard baseline while changing the parameters (temperature and gamma).\n",
    "\n",
    "Since training the policy is stochastic, for each setting I performed the policy training 5 times with 5 different seeds to be able to replicate the experiments.\n",
    "\n",
    "At the end of this section there will be a wandb report with the results, for every setting the 5 runs are aggregated showing their mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mkTymBWNPESI",
   "metadata": {
    "id": "mkTymBWNPESI"
   },
   "source": [
    "## Standard run\n",
    "Base parameters, temperature = 1 and gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b440c-2b1d-42db-81f2-9bdd6d036d3a",
   "metadata": {
    "id": "378b440c-2b1d-42db-81f2-9bdd6d036d3a",
    "outputId": "396212df-359e-4c94-c804-398648e3f57b"
   },
   "outputs": [],
   "source": [
    "# Random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard Run \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 1,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    # Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    # Set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b48689-6e55-4f7d-aa56-c4ce6459e765",
   "metadata": {},
   "source": [
    "## Standard run and higher temperature (5)\n",
    "Higher temperature, temperature = 5 and gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef897b43-445f-467b-8e2e-8bd9e30fc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard and temperature 5 \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98a63e-5e66-4ecc-98b7-ce1341a1e81a",
   "metadata": {},
   "source": [
    "## Standard run and higher temperature (10)\n",
    "Even higher temperature, temperature = 10 and gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578621a9-c76a-46c9-8a80-9b9411c7ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard and temperature 10 \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 10,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e2878-19d2-410b-bb75-e468082ad22b",
   "metadata": {},
   "source": [
    "## Standard run and lower gamma\n",
    "Higher temperature (since it achieved better results) and lower gamma, temperature = 5 and gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e338ef-b678-433f-8404-3ab28b34b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard and gamma 0.9 \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.9,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915a9b8",
   "metadata": {},
   "source": [
    "## Standard Runs Report\n",
    "This is the public report created with weights and biases: https://api.wandb.ai/links/marcouni/c8d6xyuv\n",
    "\n",
    "It is quite easy to see that the best performance is obtained when the temperature is 5 and gamma is 0.99. Its average total reward is almost always the highest and it is extremely fast to converge (highest average length of all episodes) to a very stable perfect policy (always able to reach the 500th step in each episode). Its test average reward is perfect, meaning that the final policy is one of the possible best policies for this environment.\n",
    "\n",
    "The standard run and the one with temperature = 10 are not that different, except that the latter seems to be a little slower to converge (lower average length of all episodes) but is more stable (perfect test average total reward).\n",
    "\n",
    "It is also clear that the worst run is the one with a lower gamma, being the worst by every metric. This result shows the importance of future steps (through the importance of future rewards) in Cartpole. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5adad7-759b-4000-925b-701f41fe6e97",
   "metadata": {
    "id": "2a5adad7-759b-4000-925b-701f41fe6e97"
   },
   "source": [
    "# Exercise 2: `REINFORCE` with a Value Baseline (warm up)\n",
    "\n",
    "In this exercise we will augment my implementation (or your own) of `REINFORCE` to subtract a baseline from the target in the update equation in order to stabilize (and hopefully speed-up) convergence. For now we will stick to the Cartpole environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07",
   "metadata": {
    "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07"
   },
   "source": [
    "**First Things First**: Recall from the slides on Deep Reinforcement Learning that we can **subtract** any function that doesn't depend on the current action from the q-value without changing the (maximum of our) objecttive function $J$:  \n",
    "\n",
    "$$ \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_{s} \\mu(s) \\sum_a \\left( q_{\\pi}(s, a) - b(s) \\right) \\nabla \\pi(a \\mid s, \\boldsymbol{\\theta}) $$\n",
    "\n",
    "In `REINFORCE` this means we can subtract from our target $G_t$:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - b(S_t)) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
    "\n",
    "Since we are only interested in the **maximum** of our objective, we can also **rescale** our target by any function that also doesn't depend on the action. A **simple baseline** which is even independent of the state -- that is, it is **constant** for each episode -- is to just **standardize rewards within the episode**. So, we **subtract** the average return and **divide** by the variance of returns:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha \\left(\\frac{G_t - \\bar{G}}{\\sigma_G}\\right) \\nabla  \\pi(A_t \\mid s, \\boldsymbol{\\theta}) $$\n",
    "\n",
    "This baseline is **already** implemented in my implementation of `REINFORCE`. Experiment with and without this standardization baseline and compare the performance. We are going to do something more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df",
   "metadata": {
    "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df"
   },
   "source": [
    "**The Real Exercise**: Standard practice is to use the state-value function $v(s)$ as a baseline. This is intuitively appealing -- we are more interested in updating out policy for returns that estimate the current **value** worse. Our new update becomes:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - \\tilde{v}(S_t \\mid \\mathbf{w})) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
    "\n",
    "where $\\tilde{v}(s \\mid \\mathbf{w})$ is a **deep neural network** with parameters $w$ that estimates $v_\\pi(s)$. What neural network? Typically, we use the **same** network architecture as that of the Policy.\n",
    "\n",
    "**Your Task**: Modify your implementation to fit a second, baseline network to estimate the value function and use it as **baseline**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed0e92",
   "metadata": {},
   "source": [
    "# Different baselines\n",
    "This section contains the results of the experiments with REINFORCE on Cartpole using different kinds of baselines (no baseline, standard baseline and a Value Net as a baseline).\n",
    "\n",
    "Since previously the best results were obtained with temperature = 5 and gamma = 0.99 these parameters will be used in all the following runs. I also experimented with changing the number of layers of the Policy and Value Nets.\n",
    "\n",
    "Once again, for each setting I performed the policy training 5 times with 5 different seeds to be able to replicate the experiments and at the end of this section there will be a wandb report with the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f97e3",
   "metadata": {},
   "source": [
    "## Without the baseline\n",
    "Since I already experimented with Reinforce with the standard baseline the next run will not cointain a baseline of any kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76680bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Without the baseline \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": None,\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ad67f-180b-4009-a6e7-752fe1e4bc58",
   "metadata": {},
   "source": [
    "## With Baseline network\n",
    "Using ValueNet, a simple Net with the same architecture as the Policy Net that has to learn to estimate the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6187b4f3-568a-49a5-93f6-cc7cccb8cc2f",
   "metadata": {
    "id": "6187b4f3-568a-49a5-93f6-cc7cccb8cc2f"
   },
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, env, hidden_layers=32):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], hidden_layers)\n",
    "        self.fc2 = nn.Linear(hidden_layers, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = self.fc2(s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f",
   "metadata": {
    "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f"
   },
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Baseline Network \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f186e88-9437-4453-b4f8-27c0d3c8e92f",
   "metadata": {},
   "source": [
    "## With baseline, 128 hidden layer\n",
    "Adding more hidden layers to the ValueNet and PolicyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22488e84-fadd-44e7-b9a8-4d3ba72f0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Baseline Network 128 layers temp 5\",\n",
    "          config={\n",
    "              \"hidden_layers\": 128,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac2ae4a-f4e8-432d-aa66-fa50cc891a8a",
   "metadata": {},
   "source": [
    "## With baseline, 16 hidden layer\n",
    "Lowering the number of hidden layers of the ValueNet and PolicyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0652b1-5cbf-4cc1-8e67-9d80ab45be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Baseline Network 16 layers temp 5\",\n",
    "          config={\n",
    "              \"hidden_layers\": 16,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c1e91",
   "metadata": {},
   "source": [
    "## Baselines Runs Report\n",
    "This is the public report created with weights and biases: https://api.wandb.ai/links/marcouni/0ksdyjpo\n",
    "\n",
    "Once again the best setting seems to be the one with a standard baseline and temperature = 5, closely followed by the settings with baseline networks with 32 (standard) and 128 hidden layers (more layers achieve a slightly better result). Given these results using a standard baseline and temperature = 5 seems like the best case scenario for Cartpole.\n",
    "This is probably due to the simplicity (the standardized mean of the discounted return seems like a very good approximation of the value function) of the problem since its convergence is incredibly fast and stable. \n",
    "\n",
    "Using a ValueNet also leads to a stable training and a perfect policy (the test average total reward is perfect with 32 and 128 hidden layers) but seems to slow down the convergence as it needs more time to learn to approximate the value function. \n",
    "\n",
    "When the layers of the two Networks are not enough (16 hidden layers) they are unable to correctly approximate the value function and the policy.\n",
    "\n",
    "As expected the setting with no baseline produces the worst results, the runs are much less stable, as it can be seen in average total reward and average length all episodes. The final policy is also extremely unstable, it can be perfect or horrible ( test average total reward has a very high variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc5eec-9ebc-4867-b698-cfefbe6387d8",
   "metadata": {},
   "source": [
    " # Longer runs\n",
    " Lastly I also experiments with much longer runs with two of the best settings, with standard baseline and ValueNet, to see if their performance gap was due to the number of training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a52700c-e8cd-4048-b567-bb885b9eea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Long Standard\",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 5000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 50,\n",
    "              \"test_episodes\" : 500,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf6d680-b49d-4b82-81e8-fb431e225e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Long Baseline Network \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 5000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 50,\n",
    "              \"test_episodes\" : 500,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c664f9",
   "metadata": {},
   "source": [
    "## Longer Runs Report\n",
    "This is the public report created with weights and biases: https://api.wandb.ai/links/marcouni/r8dqdyc5\n",
    "\n",
    "It is clear that increasing the number of training episodes deteriorates both settings' performances, making the training more unstable and the final policy less good. The one that suffers a worse decline in reward is the one with ValueNet, probably due to overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf1447-d222-4b24-a357-5b7f9824390c",
   "metadata": {
    "id": "64bf1447-d222-4b24-a357-5b7f9824390c"
   },
   "source": [
    "-----\n",
    "# Exercise 3: Going Deeper\n",
    "\n",
    "As usual, pick **AT LEAST ONE** of the following exercises to complete.\n",
    "\n",
    "### Exercise 3.1: Solving Lunar Lander with `REINFORCE` (easy)\n",
    "\n",
    "Use my (or even better, improve on my) implementation of `REINFORCE` to solve the [Lunar Lander Environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/). This environment is a little bit harder than Cartpole, but not much. Make sure you perform the same types of analyses we did during the lab session to quantify and qualify the performance of your agents.\n",
    "\n",
    "### Exercise 3.2: Solving Cartpole and Lunar Lander with `Deep Q-Learning` (harder)\n",
    "\n",
    "On policy Deep Reinforcement Learning tends to be **very unstable**. Write an implementation (or adapt an existing one) of `Deep Q-Learning` to solve our two environments (Cartpole and Lunar Lander). To do this you will need to implement a **Replay Buffer** and use a second, slow-moving **target Q-Network** to stabilize learning.\n",
    "\n",
    "### Exercise 3.3: Solving the OpenAI CarRacing environment (hardest)\n",
    "\n",
    "Use `Deep Q-Learning` -- or even better, an off-the-shelf implementation of **Proximal Policy Optimization (PPO)** -- to train an agent to solve the [OpenAI CarRacing](https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN) environment. This will be the most *fun*, but also the most *difficult*. Some tips:\n",
    "\n",
    "1. Make sure you use the `continuous=False` argument to the environment constructor. This ensures that the action space is **discrete** (we haven't seen how to work with continuous action spaces).\n",
    "2. Your Q-Network will need to be a CNN. A simple one should do, with two convolutional + maxpool layers, folowed by a two dense layers. You will **definitely** want to use a GPU to train your agents.\n",
    "3. The observation space of the environment is a single **color image** (a single frame of the game). Most implementations stack multiple frames (e.g. 3) after converting them to grayscale images as an observation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d99e29",
   "metadata": {},
   "source": [
    "## CarRacing-v2 environment\n",
    "A top-down racing environment with a randomly generated track. The car starts at rest in the center of the road, and its goal is to visit as many tiles as it can in the least amount of time possible. The reward is -0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles visited in the track and 1000 is the maximum number of actions it can take. The episode can also terminate if the car terminates the track or goes outside the playfield, far off the track, in which case it will receive -100 reward and die. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points. \n",
    "\n",
    "There are 5 possible actions in the discrete action space: do nothing, steer left, steer right, gas, brake. A state consists of 96x96 pixels frame. Given a stack of frames, the car decides which action to take."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a79147",
   "metadata": {},
   "source": [
    "The raw frames are preprocessed by cropping an 84  84 region of the image that roughly captures the playing area (every frame always contains a black area at the bottom of the frame, so we cut that black area) and converting their RGB representation to gray-scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb9fe414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess(img):\n",
    "    img = img[:84, 6:90] # CarRacing-v2-specific cropping\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd508d6c",
   "metadata": {},
   "source": [
    "The game screen gradually zooms in for the first 50 steps, thus, we will not use the first 50 steps of the game.\n",
    "\n",
    "The preprocessing is applied to the last 4 frames, they are stacked to produce the input to the Q-function.\n",
    "\n",
    "A simple frame-skipping technique is used: the agent sees and selects actions on every kth frame instead of every frame and its last action is repeated on skipped frames. Running the emulator forward for one step requires much less computation than having the agent select an action, this technique allows the agent to play roughly k times more games without significantly increasing the runtime.\n",
    "\n",
    "ImageEnv is a gymnasium wrapper that redefines reset() and step() following the previously defined changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "206ce119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEnv(gymnasium.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        skip_frames=4,         # How many frames are skipped\n",
    "        stack_frames=4,        # How many frames to stack\n",
    "        initial_no_op=50,      # No actions during the zoom in\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ImageEnv, self).__init__(env, **kwargs)\n",
    "        self.initial_no_op = initial_no_op\n",
    "        self.skip_frames = skip_frames\n",
    "        self.stack_frames = stack_frames\n",
    "    \n",
    "    def reset(self, seed = None, options = None ):\n",
    "        # Reset the original environment.\n",
    "        state, info = self.env.reset(seed= seed, options=options )\n",
    "\n",
    "        # Do nothing for the first `self.initial_no_op` steps, zoom in\n",
    "        for _ in range(self.initial_no_op):\n",
    "            state, _, _, _, info = self.env.step(0)   # action 0 : do nothing\n",
    "        \n",
    "        # Convert a frame to 84 X 84 gray scale one\n",
    "        state = preprocess(state)\n",
    "\n",
    "        # The initial observation is simply a copy of the frame `s`, np.tile repeats s self.stack_frames times\n",
    "        self.stacked_state = np.tile(state, (self.stack_frames, 1, 1))  # [4, 84, 84]\n",
    "        return self.stacked_state, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # We take the same action for self.skip_frames steps\n",
    "        reward = 0\n",
    "        for _ in range(self.skip_frames):\n",
    "            state, r, terminated, truncated, info = self.env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        # Convert a frame to 84 X 84 gray scale one\n",
    "        state = preprocess(state)\n",
    "\n",
    "        # Push the current frame `state` at the end of self.stacked_state while removing the oldest frame\n",
    "        self.stacked_state = np.concatenate((self.stacked_state[1:], state[np.newaxis]), axis=0)\n",
    "\n",
    "        return self.stacked_state, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce29bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of an observation:  (4, 84, 84)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Discrete action space: do nothing, steer left, steer right, gas, brake.\n",
    "env = gymnasium.make('CarRacing-v2', continuous=False)\n",
    "env = ImageEnv(env)\n",
    "\n",
    "# Check reset\n",
    "state, _ = env.reset()\n",
    "print(\"The shape of an observation: \", state.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for i in range(4):\n",
    "    axes[i].imshow(state[i], cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0497fa6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check step, gas action for 4 steps, check moving background\n",
    "for i in range(4):\n",
    "    state, r, terminated, truncated, info = env.step(3)  # 3rd action is `gas` action\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for i in range(4):\n",
    "    axes[i].imshow(state[i], cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ccdfb",
   "metadata": {},
   "source": [
    "## DQN\n",
    "A Neural Network that takes in n_observations frames and tries to predict the expected return of taking each possible action given the current input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62abd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NetDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(NetDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_observations, 16, kernel_size=7, stride=3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4)\n",
    "        # Image 5*5 after convolutions and max pooling\n",
    "        self.fc1 = nn.Linear(32*5*5, 256)  \n",
    "        self.fc2 = nn.Linear(256, n_actions)   \n",
    "        self.max_pool= nn.MaxPool2d(kernel_size= (2,2))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.max_pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477c8877",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "It stores the transitions (state, action, next state, reward) that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "682bf902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e5)):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0     # Pointer\n",
    "\t\tself.size = 0    # Current size\n",
    "\n",
    "\t\tself.state = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "\t\tself.action = np.zeros((max_size, *action_dim), dtype=np.int64)\n",
    "\t\tself.next_state = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "\t\tself.reward = np.zeros((max_size, 1), dtype=np.float32)\n",
    "\t\tself.terminated = np.zeros((max_size, 1), dtype=np.float32)\n",
    "\n",
    "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\tdef add(self, state, action, next_state, reward, terminated):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.terminated[self.ptr] = terminated\n",
    "\n",
    "        # Update pointer and current size\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size  \n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\t# Collect batch_size random integers from 0 to current size \n",
    "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.terminated[ind]).to(self.device)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04df5ab",
   "metadata": {},
   "source": [
    "## DQNAgent\n",
    "DQN agent:\n",
    "\n",
    "- act() takes as input one state and outputs an action by following an epsilon-greedy policy.\n",
    "\n",
    "- learn() samples a batch from replay buffer and trains the NetDQN with MSE loss using temporal difference, Deep Q Leaning.\n",
    "\n",
    "- process() takes one transition (state, action, next state, reward) as input and decides if the agent can learn (after the warmup) and if the target network has to be updated, it also decays the current epsilon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dbbc9a",
   "metadata": {},
   "source": [
    "![Deep Q Learning](Images/DeepQLearning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b731ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=0.0005,\n",
    "        epsilon=1.0,\n",
    "        epsilon_min=0.1,\n",
    "        gamma=0.99,\n",
    "        batch_size=64,\n",
    "        warmup_steps=2500,\n",
    "        buffer_size=int(1e5),\n",
    "        target_update_interval=5000,\n",
    "    ):\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.target_update_interval = target_update_interval   # After how many steps update target net\n",
    "\n",
    "        self.network = NetDQN(state_dim[0], action_dim)\n",
    "        self.target_network = NetDQN(state_dim[0], action_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr)\n",
    "\n",
    "        self.buffer = ReplayBuffer(state_dim, (1, ), buffer_size)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "        self.network.to(self.device)\n",
    "        self.target_network.to(self.device)\n",
    "        \n",
    "        self.total_steps = 0\n",
    "        self.epsilon_decay = (epsilon - epsilon_min) / 5e5\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def act(self, state, training=True):\n",
    "        # Set network to train() mode if training\n",
    "        self.network.train(training)\n",
    "        # Random action if training and if randomly chosen with epsilon or we are in the first warmup steps\n",
    "        if training and ((np.random.rand() < self.epsilon) or (self.total_steps < self.warmup_steps)):\n",
    "            action = np.random.randint(0, self.action_dim)\n",
    "        # Follow the current policy, select best action given the approximated q value\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "            qvalue = self.network(state)\n",
    "            action = torch.argmax(qvalue).item()\n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        # Sample batch size transitions from replay buffer \n",
    "        state, action, next_state, reward, terminated = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Calculated with the target network, detach: do not train the target net\n",
    "        next_qvalue = self.target_network(next_state).detach()\n",
    "        temporal_difference_target = reward + (1. - terminated) * self.gamma * next_qvalue.max(dim=1, keepdim=True).values\n",
    "        # MSE loss between current q value (gather q value that corresponds to action) and temporal difference of the target\n",
    "        loss = F.mse_loss(self.network(state).gather(1, action.long()), temporal_difference_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        result = {\n",
    "            'reward': reward.cpu(),\n",
    "            'training_loss': loss.item()\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process(self, transition):\n",
    "        result = {}\n",
    "        self.total_steps += 1\n",
    "        self.buffer.add(*transition)\n",
    "        \n",
    "        # Start learning after the warmup\n",
    "        if self.total_steps > self.warmup_steps:\n",
    "            result = self.learn()\n",
    "        \n",
    "        # Update the target network\n",
    "        if self.total_steps % self.target_update_interval == 0:\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "        # Epsilon decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93672c",
   "metadata": {},
   "source": [
    "## Training the DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b055cd9",
   "metadata": {},
   "source": [
    "To evaluate an agent, let it play n_evals games and collect its mean performance : mean score and episode length. The maximum episode length is 238 since each step takes 4 actions and the first 50 frames are skipped (action = do nothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a567ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the agent \n",
    "def evaluate(eval_env, agent, n_evals=5):\n",
    "    scores = 0\n",
    "    episode_length = 0\n",
    "    # Play n_evals games until done and store their avg score and avg length\n",
    "    for _ in range(n_evals):\n",
    "        (state, _), done, ret = eval_env.reset(), False, 0\n",
    "        while not done:\n",
    "            action = agent.act(state, training=False)\n",
    "            next_state, reward, terminated, truncated, info = eval_env.step(action)\n",
    "            state = next_state\n",
    "            ret += reward\n",
    "            episode_length += 1\n",
    "            done = terminated or truncated\n",
    "        scores += ret\n",
    "    result = {\n",
    "            'scores': np.round(scores / n_evals, 4),\n",
    "            'average_length': np.round(episode_length / n_evals, 4)\n",
    "        }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b68ddab",
   "metadata": {},
   "source": [
    "Train the DQN Agent for 1 million steps and evaluate it every 5000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5913b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "run=wandb.init(\n",
    "          project=\"Lab3-DRL-DQN_Agent\",\n",
    "          name = \"Training DQN\",\n",
    "          config={\n",
    "            \"lr\" : 0.0005,\n",
    "            \"epsilon\" : 1.0,\n",
    "            \"epsilon_min\" : 0.1,\n",
    "            \"gamma\" : 0.99,\n",
    "            \"batch_size\" : 64,\n",
    "            \"warmup_steps\" : 2500,\n",
    "            \"buffer_size\" :int(1e5),\n",
    "            \"target_update_interval\" : 5000,\n",
    "            \"max_steps\": int(1e6),\n",
    "            \"eval_interval\" : 5000,\n",
    "              })\n",
    "    \n",
    "# Copy the configuration\n",
    "config = wandb.config\n",
    "\n",
    "# Create environments to train and evaluate\n",
    "env = gymnasium.make('CarRacing-v2', continuous=False)\n",
    "env = ImageEnv(env)\n",
    "\n",
    "eval_env = gymnasium.make('CarRacing-v2', continuous=False)\n",
    "eval_env = ImageEnv(eval_env)\n",
    "\n",
    "# General definitions\n",
    "state_dim = (4, 84, 84)\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_dim, action_dim, config.lr, config.epsilon, config.epsilon_min, config.gamma, config.batch_size,\n",
    "                 config.warmup_steps, config.buffer_size, config.target_update_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fde452",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "# To save the best agent, the one with the best eval avg score\n",
    "best_avg_score = -100\n",
    "losses = []\n",
    "rewards = []\n",
    "\n",
    "negative_reward_counter = 0\n",
    "time_frame_counter = 1\n",
    "while True:\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    result = agent.process((state, action, next_state, reward, terminated))  \n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    # If continually getting negative reward 25 times after the tolerance steps, terminate this episode\n",
    "    negative_reward_counter = negative_reward_counter + 1 if time_frame_counter > 100 and reward < 0 else 0\n",
    "    time_frame_counter += 1\n",
    "\n",
    "    if terminated or truncated or negative_reward_counter >= 25:\n",
    "        state, _ = env.reset()\n",
    "        negative_reward_counter = 0\n",
    "        time_frame_counter = 1\n",
    "        \n",
    "    if agent.total_steps % config.eval_interval == 0:\n",
    "        ret = evaluate(eval_env, agent)\n",
    "\n",
    "        scores_metric = {\"Average Eval Scores\": ret['scores']}\n",
    "        episode_length_metric = {\"Average Episode Length\": ret['average_length']}\n",
    "        wandb.log({**scores_metric, **episode_length_metric}, commit = False)\n",
    "        \n",
    "        # Save the best agent\n",
    "        if best_avg_score < ret['scores']:\n",
    "            best_avg_score = ret['scores']\n",
    "            # Save all the parameters of the model\n",
    "            best_model_state_dict = agent.network.state_dict()\n",
    "\n",
    "    # Wait the warmup_steps\n",
    "    if agent.total_steps > agent.warmup_steps:        \n",
    "      # Accumulate losses and reward\n",
    "      losses.append(result[\"training_loss\"])\n",
    "      rewards.append(result[\"reward\"])\n",
    "      # Only save avg losses and rewards for more readable graphs\n",
    "      if agent.total_steps % 100 == 0:\n",
    "          loss_metric = {\"Trainin Loss\": np.mean(losses[-100:])}\n",
    "          training_reward_metric = {\"Training Reward\": np.mean(rewards[-100:])}\n",
    "          wandb.log({**loss_metric, **training_reward_metric})\n",
    "          print(f'Avg loss at episode {agent.total_steps}/{config.max_steps}: {np.mean(losses[-100:])}')\n",
    "          print(f'Avg reward at episode {agent.total_steps}/{config.max_steps}: {np.mean(rewards[-100:])}')\n",
    "\n",
    "    # Save the best model weights and biases as an artifact every 100000 steps\n",
    "    if agent.total_steps % 100000 == 0:\n",
    "        # Save the best model on weights and biases as an artifact\n",
    "        model_artifact = wandb.Artifact(\n",
    "                    \"DQN_Agent\"+ str(agent.total_steps) , type=\"model\",\n",
    "                    description=\"best model for Lab3-DRL\" + str(agent.total_steps) ,\n",
    "                    metadata=dict(config))\n",
    "\n",
    "        torch.save(best_model_state_dict, \"best_model.pth\")\n",
    "        model_artifact.add_file(\"best_model.pth\")\n",
    "        wandb.save(\"best_model.pth\")\n",
    "        run.log_artifact(model_artifact)\n",
    "\n",
    "    # After all the steps save the best model and break\n",
    "    if agent.total_steps > config.max_steps:\n",
    "        # Load saved weights of the best model\n",
    "        agent.network.load_state_dict(best_model_state_dict)\n",
    "\n",
    "        # Save the best model on weights and biases as an artifact\n",
    "        model_artifact = wandb.Artifact(\n",
    "                    \"DQN_Agent_final\" , type=\"model\",\n",
    "                    description=\"best model for Lab3-DRL\",\n",
    "                    metadata=dict(config))\n",
    "\n",
    "        torch.save(agent.network.state_dict(), \"best_model.pth\")\n",
    "        model_artifact.add_file(\"best_model.pth\")\n",
    "        wandb.save(\"best_model.pth\")\n",
    "        run.log_artifact(model_artifact)\n",
    "         # Close wandb run\n",
    "        wandb.finish()\n",
    "        env.close()\n",
    "        eval_env.close()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea0254",
   "metadata": {},
   "source": [
    "Since I was unable to complete the whole training in a single step (the vpn needed to enable the connection to the papavero server is a free version that does not allow a connection longer than 7 hours while the full training needed at least 10 hours) I decided to load the best model saved in the first part of the training (already achieving good results) to continue training.\n",
    "\n",
    "Obviously this is not the same as a single training of all the steps since the Replay Buffer was reset, that's why I added 50000 extra steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baed8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training\n",
    "run=wandb.init(\n",
    "          project=\"Lab3-DRL-DQN_Agent\",\n",
    "          name = \"Training DQN part 2\",\n",
    "          config={\n",
    "            \"lr\" : 0.0005,\n",
    "            \"epsilon\" : 0.1001,  \n",
    "            \"epsilon_min\" : 0.0999,\n",
    "            \"gamma\" : 0.99,\n",
    "            \"batch_size\" : 64,\n",
    "            \"warmup_steps\" : 0,\n",
    "            \"buffer_size\" :int(1e5),\n",
    "            \"target_update_interval\" : 5000,\n",
    "            \"max_steps\": 450000,\n",
    "            \"eval_interval\" : 5000,\n",
    "              })\n",
    "    \n",
    "# Copy the configuration\n",
    "config = wandb.config\n",
    "\n",
    "# Create environments to train and evaluate\n",
    "env = gymnasium.make('CarRacing-v2', continuous=False)\n",
    "env = ImageEnv(env)\n",
    "\n",
    "eval_env = gymnasium.make('CarRacing-v2', continuous=False)\n",
    "eval_env = ImageEnv(eval_env)\n",
    "\n",
    "# General definitions\n",
    "state_dim = (4, 84, 84)\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_dim, action_dim, config.lr, config.epsilon, config.epsilon_min, config.gamma, config.batch_size,\n",
    "                 config.warmup_steps, config.buffer_size, config.target_update_interval)\n",
    "\n",
    "# Load the previous model, before the interruption\n",
    "artifact = run.use_artifact('Lab3-DRL-DQN_Agent/DQN_Agent600000:v0', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "agent.network.load_state_dict(torch.load(artifact_dir+'/best_model.pth'))\n",
    "agent.target_network.load_state_dict(torch.load(artifact_dir+'/best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "# To save the best agent, the one with the best eval avg score\n",
    "best_avg_score = 0\n",
    "losses = []\n",
    "rewards = []\n",
    "\n",
    "negative_reward_counter = 0\n",
    "time_frame_counter = 1\n",
    "while True:\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    result = agent.process((state, action, next_state, reward, terminated))  \n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    # If continually getting negative reward 25 times after the tolerance steps, terminate this episode\n",
    "    negative_reward_counter = negative_reward_counter + 1 if time_frame_counter > 100 and reward < 0 else 0\n",
    "    time_frame_counter += 1\n",
    "\n",
    "    if terminated or truncated or negative_reward_counter >= 25:\n",
    "        state, _ = env.reset()\n",
    "        negative_reward_counter = 0\n",
    "        time_frame_counter = 1\n",
    "        \n",
    "    if agent.total_steps % config.eval_interval == 0:\n",
    "        ret = evaluate(eval_env, agent)\n",
    "\n",
    "        scores_metric = {\"Average Eval Scores\": ret['scores']}\n",
    "        episode_length_metric = {\"Average Episode Length\": ret['average_length']}\n",
    "        wandb.log({**scores_metric, **episode_length_metric}, commit = False)\n",
    "        \n",
    "        # Save the best agent\n",
    "        if best_avg_score < ret['scores']:\n",
    "            best_avg_score = ret['scores']\n",
    "            # Save all the parameters of the model\n",
    "            best_model_state_dict = agent.network.state_dict()\n",
    "\n",
    "    # Wait the warmup_steps\n",
    "    if agent.total_steps > agent.warmup_steps:        \n",
    "      # Accumulate losses and reward\n",
    "      losses.append(result[\"training_loss\"])\n",
    "      rewards.append(result[\"reward\"])\n",
    "      # Only save avg losses and rewards for more readable graphs\n",
    "      if agent.total_steps % 100 == 0:\n",
    "          loss_metric = {\"Trainin Loss\": np.mean(losses[-100:])}\n",
    "          training_reward_metric = {\"Training Reward\": np.mean(rewards[-100:])}\n",
    "          wandb.log({**loss_metric, **training_reward_metric})\n",
    "          print(f'Avg loss at episode {agent.total_steps}/{config.max_steps}: {np.mean(losses[-100:])}')\n",
    "          print(f'Avg reward at episode {agent.total_steps}/{config.max_steps}: {np.mean(rewards[-100:])}')\n",
    "\n",
    "    # Save the best model weights and biases as an artifact every 100000 steps\n",
    "    if agent.total_steps % 100000 == 0:\n",
    "        # Save the best model on weights and biases as an artifact\n",
    "        model_artifact = wandb.Artifact(\n",
    "                    \"DQN_Agent\"+ str(agent.total_steps+600000) , type=\"model\",\n",
    "                    description=\"best model for Lab3-DRL\" + str(agent.total_steps+600000) ,\n",
    "                    metadata=dict(config))\n",
    "\n",
    "        torch.save(best_model_state_dict, \"best_model.pth\")\n",
    "        model_artifact.add_file(\"best_model.pth\")\n",
    "        wandb.save(\"best_model.pth\")\n",
    "        run.log_artifact(model_artifact)\n",
    "\n",
    "    # After all the steps save the best model and break\n",
    "    if agent.total_steps > config.max_steps:\n",
    "        # Load saved weights of the best model\n",
    "        agent.network.load_state_dict(best_model_state_dict)\n",
    "\n",
    "        # Save the best model on weights and biases as an artifact\n",
    "        model_artifact = wandb.Artifact(\n",
    "                    \"DQN_Agent_final\" , type=\"model\",\n",
    "                    description=\"best model for Lab3-DRL\",\n",
    "                    metadata=dict(config))\n",
    "\n",
    "        torch.save(agent.network.state_dict(), \"best_model.pth\")\n",
    "        model_artifact.add_file(\"best_model.pth\")\n",
    "        wandb.save(\"best_model.pth\")\n",
    "        run.log_artifact(model_artifact)\n",
    "         # Close wandb run\n",
    "        wandb.finish()\n",
    "        env.close()\n",
    "        eval_env.close()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09453692",
   "metadata": {},
   "source": [
    "### Training DQN Agent Report\n",
    "\n",
    "This is the public report created with weights and biases: https://api.wandb.ai/links/marcouni/jmwlyywk\n",
    "\n",
    "The first training starts with negative training rewards and average evaluation scores, the car is not staying on track or is not moving. The beginning of this training is more focused on exploration, with a higher epsilon.\n",
    "\n",
    "After enough steps, with the constant decay of epsilon, the car is able to take better actions, learning from the collected transitions and improving its performances. The peak average evaluation score is 828 in the first part of training, which is a good result considering that the best possible score is 1000-0.1*(number of frames to finish the randomly generated track + 50 \"do nothing\" actions at the start) and that the car can only choose 1 action at a time (discrete action space).\n",
    "\n",
    "After the interruption, the agent starts with a good performance, but the training reward is a little lower than the one at the end of the first part of training. This is probably due to the reset of the Replay Buffer that needs to collect all the transitions from the start.\n",
    "\n",
    "The second part of the training does seem to have a better average evaluation score, with a peak of 887, meaning that it did improve the agent's performances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb95146",
   "metadata": {},
   "source": [
    "## Final evaluation\n",
    "Evaluate the best agents found before and after the training interruption (after 600 000 steps of the original training).\n",
    "\n",
    "After loading the best agents, use a test environment (wrapped in a video recorder) to evaluate it on 500 episodes. Both agents will be tested on the same tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03656a3",
   "metadata": {},
   "source": [
    "Before interruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b2ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# Get the best agent to test\n",
    "run = wandb.init(project=\"Lab3-DRL-DQN_Agent\",\n",
    "          name = \"Test DQN Agent Before\",\n",
    "          config={\n",
    "            \"test_episodes\" : 500,\n",
    "          }\n",
    "          )\n",
    "config = wandb.config\n",
    "\n",
    "artifact = run.use_artifact('Lab3-DRL-DQN_Agent/DQN_Agent600000:v0', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "state_dim = (4, 84, 84)\n",
    "action_dim = 5\n",
    "\n",
    "# Most parameters are not used when evaluated, they do not matter\n",
    "agent = DQNAgent(state_dim, action_dim, 0.0005, 1, 0.1, 0.99, 34, 2500, int(1e5), 5000)\n",
    "agent.network.load_state_dict(torch.load(artifact_dir+'/best_model.pth'))\n",
    "\n",
    "# Test environment\n",
    "test_env = gymnasium.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "test_env = ImageEnv(test_env)\n",
    "\n",
    "# Wrap the env in the record video \n",
    "recorder = gymnasium.wrappers.RecordVideo(env=test_env, video_folder=\"/data01/dl24marchi/DLAProjects/Videos\", name_prefix=\"Test DRL before\", episode_trigger=lambda x: x)\n",
    "recorder.metadata['render_fps'] = 10\n",
    "# To test both agents on the same tracks\n",
    "recorder.reset(seed = 111)\n",
    "\n",
    "recorder.start_video_recorder()\n",
    "scores = 0\n",
    "for i in range(config.test_episodes):\n",
    "  # Reset environment and return\n",
    "  (state, _), done, ret = recorder.reset(), False, 0\n",
    "  while not done:\n",
    "      # Take the best action, deterministic\n",
    "      action = agent.act(state, training=False)\n",
    "      next_state, reward, terminated, truncated, info = recorder.step(action)\n",
    "      state = next_state\n",
    "      # Collect the reward\n",
    "      ret += reward\n",
    "      done = terminated or truncated\n",
    "  # Collect all the scores of all the episodes\n",
    "  scores += ret\n",
    "  print(f'Test score {ret} episode : {i}')\n",
    "  testing_ret_metric = {\"Testing return\": ret}\n",
    "  wandb.log({**testing_ret_metric})\n",
    "# Average score of all the episodes\n",
    "testing_scores_metric = {\"Mean score\": np.round(scores/ config.test_episodes,3)}\n",
    "wandb.log({**testing_scores_metric})\n",
    "\n",
    "recorder.close_video_recorder()\n",
    "\n",
    "test_env.close()\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b03710",
   "metadata": {},
   "source": [
    "After interruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e029510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# Get the best agent after all the training to test\n",
    "run = wandb.init(project=\"Lab3-DRL-DQN_Agent\",\n",
    "          name = \"Test DQN Agent After\",\n",
    "          config={\n",
    "            \"test_episodes\" : 500,\n",
    "          }\n",
    "          )\n",
    "config = wandb.config\n",
    "\n",
    "artifact = run.use_artifact('Lab3-DRL-DQN_Agent/DQN_Agent_final:v0', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "state_dim = (4, 84, 84)\n",
    "action_dim = 5\n",
    "\n",
    "agent = DQNAgent(state_dim, action_dim, 0.0005, 1, 0.1, 0.99, 34, 2500, int(1e5), 5000)\n",
    "agent.network.load_state_dict(torch.load(artifact_dir+'/best_model.pth'))\n",
    "\n",
    "# Test environment\n",
    "test_env = gymnasium.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "test_env = ImageEnv(test_env)\n",
    "\n",
    "# Wrap the env in the record video \n",
    "recorder = gymnasium.wrappers.RecordVideo(env=test_env, video_folder=\"/data01/dl24marchi/DLAProjects/Videos\", name_prefix=\"Test DRL after\", episode_trigger=lambda x: x)\n",
    "recorder.metadata['render_fps'] = 10\n",
    "# To test both agents on the same tracks\n",
    "recorder.reset(seed = 111)\n",
    "\n",
    "recorder.start_video_recorder()\n",
    "scores = 0\n",
    "for i in range(config.test_episodes):\n",
    "  # Reset environment and return\n",
    "  (state, _), done, ret = recorder.reset(), False, 0\n",
    "  while not done:\n",
    "      # Take the best action, deterministic\n",
    "      action = agent.act(state, training=False)\n",
    "      next_state, reward, terminated, truncated, info = recorder.step(action)\n",
    "      state = next_state\n",
    "      # Collect the reward\n",
    "      ret += reward\n",
    "      done = terminated or truncated\n",
    "  # Collect all the scores of all the episodes\n",
    "  scores += ret\n",
    "  print(f'Test score {ret} episode : {i}')\n",
    "  testing_ret_metric = {\"Testing return\": ret}\n",
    "  wandb.log({**testing_ret_metric})\n",
    "# Average score of all the episodes\n",
    "testing_scores_metric = {\"Mean score\": np.round(scores/ config.test_episodes,3)}\n",
    "wandb.log({**testing_scores_metric})\n",
    "\n",
    "recorder.close_video_recorder()\n",
    "\n",
    "test_env.close()\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e352be",
   "metadata": {},
   "source": [
    "#### Final evaluation Report\n",
    "\n",
    "This is the public report created with weights and biases: https://api.wandb.ai/links/marcouni/wl302iub\n",
    "\n",
    "The first thing to note is that the final model does have a higher mean score than the best model before the interruption. The second part of the training was useful and improved the agent's driving ability. Both the mean scores are quite high, meaning that both agents are somewhat able to drive in most tracks.\n",
    "\n",
    "In the \"Selected Videos DRL\" folder there will be all the videos I will cite and some extra videos to show the agents' driving skills.\n",
    "\n",
    "Focusing on the single episodes, both agents reached max scores on episode 329 (that track seems quite simple, having the only difficult curve at the start when the car is going more slowly). The final agent achieved 930.331 while the other agent achieved 928.931.\n",
    "\n",
    "The worst episode for the final model is 359 where it achieved a score of 3.629 while the other agent achieved a score of 855. The final agent is too close to the left side of the road when approaching the first curve and is totally unable to recover even though the track itself is not that complex (the first turn is really similar to the one it was able to execute perfectly on episode 329), as proved by the fact that the \"before\" agent was able to achieve a good score.\n",
    "\n",
    "The worst episode for the first model is 17 where it achieved a score of 64.679 while the other agent achieved a score of 451.448. The track of this episode does look very complex and the first agent is unable to get back on the road after its mistake. The \"after\" agent achieves a better score by keep trying to get back on the road, even after going the wrong direction.\n",
    "\n",
    "Looking at the singular episode rewards, the final agent has both more high rewards and low rewards compared to the other, more stable, agent. Looking at the previous cases, the extra training on the final agent made it more greedy, driving more \"recklessly\" to achieve the better scores compared to the first model, which tends to achieve good scores in most tracks driving more \"safely\".\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
