{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc",
   "metadata": {
    "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc"
   },
   "source": [
    "# Deep Reinforcement Learning Laboratory\n",
    "\n",
    "This laboratory is focused on getting more advanced versions of Deep Reinforcement Learning algorithms up and running. Deep Reinforcement Learning is **hard**, and getting agents to stably train can be frustrating and requires quite a bit of subtlety in analysis of intermediate results. We will start by implementing `REINFORCE` on the [Cartpole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uldpcHx0OHyx",
   "metadata": {
    "id": "uldpcHx0OHyx"
   },
   "source": [
    "## Imports and weights and biases login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689f766-b099-4948-a1b2-37e92975178f",
   "metadata": {
    "id": "9689f766-b099-4948-a1b2-37e92975178f"
   },
   "outputs": [],
   "source": [
    "# Using weights and biases\n",
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241e9bdc-7aa9-4a12-a57f-088fdc87eab6",
   "metadata": {
    "id": "241e9bdc-7aa9-4a12-a57f-088fdc87eab6"
   },
   "outputs": [],
   "source": [
    "# Standard imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Plus one non standard one -- we need this to sample from policies.\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e806e9d4-bb04-40ed-9cd4-f016823b100c",
   "metadata": {
    "id": "e806e9d4-bb04-40ed-9cd4-f016823b100c",
    "outputId": "753a212b-355e-4133-f7e1-53e9b5895040"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarco-chisci\u001b[0m (\u001b[33mmarcouni\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login to weights and biases account\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f",
   "metadata": {
    "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f"
   },
   "source": [
    "# Exercise 1: `REINFORCE` Implementation (warm up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dk0EFuXUOpXz",
   "metadata": {
    "id": "Dk0EFuXUOpXz"
   },
   "source": [
    "## Policy Net\n",
    "A simple policy network with one hidden layer and a temperature parameter to smooth the output (the higher the temperature the smoother the output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ea9416-b07e-4354-bf36-1150394b0698",
   "metadata": {
    "id": "16ea9416-b07e-4354-bf36-1150394b0698"
   },
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, env, hidden_layers=32, temperature=1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], hidden_layers)\n",
    "        self.fc2 = nn.Linear(hidden_layers, env.action_space.n)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = F.softmax(self.fc2(s)/self.temperature, dim=-1)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uvEAh3bgOWTz",
   "metadata": {
    "id": "uvEAh3bgOWTz"
   },
   "source": [
    "## Episode Runner\n",
    "Class that runs the training episodes collecting all the useful info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd2dd9d-a60b-4892-878f-d83f64ac5f63",
   "metadata": {
    "id": "cbd2dd9d-a60b-4892-878f-d83f64ac5f63",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A class that , given an environment, a policy network and the max lenght of the episode is in charge\n",
    "# of running it\n",
    "class Episode_runner:\n",
    "    def __init__(self, env, policy, maxlen=500):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.maxlen= maxlen\n",
    "\n",
    "    # Given observation and the policy, sample from pi(a | obs). Returns the\n",
    "    # selected action and the log probability of that action (needed for policy gradient).\n",
    "    def select_action(self, obs):\n",
    "        dist = Categorical(self.policy(obs))\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return (action.item(), log_prob.reshape(1))\n",
    "\n",
    "    # Given the environment and the policy, run it up to the maximum number of steps.\n",
    "    def run_episode(self):\n",
    "        # Collect just about everything.\n",
    "        observations = []\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "\n",
    "        # Reset the environment and start the episode.\n",
    "        (obs, info) = self.env.reset() \n",
    "        for i in range(self.maxlen):\n",
    "            # Get the current observation, run the policy and select an action.\n",
    "            obs = torch.tensor(obs)\n",
    "            (action, log_prob) = self.select_action(obs)\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "            # Advance the episode by executing the selected action.\n",
    "            (obs, reward, term, trunc, info) = self.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if term or trunc:\n",
    "                break\n",
    "        return (observations, actions, torch.cat(log_probs), rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc5787-1c38-4f5c-b716-e40e900d00d2",
   "metadata": {},
   "source": [
    "## Deterministic Episode Runner\n",
    "Class that runs the testing episodes. The main difference with the training Episode Runner is that the action selected is always the one with the maximum probability, hence why it's called deterministic.\n",
    "\n",
    "To test the current best policy, the total average reward and episode length is calculated using the best policy in a deterministic manner on test_episodes episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f505e44c-0d5e-48f1-b874-d318424b3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class that , given an episode runner (with an environment and a policy network), an episode render to show the policy \n",
    "# and the max lenght of each episode evaluates the quality of the learned policy network, \n",
    "# always selecting the action with max probability \n",
    "class Determinist_Test_Episode_runner:\n",
    "    def __init__(self, episode_runner, episode_runner_render, maxlen=500):\n",
    "        self.ep_runner = episode_runner\n",
    "        self.ep_run_render = episode_runner_render\n",
    "        self.maxlen= maxlen\n",
    "\n",
    "    #select the most probable action given the policy and current observation\n",
    "    def select_action(self, obs):\n",
    "        dist = Categorical(self.ep_runner.policy(obs))\n",
    "        action= torch.argmax(dist.log_prob(dist.enumerate_support()))\n",
    "        return action.item()\n",
    "\n",
    "    # Given the environment and the policy, run it up to the maximum number of steps \n",
    "    def run_episode(self):\n",
    "        # Collect just about everything.\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        # Reset the environment and start the episode.\n",
    "        (obs, info) = self.ep_runner.env.reset() \n",
    "        for i in range(self.maxlen):\n",
    "            # Get the current observation, run the policy and select an action.\n",
    "            obs = torch.tensor(obs)\n",
    "            action = self.select_action(obs)\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "\n",
    "            # Advance the episode by executing the selected action.\n",
    "            (obs, reward, term, trunc, info) = self.ep_runner.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if term or trunc:\n",
    "                break\n",
    "        return (observations, actions, rewards)\n",
    "\n",
    "    def test(self, test_episodes):\n",
    "        print('Testing the best policy')\n",
    "        self.ep_runner.policy.eval()\n",
    "        total_reward = 0\n",
    "        episode_lengths = []\n",
    "        for _ in range(test_episodes):\n",
    "            (_, _, rewards) = self.run_episode()\n",
    "            total_reward += np.sum(rewards)\n",
    "            episode_lengths.append(len(rewards))\n",
    "        test_average_episode_len_metric = {\"test_average_episode_length\": np.mean(episode_lengths)}\n",
    "        test_average_rewards_metric = {\"test_average_total_reward\": total_reward / test_episodes}\n",
    "        wandb.log({**test_average_rewards_metric, **test_average_episode_len_metric})\n",
    "\n",
    "        (obs, _, _, _) = self.ep_run_render.run_episode()\n",
    "        self.ep_runner.policy.train()\n",
    "        print(f'Average Total reward: {total_reward / test_episodes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BhypW9ySOthT",
   "metadata": {
    "id": "BhypW9ySOthT"
   },
   "source": [
    "## Reinforce\n",
    "Implementation of REINFORCE policy gradient algorithm.\n",
    "\n",
    "It receives two \"training\" episode runners, one to train the policy and one to show the rendering of some episodes if display = True to see the progress.\n",
    "The training parameters are:\n",
    "- gamma: used to compute discounted total reward, balances the importance of immediate versus future reward. A higher value gives a higher weight to future rewards while a lower value prioritizes immediate rewards.\n",
    "- baseline: the type of baseline used to compute the value function. None (no baseline), std (standard baseline, standardize rewards within the episode) or a ValueNet (the value function is approximated by a Neural Network with the same architecture as the policy network)\n",
    "- num_episodes: number of training episodes to train the policy and baseline net\n",
    "- eval_every: after how many training episodes the policy is evaluated\n",
    "- eval_episodes: the number of episodes the policy is evaluated on\n",
    "- lr and lr_baseline: learning rates of the policy and baseline net\n",
    "\n",
    "Given all these inputs, for each episode, the discounted reward, running reward and episode length (in the cartpole case the episode length is equal to the reward of that episode) are computed. If there is a baseline, the policy loss, that is used to optimize the policy, is calculated with the target value. If the baseline is a Neural Network it is also optimized using its loss (how closely it approximates the value function).\n",
    "\n",
    "After eval_every episodes of training the policy is evaluated on eval_episodes episodes. Instead of using the running average the function calculates the average total reward of the episodes that were runned and their average length. As previously said, these two quantities are the same in Cartpole since the reward of each episode is its length. During this evaluation the best model (the one with the best average reward) is saved.\n",
    "\n",
    "I also chose to collect all the episodes lengths and losses, every 10 episodes are aggregated in a single value (the mean) to create graphs that are easier to read.\n",
    "\n",
    "Lastly, the function calculates the average episode length of the entire training to show a general trend, it can be used to investigate the stability of a training and how fast it is able to converge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a50f666-126a-4a16-9cbc-d9c846a1c3a7",
   "metadata": {
    "id": "2a50f666-126a-4a16-9cbc-d9c846a1c3a7"
   },
   "outputs": [],
   "source": [
    "# Utility to compute the discounted total reward. Torch doesn't like flipped arrays, so we need to\n",
    "# .copy() the final numpy array.\n",
    "def compute_returns(rewards, gamma):\n",
    "    return np.flip(np.cumsum([gamma**(i+1)*r for (i, r) in enumerate(rewards)][::-1]), 0).copy()\n",
    "\n",
    "# Implementation of the REINFORCE policy gradient algorithm.\n",
    "# It receives the episode runner, the wandb run to save the results, the episode runner render that is used to monitor training \n",
    "# when display = True, the gamma parameter, the number of episodes to train the policy and baseline net, the type of baseline used,\n",
    "# eval_every (after how many training steps we evaluate the policy), eval_episode (how many episodes we \n",
    "# evaluate the policy on) and the learning rates\n",
    "def reinforce(episode_runner, wandb, episode_runner_render=None, gamma=0.99, num_episodes=2000,\n",
    "              baseline=None, display=False, eval_every=100, eval_episodes=100, lr= 1e-2, lr_baseline = 1e-3 ):\n",
    "    # We use Adam instead of SGD.\n",
    "    opt = torch.optim.Adam(episode_runner.policy.parameters(), lr= lr)\n",
    "\n",
    "    # If we have a baseline network, create the optimizer.\n",
    "    if isinstance(baseline, nn.Module):\n",
    "        opt_baseline = torch.optim.Adam(baseline.parameters(), lr= lr_baseline)  \n",
    "        baseline.train()\n",
    "        print('Training agent with baseline value network.')\n",
    "    elif baseline == 'std':\n",
    "        print('Training agent with standardization baseline.')\n",
    "    else:\n",
    "        print('Training agent with no baseline.')\n",
    "\n",
    "    #Collect running rewards, all the episodes lengths and training loss\n",
    "    running_rewards = [0.0]\n",
    "    all_episodes_lenghts = []\n",
    "    training_losses = []\n",
    "    value_losses = []\n",
    "\n",
    "    #save the latest policy with the greatest average totale reward\n",
    "    best_model_state_dict = None\n",
    "    best_avg_tot_rew = 0\n",
    "    \n",
    "    # The main training loop.\n",
    "    episode_runner.policy.train()\n",
    "    for episode in range(1, num_episodes+1):\n",
    "        # Run an episode of the environment, collect everything needed for policy update.\n",
    "        (observations, actions, log_probs, rewards) = episode_runner.run_episode()\n",
    "\n",
    "        # Compute the discounted reward for every step of the episode.\n",
    "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
    "        # Keep a running average of total discounted rewards for the whole episode.\n",
    "        running_rewards.append(0.05 * returns[0].item() + 0.95 * running_rewards[-1])\n",
    "        running_rewards_metric = {\"running_reward\": running_rewards[-1]}\n",
    "\n",
    "        # Handle baseline.\n",
    "        if isinstance(baseline, nn.Module):\n",
    "            with torch.no_grad():\n",
    "                target = returns - baseline(torch.stack(observations))\n",
    "        elif baseline == 'std':                                       #Standardize returns\n",
    "            target = (returns - returns.mean()) / returns.std()\n",
    "        else:\n",
    "            target = returns\n",
    "\n",
    "        # Make an optimization step\n",
    "        opt.zero_grad()\n",
    "        loss = (-log_probs * target).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Collect each episode length and training loss\n",
    "        all_episodes_lenghts.append(len(returns))\n",
    "        training_losses.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "        # Log only the mean training loss, episode lenght and running reward of 10 episode to make the graphs cleaner\n",
    "        if episode % 10 == 0:\n",
    "            loss_policy_metric = {\"loss_policy\": np.mean(training_losses[-10:])}\n",
    "            episode_length_metric = {\"episode_length\": np.mean(all_episodes_lenghts[-10:])}\n",
    "            wandb.log({**loss_policy_metric, **episode_length_metric}, commit = False)\n",
    "\n",
    "        # Update baseline network.\n",
    "        if isinstance(baseline, nn.Module):\n",
    "            opt_baseline.zero_grad()\n",
    "            loss_baseline = ((returns - baseline(torch.stack(observations)))**2.0).mean()\n",
    "            loss_baseline.backward()\n",
    "            opt_baseline.step()\n",
    "            value_losses.append(loss_baseline.detach().cpu().numpy())\n",
    "            if episode % 10 == 0:\n",
    "                loss_value_metric = {\"loss_value\": np.mean(value_losses[-10:])}\n",
    "                wandb.log({**loss_value_metric}, commit = False)\n",
    "\n",
    "        # Render and evaluate the current policy after every \"eval_every\" policy updates.\n",
    "        if episode % eval_every == 0:\n",
    "            episode_runner.policy.eval()\n",
    "            total_reward = 0\n",
    "            episode_lengths = []\n",
    "            #evaluate on \"eval_episodes\" episodes the total reward and the episodes length\n",
    "            for _ in range(eval_episodes):\n",
    "                (_, _, _, rewards) = episode_runner.run_episode()\n",
    "                total_reward += np.sum(rewards)\n",
    "                episode_lengths.append(len(rewards))\n",
    "            average_episode_len_metric = {\"average_episode_length\": np.mean(episode_lengths)}\n",
    "            average_rewards_metric = {\"average_total_reward\": total_reward / eval_episodes}\n",
    "            wandb.log({**average_rewards_metric, **average_episode_len_metric}, commit = False)\n",
    "            # Update best policy, the one with the best average total reward at testing time\n",
    "            if  total_reward / eval_episodes >= best_avg_tot_rew:\n",
    "                best_avg_tot_rew = total_reward / eval_episodes\n",
    "                # save all the parameters of the best policy\n",
    "                best_model_state_dict = episode_runner.policy.state_dict()\n",
    "            if display:\n",
    "                (obs, _, _, _) = episode_runner_render.run_episode()\n",
    "            episode_runner.policy.train()\n",
    "            print(f'Running reward of episode {episode}/{num_episodes}: {running_rewards[-1]}')\n",
    "            print(f'Average Total reward: {total_reward / eval_episodes}')\n",
    "        \n",
    "        wandb.log({**running_rewards_metric})\n",
    "\n",
    "    # Lastly, calculate and print the average episode lenght of the entire training\n",
    "    print(f'Average length of all episodes: {np.mean(all_episodes_lenghts)}')\n",
    "    average_all_episodes_metric= {\"average_lenght_all_episodes\": np.mean(all_episodes_lenghts)}\n",
    "    wandb.log({**average_all_episodes_metric})\n",
    "    \n",
    "    episode_runner.policy.eval()\n",
    "    if isinstance(baseline, nn.Module):\n",
    "        baseline.eval()\n",
    "    return best_model_state_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea05c3",
   "metadata": {},
   "source": [
    "# Standard runs\n",
    "In the following section I tested `REINFORCE` on the Cartpole environment using the standard baseline while changing the parameters (temperature and gamma).\n",
    "\n",
    "Since training the policy is stochastic, for each setting I performed the policy training 5 times with 5 different seeds to be able to replicate the experiments.\n",
    "\n",
    "At the end of this section there will be a wandb report with the results, for every setting the 5 runs are aggregated showing their mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mkTymBWNPESI",
   "metadata": {
    "id": "mkTymBWNPESI"
   },
   "source": [
    "## Standard run\n",
    "Base parameters, temperature = 1 and gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b440c-2b1d-42db-81f2-9bdd6d036d3a",
   "metadata": {
    "id": "378b440c-2b1d-42db-81f2-9bdd6d036d3a",
    "outputId": "396212df-359e-4c94-c804-398648e3f57b"
   },
   "outputs": [],
   "source": [
    "# Random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard Run \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 1,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    # Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    # Set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b48689-6e55-4f7d-aa56-c4ce6459e765",
   "metadata": {},
   "source": [
    "## Standard run and higher temperature (5)\n",
    "Higher temperature, temperature = 5 and gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef897b43-445f-467b-8e2e-8bd9e30fc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard and temperature 5 \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98a63e-5e66-4ecc-98b7-ce1341a1e81a",
   "metadata": {},
   "source": [
    "## Standard run and higher temperature (10)\n",
    "Even higher temperature, temperature = 10 and gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578621a9-c76a-46c9-8a80-9b9411c7ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard and temperature 10 \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 10,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e2878-19d2-410b-bb75-e468082ad22b",
   "metadata": {},
   "source": [
    "## Standard run and lower gamma\n",
    "Higher temperature (since it achieved better results) and lower gamma, temperature = 5 and gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e338ef-b678-433f-8404-3ab28b34b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard and gamma 0.9 \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.9,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915a9b8",
   "metadata": {},
   "source": [
    "## Standard Runs Report\n",
    "This is the public report created with weights and biases: https://api.wandb.ai/links/marcouni/c8d6xyuv\n",
    "\n",
    "It is quite easy to see that the best performance is obtained when the temperature is 5 and gamma is 0.99. Its average total reward is almost always the highest and it is extremely fast to converge (highest average length of all episodes) to a very stable perfect policy (always able to reach the 500th step in each episode). Its test average reward is perfect, meaning that the final policy is one of the possible best policies for this environment.\n",
    "\n",
    "The standard run and the one with temperature = 10 are not that different, except that the latter seems to be a little slower to converge (lower average length of all episodes) but is more stable (perfect test average total reward).\n",
    "\n",
    "It is also clear that the worst run is the one with a lower gamma, being the worst by every metric. This result shows the importance of future steps (through the importance of future rewards) in Cartpole. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5adad7-759b-4000-925b-701f41fe6e97",
   "metadata": {
    "id": "2a5adad7-759b-4000-925b-701f41fe6e97"
   },
   "source": [
    "# Exercise 2: `REINFORCE` with a Value Baseline (warm up)\n",
    "\n",
    "In this exercise we will augment my implementation (or your own) of `REINFORCE` to subtract a baseline from the target in the update equation in order to stabilize (and hopefully speed-up) convergence. For now we will stick to the Cartpole environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07",
   "metadata": {
    "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07"
   },
   "source": [
    "**First Things First**: Recall from the slides on Deep Reinforcement Learning that we can **subtract** any function that doesn't depend on the current action from the q-value without changing the (maximum of our) objecttive function $J$:  \n",
    "\n",
    "$$ \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_{s} \\mu(s) \\sum_a \\left( q_{\\pi}(s, a) - b(s) \\right) \\nabla \\pi(a \\mid s, \\boldsymbol{\\theta}) $$\n",
    "\n",
    "In `REINFORCE` this means we can subtract from our target $G_t$:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - b(S_t)) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
    "\n",
    "Since we are only interested in the **maximum** of our objective, we can also **rescale** our target by any function that also doesn't depend on the action. A **simple baseline** which is even independent of the state -- that is, it is **constant** for each episode -- is to just **standardize rewards within the episode**. So, we **subtract** the average return and **divide** by the variance of returns:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha \\left(\\frac{G_t - \\bar{G}}{\\sigma_G}\\right) \\nabla  \\pi(A_t \\mid s, \\boldsymbol{\\theta}) $$\n",
    "\n",
    "This baseline is **already** implemented in my implementation of `REINFORCE`. Experiment with and without this standardization baseline and compare the performance. We are going to do something more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df",
   "metadata": {
    "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df"
   },
   "source": [
    "**The Real Exercise**: Standard practice is to use the state-value function $v(s)$ as a baseline. This is intuitively appealing -- we are more interested in updating out policy for returns that estimate the current **value** worse. Our new update becomes:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - \\tilde{v}(S_t \\mid \\mathbf{w})) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
    "\n",
    "where $\\tilde{v}(s \\mid \\mathbf{w})$ is a **deep neural network** with parameters $w$ that estimates $v_\\pi(s)$. What neural network? Typically, we use the **same** network architecture as that of the Policy.\n",
    "\n",
    "**Your Task**: Modify your implementation to fit a second, baseline network to estimate the value function and use it as **baseline**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed0e92",
   "metadata": {},
   "source": [
    "# Different baselines\n",
    "This section contains the results of the experiments with REINFORCE on Cartpole using different kinds of baselines (no baseline, standard baseline and a Value Net as a baseline).\n",
    "\n",
    "Since previously the best results were obtained with temperature = 5 and gamma = 0.99 these parameters will be used in all the following runs. I also experimented with changing the number of layers of the Policy and Value Nets.\n",
    "\n",
    "Once again, for each setting I performed the policy training 5 times with 5 different seeds to be able to replicate the experiments and at the end of this section there will be a wandb report with the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f97e3",
   "metadata": {},
   "source": [
    "## Without the baseline\n",
    "Since I already experimented with Reinforce with the standard baseline the next run will not cointain a baseline of any kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76680bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Without the baseline \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": None,\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ad67f-180b-4009-a6e7-752fe1e4bc58",
   "metadata": {},
   "source": [
    "## With Baseline network\n",
    "Using ValueNet, a simple Net with the same architecture as the Policy Net that has to learn to estimate the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6187b4f3-568a-49a5-93f6-cc7cccb8cc2f",
   "metadata": {
    "id": "6187b4f3-568a-49a5-93f6-cc7cccb8cc2f"
   },
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, env, hidden_layers=32):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], hidden_layers)\n",
    "        self.fc2 = nn.Linear(hidden_layers, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = self.fc2(s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f",
   "metadata": {
    "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f"
   },
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Baseline Network \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f186e88-9437-4453-b4f8-27c0d3c8e92f",
   "metadata": {},
   "source": [
    "## With baseline, 128 hidden layer\n",
    "Adding more hidden layers to the ValueNet and PolicyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22488e84-fadd-44e7-b9a8-4d3ba72f0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Baseline Network 128 layers temp 5\",\n",
    "          config={\n",
    "              \"hidden_layers\": 128,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac2ae4a-f4e8-432d-aa66-fa50cc891a8a",
   "metadata": {},
   "source": [
    "## With baseline, 16 hidden layer\n",
    "Lowering the number of hidden layers of the ValueNet and PolicyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0652b1-5cbf-4cc1-8e67-9d80ab45be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Baseline Network 16 layers temp 5\",\n",
    "          config={\n",
    "              \"hidden_layers\": 16,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c1e91",
   "metadata": {},
   "source": [
    "## Baselines Runs Report\n",
    "This is the public report created with weights and biases: https://api.wandb.ai/links/marcouni/0ksdyjpo\n",
    "\n",
    "Once again the best setting seems to be the one with a standard baseline and temperature = 5, closely followed by the settings with baseline networks with 32 (standard) and 128 hidden layers (more layers achieve a slightly better result). Given these results using a standard baseline and temperature = 5 seems like the best case scenario for Cartpole.\n",
    "This is probably due to the simplicity (the standardized mean of the discounted return seems like a very good approximation of the value function) of the problem since its convergence is incredibly fast and stable. \n",
    "\n",
    "Using a ValueNet also leads to a stable training and a perfect policy (the test average total reward is perfect with 32 and 128 hidden layers) but seems to slow down the convergence as it needs more time to learn to approximate the value function. \n",
    "\n",
    "When the layers of the two Networks are not enough (16 hidden layers) they are unable to correctly approximate the value function and the policy.\n",
    "\n",
    "As expected the setting with no baseline produces the worst results, the runs are much less stable, as it can be seen in average total reward and average length all episodes. The final policy is also extremely unstable, it can be perfect or horrible ( test average total reward has a very high variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc5eec-9ebc-4867-b698-cfefbe6387d8",
   "metadata": {},
   "source": [
    " # Longer runs\n",
    " Lastly I also experiments with much longer runs with two of the best settings, with standard baseline and ValueNet, to see if their performance gap was due to the number of training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a52700c-e8cd-4048-b567-bb885b9eea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Long Standard\",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 5000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 50,\n",
    "              \"test_episodes\" : 500,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf6d680-b49d-4b82-81e8-fb431e225e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Long Baseline Network \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 5000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 50,\n",
    "              \"test_episodes\" : 500,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c664f9",
   "metadata": {},
   "source": [
    "## Longer Runs Report\n",
    "This is the public report created with weights and biases: https://api.wandb.ai/links/marcouni/r8dqdyc5\n",
    "\n",
    "It is clear that increasing the number of training episodes deteriorates both settings' performances, making the training more unstable and the final policy less good. The one that suffers a worse decline in reward is the one with ValueNet, probably due to overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b3194",
   "metadata": {},
   "source": [
    "# Video recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0459791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "env_render = gymnasium.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "# wrap the env in the record video\n",
    "recorder = gymnasium.wrappers.RecordVideo(env=env_render, video_folder=\"/data01/dl24marchi/DLAProjects/videos\", name_prefix=\"test-video\", episode_trigger=lambda x: x%10 == 0)\n",
    "\n",
    "# env reset for a fresh start\n",
    "observation, info = recorder.reset()\n",
    "\n",
    "###\n",
    "# Start the recorder\n",
    "recorder.start_video_recorder()\n",
    "\n",
    "# And run the final agent for a few episodes.\n",
    "for _ in range(100):\n",
    "    run_episode(recorder, policy)\n",
    "\n",
    "####\n",
    "# Don't forget to close the video recorder before the env!\n",
    "recorder.close_video_recorder()\n",
    "\n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf1447-d222-4b24-a357-5b7f9824390c",
   "metadata": {
    "id": "64bf1447-d222-4b24-a357-5b7f9824390c"
   },
   "source": [
    "-----\n",
    "# Exercise 3: Going Deeper\n",
    "\n",
    "As usual, pick **AT LEAST ONE** of the following exercises to complete.\n",
    "\n",
    "### Exercise 3.1: Solving Lunar Lander with `REINFORCE` (easy)\n",
    "\n",
    "Use my (or even better, improve on my) implementation of `REINFORCE` to solve the [Lunar Lander Environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/). This environment is a little bit harder than Cartpole, but not much. Make sure you perform the same types of analyses we did during the lab session to quantify and qualify the performance of your agents.\n",
    "\n",
    "### Exercise 3.2: Solving Cartpole and Lunar Lander with `Deep Q-Learning` (harder)\n",
    "\n",
    "On policy Deep Reinforcement Learning tends to be **very unstable**. Write an implementation (or adapt an existing one) of `Deep Q-Learning` to solve our two environments (Cartpole and Lunar Lander). To do this you will need to implement a **Replay Buffer** and use a second, slow-moving **target Q-Network** to stabilize learning.\n",
    "\n",
    "### Exercise 3.3: Solving the OpenAI CarRacing environment (hardest)\n",
    "\n",
    "Use `Deep Q-Learning` -- or even better, an off-the-shelf implementation of **Proximal Policy Optimization (PPO)** -- to train an agent to solve the [OpenAI CarRacing](https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN) environment. This will be the most *fun*, but also the most *difficult*. Some tips:\n",
    "\n",
    "1. Make sure you use the `continuous=False` argument to the environment constructor. This ensures that the action space is **discrete** (we haven't seen how to work with continuous action spaces).\n",
    "2. Your Q-Network will need to be a CNN. A simple one should do, with two convolutional + maxpool layers, folowed by a two dense layers. You will **definitely** want to use a GPU to train your agents.\n",
    "3. The observation space of the environment is a single **color image** (a single frame of the game). Most implementations stack multiple frames (e.g. 3) after converting them to grayscale images as an observation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a12d96",
   "metadata": {},
   "source": [
    "Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c55efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def process_state_image(state):\n",
    "    state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "    state = state.astype(float)\n",
    "    state /= 255.0\n",
    "    return state\n",
    "\n",
    "def generate_state_frame_stack_from_queue(deque):\n",
    "    frame_stack = np.array(deque)\n",
    "    # Move stack dimension to the channel dimension (stack, x, y) -> (x, y, stack)\n",
    "    return np.transpose(frame_stack, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8171247",
   "metadata": {},
   "source": [
    "Codice GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab3562d-9e4a-4ac1-9667-a9ea36412a24",
   "metadata": {
    "id": "6ab3562d-9e4a-4ac1-9667-a9ea36412a24"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CarRacingDQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space    = [\n",
    "            (-1, 1, 0.2), (0, 1, 0.2), (1, 1, 0.2), #           Action Space Structure\n",
    "            (-1, 1,   0), (0, 1,   0), (1, 1,   0), #        (Steering Wheel, Gas, Break)\n",
    "            (-1, 0, 0.2), (0, 0, 0.2), (1, 0, 0.2), # Range        -1~1       0~1   0~1\n",
    "            (-1, 0,   0), (0, 0,   0), (1, 0,   0)\n",
    "        ],\n",
    "        frame_stack_num = 3,\n",
    "        memory_size     = 5000,\n",
    "        gamma           = 0.95,  # discount rate\n",
    "        epsilon         = 1.0,   # exploration rate\n",
    "        epsilon_min     = 0.1,\n",
    "        epsilon_decay   = 0.9999,\n",
    "        learning_rate   = 0.001,\n",
    "        device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ):\n",
    "        self.action_space    = action_space\n",
    "        self.frame_stack_num = frame_stack_num\n",
    "        self.memory          = deque(maxlen=memory_size)\n",
    "        self.gamma           = gamma\n",
    "        self.epsilon         = epsilon\n",
    "        self.epsilon_min     = epsilon_min\n",
    "        self.epsilon_decay   = epsilon_decay\n",
    "        self.learning_rate   = learning_rate\n",
    "        self.device          = device\n",
    "        self.model           = self.build_model().to(self.device)\n",
    "        self.target_model    = self.build_model().to(self.device)\n",
    "        self.update_target_model()\n",
    "        self.optimizer       = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = nn.Sequential(\n",
    "            nn.Conv2d(self.frame_stack_num, 6, kernel_size=7, stride=3, activation='relu'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(6, 12, kernel_size=4, activation='relu'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(12 * 18 * 18, 216),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(216, len(self.action_space))\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, self.action_space.index(action), reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                act_values = self.model(state)\n",
    "            action_index = torch.argmax(act_values).item()\n",
    "        else:\n",
    "            action_index = random.randrange(len(self.action_space))\n",
    "        return self.action_space[action_index]\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        train_state = []\n",
    "        train_target = []\n",
    "\n",
    "        for state, action_index, reward, next_state, done in minibatch:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)\n",
    "            target = self.model(state).detach().cpu().numpy()[0]\n",
    "\n",
    "            if done:\n",
    "                target[action_index] = reward\n",
    "            else:\n",
    "                t = self.target_model(next_state).detach().cpu().numpy()[0]\n",
    "                target[action_index] = reward + self.gamma * np.amax(t)\n",
    "\n",
    "            train_state.append(state.cpu().numpy())\n",
    "            train_target.append(target)\n",
    "\n",
    "        train_state = torch.FloatTensor(np.array(train_state)).to(self.device)\n",
    "        train_target = torch.FloatTensor(np.array(train_target)).to(self.device)\n",
    "\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(train_state)\n",
    "        loss = nn.MSELoss()(outputs, train_target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_state_dict(torch.load(name))\n",
    "        self.update_target_model()\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.target_model.state_dict(), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ccdfb",
   "metadata": {},
   "source": [
    "## DQN\n",
    "A Neural Network that takes in the difference between the current and previous screen patches. The network tries to predict the expected return of taking each action given the current input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62abd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_observations, 6, kernel_size=7, stride=3)\n",
    "        self.conv2 = nn.Conv2d(6, 12, kernel_size=4)\n",
    "        self.fc1 = nn.Linear(12*18*18, 256)  # da testare se 12*18*18 è giusto\n",
    "        self.fc2 = nn.Linear(256, n_actions)   \n",
    "        self.max_pool= nn.MaxPool2d(kernel_size= (2,2))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.max_pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.max_pool\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea41a5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gymnasium.make('CarRacing-v2', continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32847636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
