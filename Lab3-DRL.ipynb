{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc",
   "metadata": {
    "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc"
   },
   "source": [
    "# Deep Reinforcement Learning Laboratory\n",
    "\n",
    "In this laboratory session we will work on getting more advanced versions of Deep Reinforcement Learning algorithms up and running. Deep Reinforcement Learning is **hard**, and getting agents to stably train can be frustrating and requires quite a bit of subtlety in analysis of intermediate results. We will start by refactoring (a bit) my implementation of `REINFORCE` on the [Cartpole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb10104-aba1-4d29-b6d1-6b53371302a0",
   "metadata": {
    "id": "7bb10104-aba1-4d29-b6d1-6b53371302a0"
   },
   "source": [
    "Considerazioni finali di Cartpole:\n",
    "\n",
    "2. **Exploration**. The model is probably overfitting (or perhaps remaining too *plastic*, which can explain the unstable convergence). Our policy is *always* stochastic in that we sample from the output distribution. It would be interesting to add a temperature parameter to the policy so that we can control this behavior, or even implement a deterministic policy sampler that always selects the action with max probability to evaluate the quality of the learned policy network.\n",
    "\n",
    "3. **Discount Factor**: The discount factor (default $\\gamma = 0.99$) is an important hyperparameter that has an effect on the stability of training. Try different values for $\\gamma$ and see how it affects training. Can you think of other ways to stabilize training?\n",
    "\n",
    "Cose che ho scritto io da aggiungere:\n",
    "\n",
    "usare la temperatura nel softmax per evitare crolli.\n",
    "\n",
    "provare anche a diminuire hidden layer size\n",
    "\n",
    "di solito non si riportano le curve per i risultati, visto che è stocastico-> 5 seed e poi riportare curva di confidenza\n",
    "\n",
    "se si fanno tante prove ha senso togliere la parte di rendering, quella è solo di debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uldpcHx0OHyx",
   "metadata": {
    "id": "uldpcHx0OHyx"
   },
   "source": [
    "## Imports and weights and biases login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689f766-b099-4948-a1b2-37e92975178f",
   "metadata": {
    "id": "9689f766-b099-4948-a1b2-37e92975178f"
   },
   "outputs": [],
   "source": [
    "# Using weights and biases\n",
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241e9bdc-7aa9-4a12-a57f-088fdc87eab6",
   "metadata": {
    "id": "241e9bdc-7aa9-4a12-a57f-088fdc87eab6"
   },
   "outputs": [],
   "source": [
    "# Standard imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Plus one non standard one -- we need this to sample from policies.\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e806e9d4-bb04-40ed-9cd4-f016823b100c",
   "metadata": {
    "id": "e806e9d4-bb04-40ed-9cd4-f016823b100c",
    "outputId": "753a212b-355e-4133-f7e1-53e9b5895040"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarco-chisci\u001b[0m (\u001b[33mmarcouni\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login to weights and biases account\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f",
   "metadata": {
    "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f"
   },
   "source": [
    "## Exercise 1: Improving my `REINFORCE` Implementation (warm up)\n",
    "\n",
    "In this exercise we will refactor a bit and improve some aspects of my `REINFORCE` implementation.\n",
    "\n",
    "**First Things First**: Spend some time playing with the environment to make sure you understand how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dk0EFuXUOpXz",
   "metadata": {
    "id": "Dk0EFuXUOpXz"
   },
   "source": [
    "## Policy Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ea9416-b07e-4354-bf36-1150394b0698",
   "metadata": {
    "id": "16ea9416-b07e-4354-bf36-1150394b0698"
   },
   "outputs": [],
   "source": [
    "# A simple policy network with one hidden layer and a temperature parameter to smooth the output\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, env, hidden_layers=32, temperature=1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], hidden_layers)\n",
    "        self.fc2 = nn.Linear(hidden_layers, env.action_space.n)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = F.softmax(self.fc2(s)/self.temperature, dim=-1)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uvEAh3bgOWTz",
   "metadata": {
    "id": "uvEAh3bgOWTz"
   },
   "source": [
    "## Episode Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd2dd9d-a60b-4892-878f-d83f64ac5f63",
   "metadata": {
    "id": "cbd2dd9d-a60b-4892-878f-d83f64ac5f63",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A class that , given an environment, a policy network and the max lenght of the episode is in charge\n",
    "# of running it\n",
    "class Episode_runner:\n",
    "    def __init__(self, env, policy, maxlen=500):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.maxlen= maxlen\n",
    "\n",
    "    # Given observation and the policy, sample from pi(a | obs). Returns the\n",
    "    # selected action and the log probability of that action (needed for policy gradient).\n",
    "    def select_action(self, obs):\n",
    "        dist = Categorical(self.policy(obs))\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return (action.item(), log_prob.reshape(1))\n",
    "\n",
    "    # Given the environment and the policy, run it up to the maximum number of steps.\n",
    "    def run_episode(self):\n",
    "        # Collect just about everything.\n",
    "        observations = []\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "\n",
    "        # Reset the environment and start the episode.\n",
    "        (obs, info) = self.env.reset() \n",
    "        for i in range(self.maxlen):\n",
    "            # Get the current observation, run the policy and select an action.\n",
    "            obs = torch.tensor(obs)\n",
    "            (action, log_prob) = self.select_action(obs)\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "            # Advance the episode by executing the selected action.\n",
    "            (obs, reward, term, trunc, info) = self.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if term or trunc:\n",
    "                break\n",
    "        return (observations, actions, torch.cat(log_probs), rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc5787-1c38-4f5c-b716-e40e900d00d2",
   "metadata": {},
   "source": [
    "## Deterministic Episode Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f505e44c-0d5e-48f1-b874-d318424b3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class that , given an episode runner (with an environment and a policy network), an episode render to show the policy \n",
    "# and the max lenght of each episode evaluates the quality of the learned policy network, \n",
    "# always selecting the action with max probability \n",
    "class Determinist_Test_Episode_runner:\n",
    "    def __init__(self, episode_runner, episode_runner_render, maxlen=500):\n",
    "        self.ep_runner = episode_runner\n",
    "        self.ep_run_render = episode_runner_render\n",
    "        self.maxlen= maxlen\n",
    "\n",
    "    #select the most probable action given the policy and current observation\n",
    "    def select_action(self, obs):\n",
    "        dist = Categorical(self.ep_runner.policy(obs))\n",
    "        action= torch.argmax(dist.log_prob(dist.enumerate_support()))\n",
    "        return action.item()\n",
    "\n",
    "    # Given the environment and the policy, run it up to the maximum number of steps \n",
    "    def run_episode(self):\n",
    "        # Collect just about everything.\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        # Reset the environment and start the episode.\n",
    "        (obs, info) = self.ep_runner.env.reset() \n",
    "        for i in range(self.maxlen):\n",
    "            # Get the current observation, run the policy and select an action.\n",
    "            obs = torch.tensor(obs)\n",
    "            action = self.select_action(obs)\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "\n",
    "            # Advance the episode by executing the selected action.\n",
    "            (obs, reward, term, trunc, info) = self.ep_runner.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if term or trunc:\n",
    "                break\n",
    "        return (observations, actions, rewards)\n",
    "\n",
    "    def test(self, test_episodes):\n",
    "        print('Testing the best policy')\n",
    "        self.ep_runner.policy.eval()\n",
    "        total_reward = 0\n",
    "        episode_lengths = []\n",
    "        for _ in range(test_episodes):\n",
    "            (_, _, rewards) = self.run_episode()\n",
    "            total_reward += np.sum(rewards)\n",
    "            episode_lengths.append(len(rewards))\n",
    "        test_average_episode_len_metric = {\"test_average_episode_length\": np.mean(episode_lengths)}\n",
    "        test_average_rewards_metric = {\"test_average_total_reward\": total_reward / test_episodes}\n",
    "        wandb.log({**test_average_rewards_metric, **test_average_episode_len_metric})\n",
    "\n",
    "        (obs, _, _, _) = self.ep_run_render.run_episode()\n",
    "        self.ep_runner.policy.train()\n",
    "        print(f'Average Total reward: {total_reward / test_episodes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BhypW9ySOthT",
   "metadata": {
    "id": "BhypW9ySOthT"
   },
   "source": [
    "## Reinforce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae31dce-be71-4a86-95fd-1af902f026b8",
   "metadata": {
    "id": "0ae31dce-be71-4a86-95fd-1af902f026b8"
   },
   "source": [
    "**Next Things Next**: Now get your `REINFORCE` implementation working on the environment. You can import my (probably buggy and definitely inefficient) implementation here. Or even better, refactor an implementation into a separate package from which you can `import` the stuff you need here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8a90f-78ed-4f43-9d66-aeba7c74fe2c",
   "metadata": {
    "id": "18b8a90f-78ed-4f43-9d66-aeba7c74fe2c"
   },
   "source": [
    "**Last Things Last**: My implementation does a **super crappy** job of evaluating the agent performance during training. The running average is not a very good metric. Modify my implementation so that every $N$ iterations (make $N$ an argument to the training function) the agent is run for $M$ episodes in the environment. Collect and return: (1) The average **total** reward received over the $M$ iterations; and (2) the average episode length. Analyze the performance of your agents with these new metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a50f666-126a-4a16-9cbc-d9c846a1c3a7",
   "metadata": {
    "id": "2a50f666-126a-4a16-9cbc-d9c846a1c3a7"
   },
   "outputs": [],
   "source": [
    "# Utility to compute the discounted total reward. Torch doesn't like flipped arrays, so we need to\n",
    "# .copy() the final numpy array. There's probably a better way to do this.\n",
    "def compute_returns(rewards, gamma):\n",
    "    return np.flip(np.cumsum([gamma**(i+1)*r for (i, r) in enumerate(rewards)][::-1]), 0).copy()\n",
    "\n",
    "# Implementation of the REINFORCE policy gradient algorithm.\n",
    "# It receives the episode runner, the wandb run to save the results, the episode runner render that is used to monitor training \n",
    "# when display = True, the gamma parameter, the number of episodes to train the policy and baseline net, the type of baseline used,\n",
    "# eval_every (after how many training steps we evaluate the policy), eval_episode (how many episodes we \n",
    "# evaluate the policy on) and the learning rates\n",
    "def reinforce(episode_runner, wandb, episode_runner_render=None, gamma=0.99, num_episodes=2000,\n",
    "              baseline=None, display=False, eval_every=100, eval_episodes=100, lr= 1e-2, lr_baseline = 1e-3 ):\n",
    "    # The only non-vanilla part: we use Adam instead of SGD.\n",
    "    opt = torch.optim.Adam(episode_runner.policy.parameters(), lr= lr)\n",
    "\n",
    "    # If we have a baseline network, create the optimizer.\n",
    "    if isinstance(baseline, nn.Module):\n",
    "        opt_baseline = torch.optim.Adam(baseline.parameters(), lr= lr_baseline)  \n",
    "        baseline.train()\n",
    "        print('Training agent with baseline value network.')\n",
    "    elif baseline == 'std':\n",
    "        print('Training agent with standardization baseline.')\n",
    "    else:\n",
    "        print('Training agent with no baseline.')\n",
    "\n",
    "    #Collect running rewards, all the episodes lengths and training loss\n",
    "    running_rewards = [0.0]\n",
    "    all_episodes_lenghts = []\n",
    "    training_losses = []\n",
    "    value_losses = []\n",
    "\n",
    "    #save the latest policy with the greatest average totale reward\n",
    "    best_model_state_dict = None\n",
    "    best_avg_tot_rew = 0\n",
    "    \n",
    "    # The main training loop.\n",
    "    episode_runner.policy.train()\n",
    "    for episode in range(1, num_episodes+1):\n",
    "        # Run an episode of the environment, collect everything needed for policy update.\n",
    "        (observations, actions, log_probs, rewards) = episode_runner.run_episode()\n",
    "\n",
    "        # Compute the discounted reward for every step of the episode.\n",
    "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
    "        # Keep a running average of total discounted rewards for the whole episode.\n",
    "        running_rewards.append(0.05 * returns[0].item() + 0.95 * running_rewards[-1])\n",
    "        running_rewards_metric = {\"running_reward\": running_rewards[-1]}\n",
    "\n",
    "        # Handle baseline.\n",
    "        if isinstance(baseline, nn.Module):\n",
    "            with torch.no_grad():\n",
    "                target = returns - baseline(torch.stack(observations))\n",
    "        elif baseline == 'std':                                       #Standardize returns\n",
    "            target = (returns - returns.mean()) / returns.std()\n",
    "        else:\n",
    "            target = returns\n",
    "\n",
    "        # Make an optimization step\n",
    "        opt.zero_grad()\n",
    "        loss = (-log_probs * target).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        #log only the mean training loss, episode lenght and running reward of 10 episode to make the graphs cleaner\n",
    "        all_episodes_lenghts.append(len(returns))\n",
    "        training_losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            loss_policy_metric = {\"loss_policy\": np.mean(training_losses[-10:])}\n",
    "            episode_length_metric = {\"episode_length\": np.mean(all_episodes_lenghts[-10:])}\n",
    "            wandb.log({**loss_policy_metric, **episode_length_metric}, commit = False)\n",
    "\n",
    "        # Update baseline network.\n",
    "        if isinstance(baseline, nn.Module):\n",
    "            opt_baseline.zero_grad()\n",
    "            loss_baseline = ((returns - baseline(torch.stack(observations)))**2.0).mean()\n",
    "            loss_baseline.backward()\n",
    "            opt_baseline.step()\n",
    "            value_losses.append(loss_baseline.detach().cpu().numpy())\n",
    "            if episode % 10 == 0:\n",
    "                loss_value_metric = {\"loss_value\": np.mean(value_losses[-10:])}\n",
    "                wandb.log({**loss_value_metric}, commit = False)\n",
    "\n",
    "        # Render and evaluate the current policy after every \"eval_every\" policy updates.\n",
    "        if episode % eval_every == 0:\n",
    "            episode_runner.policy.eval()\n",
    "            total_reward = 0\n",
    "            episode_lengths = []\n",
    "            #evaluate on \"eval_episodes\" episodes the total reward and the episodes length\n",
    "            for _ in range(eval_episodes):\n",
    "                (_, _, _, rewards) = episode_runner.run_episode()\n",
    "                total_reward += np.sum(rewards)\n",
    "                episode_lengths.append(len(rewards))\n",
    "            average_episode_len_metric = {\"average_episode_length\": np.mean(episode_lengths)}\n",
    "            average_rewards_metric = {\"average_total_reward\": total_reward / eval_episodes}\n",
    "            wandb.log({**average_rewards_metric, **average_episode_len_metric}, commit = False)\n",
    "            if  total_reward / eval_episodes >= best_avg_tot_rew:\n",
    "                best_avg_tot_rew = total_reward / eval_episodes\n",
    "                # save all the parameters of the best policy\n",
    "                best_model_state_dict = episode_runner.policy.state_dict()\n",
    "            if display:\n",
    "                (obs, _, _, _) = episode_runner_render.run_episode()\n",
    "            episode_runner.policy.train()\n",
    "            print(f'Running reward of episode {episode}/{num_episodes}: {running_rewards[-1]}')\n",
    "            print(f'Average Total reward: {total_reward / eval_episodes}')\n",
    "        \n",
    "        wandb.log({**running_rewards_metric})\n",
    "\n",
    "    # lastly, calculate and print the average episode lenght of the entire training\n",
    "    print(f'Average length of all episodes: {np.mean(all_episodes_lenghts)}')\n",
    "    average_all_episodes_metric= {\"average_lenght_all_episodes\": np.mean(all_episodes_lenghts)}\n",
    "    wandb.log({**average_all_episodes_metric})\n",
    "    \n",
    "    episode_runner.policy.eval()\n",
    "    if isinstance(baseline, nn.Module):\n",
    "        baseline.eval()\n",
    "    return best_model_state_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mkTymBWNPESI",
   "metadata": {
    "id": "mkTymBWNPESI"
   },
   "source": [
    "## Standard run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "378b440c-2b1d-42db-81f2-9bdd6d036d3a",
   "metadata": {
    "id": "378b440c-2b1d-42db-81f2-9bdd6d036d3a",
    "outputId": "396212df-359e-4c94-c804-398648e3f57b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ehup1eaz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁█</td></tr><tr><td>average_total_reward</td><td>▁█</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▁▁▁▂▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▂▄█</td></tr><tr><td>loss_policy</td><td>▆▄▅▃▂▄▁▄▂▄▆▂▄█▄▃▄▅▁▂▂▂▃▄▃▄▃</td></tr><tr><td>running_reward</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▆▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>217.5</td></tr><tr><td>average_total_reward</td><td>217.5</td></tr><tr><td>episode_length</td><td>429.8</td></tr><tr><td>loss_policy</td><td>-0.00798</td></tr><tr><td>running_reward</td><td>86.6903</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Try test </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ehup1eaz' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ehup1eaz</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_162233-ehup1eaz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ehup1eaz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_162321-8agcr9eo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8agcr9eo' target=\"_blank\">Standard Run </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8agcr9eo' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8agcr9eo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 32.71589919446313\n",
      "Average Total reward: 58.8\n",
      "Running reward of episode 200/2000: 62.14137885745559\n",
      "Average Total reward: 217.5\n",
      "Running reward of episode 300/2000: 94.74059099161452\n",
      "Average Total reward: 499.25\n",
      "Running reward of episode 400/2000: 96.09381802522763\n",
      "Average Total reward: 481.95\n",
      "Running reward of episode 500/2000: 97.85864756470048\n",
      "Average Total reward: 487.85\n",
      "Running reward of episode 600/2000: 98.09670548972397\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 700/2000: 91.089020737204\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 800/2000: 98.23690690133812\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 93.5830460039909\n",
      "Average Total reward: 479.6\n",
      "Running reward of episode 1000/2000: 97.02925393227288\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 95.56164338768357\n",
      "Average Total reward: 378.55\n",
      "Running reward of episode 1200/2000: 98.18459349285615\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.34854896717847\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 97.32684233644599\n",
      "Average Total reward: 412.3\n",
      "Running reward of episode 1500/2000: 98.18772059606057\n",
      "Average Total reward: 479.55\n",
      "Running reward of episode 1600/2000: 96.71353769430705\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.337998178493\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.18296224193277\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34853930930973\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34951961317535\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 421.7265\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8agcr9eo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▄████████▆██▇██████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▄████████▆██▇██████</td></tr><tr><td>episode_length</td><td>▁▁▂▂▃▄▆▇█▇███▃███▅██▅█████████▇▇██▇█████</td></tr><tr><td>loss_policy</td><td>▇▇▃▅▂▆▆▅▅▅▅▃▅▄▄▆▆▂▇▆▂▅▆▆▄▆▅▇▆▅▅▁▇▇▄▄▇▅█▆</td></tr><tr><td>running_reward</td><td>▁▂▃▄▆▆███████▇██████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>421.7265</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.00443</td></tr><tr><td>running_reward</td><td>98.34952</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard Run </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8agcr9eo' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8agcr9eo</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_162321-8agcr9eo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8agcr9eo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_163413-trrod2gb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/trrod2gb' target=\"_blank\">Standard Run </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/trrod2gb' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/trrod2gb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 45.24261558588204\n",
      "Average Total reward: 56.1\n",
      "Running reward of episode 200/2000: 88.17505960616143\n",
      "Average Total reward: 153.85\n",
      "Running reward of episode 300/2000: 97.87176516515488\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 400/2000: 97.33309453649986\n",
      "Average Total reward: 444.05\n",
      "Running reward of episode 500/2000: 98.31654295569537\n",
      "Average Total reward: 474.7\n",
      "Running reward of episode 600/2000: 98.0786709152909\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 700/2000: 88.86195641509707\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 800/2000: 95.59207986596432\n",
      "Average Total reward: 278.35\n",
      "Running reward of episode 900/2000: 98.27400811155437\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 74.98692654753931\n",
      "Average Total reward: 108.5\n",
      "Running reward of episode 1100/2000: 94.3809585845198\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.32602943556019\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.34938634281006\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.34952462806193\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.3495254467838\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.1083392932085\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 94.68594938322703\n",
      "Average Total reward: 242.85\n",
      "Running reward of episode 1800/2000: 98.14518160834243\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34575616250852\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.3495031354734\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 415.561\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:trrod2gb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▃█▇███▅█▂██████▄███</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▃█▇███▅█▂██████▄███</td></tr><tr><td>episode_length</td><td>▁▁▃▅▃██████▇▇▄█████▃▃▅████████████▇█████</td></tr><tr><td>loss_policy</td><td>█▇▃▅▄▅▆▅▆▆▆▂▃▅▅▃▆▅▆▄▁▆▃▆▅▃▇█▅▆▆▅▆▂▆▄▄▆▇▆</td></tr><tr><td>running_reward</td><td>▁▃▄▇▇████████▅█████▇▆▇██████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>415.561</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.00062</td></tr><tr><td>running_reward</td><td>98.3495</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard Run </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/trrod2gb' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/trrod2gb</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_163413-trrod2gb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:trrod2gb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_164525-942seubo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/942seubo' target=\"_blank\">Standard Run </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/942seubo' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/942seubo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 66.2024049949195\n",
      "Average Total reward: 329.0\n",
      "Running reward of episode 200/2000: 90.71400884176553\n",
      "Average Total reward: 306.4\n",
      "Running reward of episode 300/2000: 71.32711784746544\n",
      "Average Total reward: 117.2\n",
      "Running reward of episode 400/2000: 95.85379320565168\n",
      "Average Total reward: 470.1\n",
      "Running reward of episode 500/2000: 88.6751074013284\n",
      "Average Total reward: 276.0\n",
      "Running reward of episode 600/2000: 98.0568359457516\n",
      "Average Total reward: 378.95\n",
      "Running reward of episode 700/2000: 85.50913099454664\n",
      "Average Total reward: 199.4\n",
      "Running reward of episode 800/2000: 96.93070227213474\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 97.00646803282702\n",
      "Average Total reward: 373.3\n",
      "Running reward of episode 1000/2000: 98.33444586527519\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.34943617252813\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 97.21923716810281\n",
      "Average Total reward: 475.85\n",
      "Running reward of episode 1300/2000: 85.95435516227276\n",
      "Average Total reward: 110.05\n",
      "Running reward of episode 1400/2000: 93.13840285329644\n",
      "Average Total reward: 475.7\n",
      "Running reward of episode 1500/2000: 98.31867284804558\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 97.29409548955047\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.34327674772928\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 91.61420618321391\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.03667735417446\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34767322535727\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 392.965\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:942seubo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▅▅▁▇▄▆▃█▆███▁███████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▅▅▁▇▄▆▃█▆███▁███████</td></tr><tr><td>episode_length</td><td>▁▁▅▇▃▅▂▅▇▆██▄▄▄▇████████▇▇▅████████▇█▇██</td></tr><tr><td>loss_policy</td><td>▆▆▆▇▆▇▅▅▆▆▆▆▆▆▅▇▇▆▆▆▆▆▆▅▃▄█▆▆▆▇▇▇▇▅▁▆▇▆▆</td></tr><tr><td>running_reward</td><td>▁▂▆█▇▇▅▇█████▇▇▇██████████▇███████▇▇████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>392.965</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.00823</td></tr><tr><td>running_reward</td><td>98.34767</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard Run </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/942seubo' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/942seubo</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_164525-942seubo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:942seubo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_165415-wjqoqnw7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/wjqoqnw7' target=\"_blank\">Standard Run </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/wjqoqnw7' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/wjqoqnw7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 64.18655985126483\n",
      "Average Total reward: 117.45\n",
      "Running reward of episode 200/2000: 91.0291745439531\n",
      "Average Total reward: 224.75\n",
      "Running reward of episode 300/2000: 92.22766498992853\n",
      "Average Total reward: 483.15\n",
      "Running reward of episode 400/2000: 98.28326097775064\n",
      "Average Total reward: 489.15\n",
      "Running reward of episode 500/2000: 98.11194037135056\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 600/2000: 98.34567881654553\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 700/2000: 97.03570630367346\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 800/2000: 98.30298266780466\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 98.31993287170565\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 98.06477831623546\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.34783959792426\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.30849526477243\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 96.38438686032744\n",
      "Average Total reward: 289.3\n",
      "Running reward of episode 1400/2000: 76.51354741278506\n",
      "Average Total reward: 232.15\n",
      "Running reward of episode 1500/2000: 77.12174365374203\n",
      "Average Total reward: 201.25\n",
      "Running reward of episode 1600/2000: 88.6691795367799\n",
      "Average Total reward: 197.65\n",
      "Running reward of episode 1700/2000: 85.7153617329825\n",
      "Average Total reward: 290.85\n",
      "Running reward of episode 1800/2000: 97.44350420632337\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34404434268545\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 97.5714299710257\n",
      "Average Total reward: 490.4\n",
      "Average length of all episodes: 377.853\n",
      "Testing the best policy\n",
      "Average Total reward: 491.805\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wjqoqnw7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▃██████████▄▃▃▂▄███</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▃██████████▄▃▃▂▄███</td></tr><tr><td>episode_length</td><td>▁▂▃▄▃▆███▇█████████▇██▆███▂▂▄▂▅▄▃▃▇▇████</td></tr><tr><td>loss_policy</td><td>▆▁▄▂▄▇▅▆▇▅▆▇▆▇▆▆▆▆▆▃▆▆▅▆▆▄▂▅█▄▅▅▃▇▆▆▆▆▄▅</td></tr><tr><td>running_reward</td><td>▁▃▆▇▇█████████████████████▆▆▇▆▇▇▇▆██████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>490.4</td></tr><tr><td>average_lenght_all_episodes</td><td>377.853</td></tr><tr><td>average_total_reward</td><td>490.4</td></tr><tr><td>episode_length</td><td>499.5</td></tr><tr><td>loss_policy</td><td>-0.00849</td></tr><tr><td>running_reward</td><td>97.57143</td></tr><tr><td>test_average_episode_length</td><td>491.805</td></tr><tr><td>test_average_total_reward</td><td>491.805</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard Run </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/wjqoqnw7' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/wjqoqnw7</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_165415-wjqoqnw7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wjqoqnw7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_170335-3lqn7k1h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/3lqn7k1h' target=\"_blank\">Standard Run </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/3lqn7k1h' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/3lqn7k1h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 52.53875272596661\n",
      "Average Total reward: 176.35\n",
      "Running reward of episode 200/2000: 93.73580729045212\n",
      "Average Total reward: 453.9\n",
      "Running reward of episode 300/2000: 97.28332806210642\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 400/2000: 98.34321299886047\n",
      "Average Total reward: 494.7\n",
      "Running reward of episode 500/2000: 98.28014721429058\n",
      "Average Total reward: 483.55\n",
      "Running reward of episode 600/2000: 97.8272487276613\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 700/2000: 98.12234334143987\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 800/2000: 87.74643931950025\n",
      "Average Total reward: 343.55\n",
      "Running reward of episode 900/2000: 75.47979736878095\n",
      "Average Total reward: 172.85\n",
      "Running reward of episode 1000/2000: 97.0255859839134\n",
      "Average Total reward: 416.3\n",
      "Running reward of episode 1100/2000: 98.33920815719178\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 97.25882011425368\n",
      "Average Total reward: 241.9\n",
      "Running reward of episode 1300/2000: 98.27504069755041\n",
      "Average Total reward: 451.55\n",
      "Running reward of episode 1400/2000: 82.65850666934456\n",
      "Average Total reward: 402.65\n",
      "Running reward of episode 1500/2000: 96.97621568823882\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 97.81900097355421\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 96.26448882181084\n",
      "Average Total reward: 223.9\n",
      "Running reward of episode 1800/2000: 96.97351240982749\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34137872623823\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.33905534025644\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 423.1615\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard Run \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 1,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b48689-6e55-4f7d-aa56-c4ce6459e765",
   "metadata": {},
   "source": [
    "## Standard run and higher temperature (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef897b43-445f-467b-8e2e-8bd9e30fc6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3lqn7k1h) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▇█████▅▁▆█▂▇▆██▂███</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▇█████▅▁▆█▂▇▆██▂███</td></tr><tr><td>episode_length</td><td>▁▁▃▄▄███▇█▇██████▅▄█████▆█████▇███▄█████</td></tr><tr><td>loss_policy</td><td>▇▇▇▆▆▇██▇██▇▇█▇█▆▇▆▇▇▇█▆▆▇▇▇▇▇▁▇▆▇▆████▇</td></tr><tr><td>running_reward</td><td>▁▃▅▇█▇██████████▇█▇███████████▇███▇█████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>423.1615</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.01007</td></tr><tr><td>running_reward</td><td>98.33906</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard Run </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/3lqn7k1h' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/3lqn7k1h</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_170335-3lqn7k1h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3lqn7k1h). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_171425-66ej7p08</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/66ej7p08' target=\"_blank\">Standard and temperature 5 </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/66ej7p08' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/66ej7p08</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 25.929437324089378\n",
      "Average Total reward: 32.65\n",
      "Running reward of episode 200/2000: 62.213972986772724\n",
      "Average Total reward: 148.2\n",
      "Running reward of episode 300/2000: 82.60212577121675\n",
      "Average Total reward: 176.7\n",
      "Running reward of episode 400/2000: 93.96289815535772\n",
      "Average Total reward: 402.3\n",
      "Running reward of episode 500/2000: 96.0098544531907\n",
      "Average Total reward: 287.7\n",
      "Running reward of episode 600/2000: 97.31797903324576\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 700/2000: 98.03687329085803\n",
      "Average Total reward: 472.55\n",
      "Running reward of episode 800/2000: 97.9945584991854\n",
      "Average Total reward: 491.95\n",
      "Running reward of episode 900/2000: 96.43207559436021\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 98.28210346671098\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 96.3751670397502\n",
      "Average Total reward: 225.35\n",
      "Running reward of episode 1200/2000: 98.29695469917188\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.34715371968664\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.28031744559776\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.34911570363776\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34666290599958\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.3495085038747\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.28316031278133\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.349132534916\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34952312538488\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 415.677\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:66ej7p08) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▃▃▇▅█████▄█████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▃▃▇▅█████▄█████████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▂▄▂▆▇█▇███▇███▆█▇█▇█████████████████</td></tr><tr><td>loss_policy</td><td>▅▅▂▂▁▄▇▃▅▄▇▇▇▆▅▆█▇▃▆▃█▄█▆▇█▆▇█▅▆▆▇▄▅█▆▆▅</td></tr><tr><td>running_reward</td><td>▁▂▃▃▅▆▆▇████████████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>415.677</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.00603</td></tr><tr><td>running_reward</td><td>98.34952</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard and temperature 5 </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/66ej7p08' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/66ej7p08</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_171425-66ej7p08/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:66ej7p08). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_172435-f28s0ldp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/f28s0ldp' target=\"_blank\">Standard and temperature 5 </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/f28s0ldp' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/f28s0ldp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 23.881031570633947\n",
      "Average Total reward: 40.15\n",
      "Running reward of episode 200/2000: 57.736844261333516\n",
      "Average Total reward: 142.0\n",
      "Running reward of episode 300/2000: 94.86828897512726\n",
      "Average Total reward: 455.2\n",
      "Running reward of episode 400/2000: 97.04106218913063\n",
      "Average Total reward: 482.9\n",
      "Running reward of episode 500/2000: 98.08374884239758\n",
      "Average Total reward: 462.8\n",
      "Running reward of episode 600/2000: 97.67164312127268\n",
      "Average Total reward: 478.95\n",
      "Running reward of episode 700/2000: 97.84771709146843\n",
      "Average Total reward: 437.2\n",
      "Running reward of episode 800/2000: 98.32861910395586\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 94.66916917965473\n",
      "Average Total reward: 475.8\n",
      "Running reward of episode 1000/2000: 98.18605883078813\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.26400679820104\n",
      "Average Total reward: 461.85\n",
      "Running reward of episode 1200/2000: 93.27029610850911\n",
      "Average Total reward: 448.0\n",
      "Running reward of episode 1300/2000: 98.31623961448183\n",
      "Average Total reward: 454.4\n",
      "Running reward of episode 1400/2000: 96.86411237301719\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.24370214549887\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 93.63175974470784\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 97.5030216207656\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.344513700994\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 96.8483474498184\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34063768343513\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 420.7645\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:f28s0ldp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▃▇█▇█▇███▇▇▇███████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▃▇█▇█▇███▇▇▇███████</td></tr><tr><td>episode_length</td><td>▁▁▂▂▄▅▇▇▇█▇▆█████▆█████▄███████▆████████</td></tr><tr><td>loss_policy</td><td>▆▇▁▄▆▇▆▅▆▆█▆▆▅▆▆▅▇▇▅▆▆▇▆▆▇▄▅▅▅▄█▅▇▆▄▄▅▇▅</td></tr><tr><td>running_reward</td><td>▁▂▃▄▅▇███████████▇█████▇████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>420.7645</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.00822</td></tr><tr><td>running_reward</td><td>98.34064</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard and temperature 5 </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/f28s0ldp' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/f28s0ldp</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_172435-f28s0ldp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:f28s0ldp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_173455-t3xk0xfd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/t3xk0xfd' target=\"_blank\">Standard and temperature 5 </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/t3xk0xfd' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/t3xk0xfd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 24.97650669296172\n",
      "Average Total reward: 30.1\n",
      "Running reward of episode 200/2000: 50.650234553148884\n",
      "Average Total reward: 114.2\n",
      "Running reward of episode 300/2000: 93.27920996501801\n",
      "Average Total reward: 403.0\n",
      "Running reward of episode 400/2000: 96.99587821207643\n",
      "Average Total reward: 472.95\n",
      "Running reward of episode 500/2000: 96.19530445629833\n",
      "Average Total reward: 267.7\n",
      "Running reward of episode 600/2000: 94.957603327465\n",
      "Average Total reward: 480.75\n",
      "Running reward of episode 700/2000: 92.80265601524734\n",
      "Average Total reward: 379.2\n",
      "Running reward of episode 800/2000: 92.58579432546331\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 98.28942081517984\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 97.86123264788272\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.34663449984711\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.34950833569528\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.24452519022364\n",
      "Average Total reward: 478.85\n",
      "Running reward of episode 1400/2000: 97.19648093998231\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.34269881793622\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34850716701838\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.18264502906281\n",
      "Average Total reward: 476.2\n",
      "Running reward of episode 1800/2000: 97.985702290408\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 96.58701336828445\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.33909044736914\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 409.4595\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:t3xk0xfd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▂▇█▅█▆█████████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▂▇█▅█▆█████████████</td></tr><tr><td>episode_length</td><td>▁▁▁▂▃▆▆▅███▇█▆▃▇██████████▆█████▇████▄██</td></tr><tr><td>loss_policy</td><td>▃▂▃█▄▄▄▁▄▇▄▂▃▂▂▂▇▅▆▅▅▃▆▆▅▂▂▆▄▆▄▅▃▃▄▂▂▄▁▄</td></tr><tr><td>running_reward</td><td>▁▂▂▃▅▇████████▇███████████▇█████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>409.4595</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.004</td></tr><tr><td>running_reward</td><td>98.33909</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard and temperature 5 </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/t3xk0xfd' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/t3xk0xfd</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_173455-t3xk0xfd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:t3xk0xfd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_174446-fx0pw6iw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/fx0pw6iw' target=\"_blank\">Standard and temperature 5 </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/fx0pw6iw' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/fx0pw6iw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 23.553795438268196\n",
      "Average Total reward: 36.95\n",
      "Running reward of episode 200/2000: 56.55064634220968\n",
      "Average Total reward: 142.8\n",
      "Running reward of episode 300/2000: 91.64790728972982\n",
      "Average Total reward: 404.05\n",
      "Running reward of episode 400/2000: 94.10095109490055\n",
      "Average Total reward: 442.35\n",
      "Running reward of episode 500/2000: 98.0458981284741\n",
      "Average Total reward: 487.9\n",
      "Running reward of episode 600/2000: 98.34152819277885\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 700/2000: 98.34351303575056\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 800/2000: 98.34947365516523\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 97.98864749088678\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 98.25435069169532\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.34896196671255\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 96.88332388828063\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.3364343989988\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 97.84215880035453\n",
      "Average Total reward: 484.75\n",
      "Running reward of episode 1500/2000: 98.214585950989\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34872653840326\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.34952072167066\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.34952542365592\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34952545149413\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34952545165896\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 431.7595\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fx0pw6iw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▃▇▇████████████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▃▇▇████████████████</td></tr><tr><td>episode_length</td><td>▁▁▁▂▃▆▆▅████████▆▇████▇█████▇███████████</td></tr><tr><td>loss_policy</td><td>▅▆▁▃▃▂▅▆▇▆▆▅█▇▇▆██▅▅▇▆▇▆▇▆▇▇▄▄█▇▆▅▇▇▇▇▇▇</td></tr><tr><td>running_reward</td><td>▁▂▂▃▅▇█▇████████████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>431.7595</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.00168</td></tr><tr><td>running_reward</td><td>98.34953</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard and temperature 5 </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/fx0pw6iw' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/fx0pw6iw</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_174446-fx0pw6iw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fx0pw6iw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_175523-dlqcw41a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/dlqcw41a' target=\"_blank\">Standard and temperature 5 </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/dlqcw41a' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/dlqcw41a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 26.83923487458664\n",
      "Average Total reward: 37.55\n",
      "Running reward of episode 200/2000: 74.51195438229841\n",
      "Average Total reward: 315.1\n",
      "Running reward of episode 300/2000: 92.18660611213245\n",
      "Average Total reward: 456.75\n",
      "Running reward of episode 400/2000: 97.48870334249546\n",
      "Average Total reward: 473.15\n",
      "Running reward of episode 500/2000: 97.1706223988939\n",
      "Average Total reward: 480.8\n",
      "Running reward of episode 600/2000: 98.26989274668249\n",
      "Average Total reward: 477.4\n",
      "Running reward of episode 700/2000: 98.16480313211207\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 800/2000: 98.0017380198416\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 98.32794873415861\n",
      "Average Total reward: 478.8\n",
      "Running reward of episode 1000/2000: 97.97978904073473\n",
      "Average Total reward: 447.55\n",
      "Running reward of episode 1100/2000: 98.14453882088127\n",
      "Average Total reward: 487.2\n",
      "Running reward of episode 1200/2000: 96.58090034173614\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.33905425501682\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.32108861951143\n",
      "Average Total reward: 499.75\n",
      "Running reward of episode 1500/2000: 98.34913927831523\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34952316530936\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.34952543812354\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.34952545157982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34952545165947\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 435.186\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard and temperature 5 \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98a63e-5e66-4ecc-98b7-ce1341a1e81a",
   "metadata": {},
   "source": [
    "## Standard run and higher temperature (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "578621a9-c76a-46c9-8a80-9b9411c7ee0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:tocar8bs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▁▂▂▄▆█▇▆██▆██████</td></tr><tr><td>average_total_reward</td><td>▁▁▁▂▂▄▆█▇▆██▆██████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▂▂▁▂▂▃▃▃▅▆█▆▇▆▆▆██▇███▆▇██████████▇█</td></tr><tr><td>loss_policy</td><td>▅▆▅▄▇▁▄▅▇▆▄▆▆▅▆▅▆▅▄▅▆▆▃▆▇▆▆▅▅▆▆▅▆▆▆▆▆▆▅█</td></tr><tr><td>running_reward</td><td>▁▆▆▇████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.00237</td></tr><tr><td>running_reward</td><td>9.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard and gamma 0.9 </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/tocar8bs' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/tocar8bs</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_180412-tocar8bs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:tocar8bs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_181033-8chjp3by</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8chjp3by' target=\"_blank\">Standard and temperature 10 </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8chjp3by' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8chjp3by</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 22.16568026537247\n",
      "Average Total reward: 29.45\n",
      "Running reward of episode 200/2000: 35.74352037674666\n",
      "Average Total reward: 55.05\n",
      "Running reward of episode 300/2000: 70.49331650832612\n",
      "Average Total reward: 120.8\n",
      "Running reward of episode 400/2000: 85.458920615001\n",
      "Average Total reward: 202.4\n",
      "Running reward of episode 500/2000: 91.57265888488924\n",
      "Average Total reward: 372.55\n",
      "Running reward of episode 600/2000: 89.44259666828034\n",
      "Average Total reward: 208.3\n",
      "Running reward of episode 700/2000: 92.22526037222525\n",
      "Average Total reward: 293.85\n",
      "Running reward of episode 800/2000: 96.1536516069143\n",
      "Average Total reward: 436.5\n",
      "Running reward of episode 900/2000: 95.63969546835189\n",
      "Average Total reward: 486.35\n",
      "Running reward of episode 1000/2000: 98.220242553561\n",
      "Average Total reward: 437.0\n",
      "Running reward of episode 1100/2000: 98.29579112130567\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 97.97254742622233\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.34722436105578\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.19224110642925\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.34847511264871\n",
      "Average Total reward: 479.35\n",
      "Running reward of episode 1600/2000: 98.30828912271858\n",
      "Average Total reward: 493.05\n",
      "Running reward of episode 1700/2000: 97.85845892919055\n",
      "Average Total reward: 467.5\n",
      "Running reward of episode 1800/2000: 94.55330319064788\n",
      "Average Total reward: 318.6\n",
      "Running reward of episode 1900/2000: 97.9378642232078\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.27888257905221\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 369.6655\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8chjp3by) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▄▆▄▅▇█▇███████▅██</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▂▄▆▄▅▇█▇███████▅██</td></tr><tr><td>episode_length</td><td>▁▁▁▁▂▃▃▅▅▄▅▅▅▅▆█▇▇████████████████▇█▇███</td></tr><tr><td>loss_policy</td><td>▆▄▇▃▅▁▇▆▅▆▂▅▆▆▂▆▄▅▇▇▅▆▆█▆▆▇▇▅▇▆▅▆█▃▆▇▄▆▇</td></tr><tr><td>running_reward</td><td>▁▂▂▂▃▄▆▇▇▇██▇▇▇█████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>369.6655</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.00391</td></tr><tr><td>running_reward</td><td>98.27888</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard and temperature 10 </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8chjp3by' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8chjp3by</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_181033-8chjp3by/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8chjp3by). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_182008-xgcwdgbk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/xgcwdgbk' target=\"_blank\">Standard and temperature 10 </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/xgcwdgbk' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/xgcwdgbk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 22.93964205123031\n",
      "Average Total reward: 39.45\n",
      "Running reward of episode 200/2000: 41.18739140270995\n",
      "Average Total reward: 57.1\n",
      "Running reward of episode 300/2000: 78.89411137546924\n",
      "Average Total reward: 217.15\n",
      "Running reward of episode 400/2000: 84.41578798734405\n",
      "Average Total reward: 359.5\n",
      "Running reward of episode 500/2000: 95.60180401230649\n",
      "Average Total reward: 451.3\n",
      "Running reward of episode 600/2000: 97.92725010766306\n",
      "Average Total reward: 459.7\n",
      "Running reward of episode 700/2000: 95.27069536172121\n",
      "Average Total reward: 488.55\n",
      "Running reward of episode 800/2000: 98.01595174989652\n",
      "Average Total reward: 473.15\n",
      "Running reward of episode 900/2000: 97.91987447672201\n",
      "Average Total reward: 498.5\n",
      "Running reward of episode 1000/2000: 97.91030455652094\n",
      "Average Total reward: 496.5\n",
      "Running reward of episode 1100/2000: 98.07849872461578\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.34792083000299\n",
      "Average Total reward: 490.35\n",
      "Running reward of episode 1300/2000: 98.27817961398614\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 97.34954934012178\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.34360506387192\n",
      "Average Total reward: 491.65\n",
      "Running reward of episode 1600/2000: 98.34949039983104\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.34951135482194\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.34582615192002\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 97.9718431222899\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 97.93907633180949\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 405.7335\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xgcwdgbk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▄▆▇▇██████████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▄▆▇▇██████████████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▂▃▄▃▆▅▇▇▇███▇███████████████████████</td></tr><tr><td>loss_policy</td><td>▅▅▂▃▇▅▅▅█▁▆▄▄▄▅▅▅▅▅█▇▆▇▆▅▆▅▆▅▆▅▅▆▅▇▇▆▆▅█</td></tr><tr><td>running_reward</td><td>▁▂▂▃▃▅▇▇▇▇██████████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>405.7335</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.00832</td></tr><tr><td>running_reward</td><td>97.93908</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard and temperature 10 </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/xgcwdgbk' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/xgcwdgbk</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_182008-xgcwdgbk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xgcwdgbk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_183022-ptzs01hv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ptzs01hv' target=\"_blank\">Standard and temperature 10 </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ptzs01hv' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ptzs01hv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 23.698670489439746\n",
      "Average Total reward: 30.45\n",
      "Running reward of episode 200/2000: 43.45003506164022\n",
      "Average Total reward: 62.2\n",
      "Running reward of episode 300/2000: 77.91009980628179\n",
      "Average Total reward: 211.05\n",
      "Running reward of episode 400/2000: 92.87547093184334\n",
      "Average Total reward: 298.95\n",
      "Running reward of episode 500/2000: 94.38419736565884\n",
      "Average Total reward: 423.3\n",
      "Running reward of episode 600/2000: 96.8122575933616\n",
      "Average Total reward: 492.5\n",
      "Running reward of episode 700/2000: 96.41839687834289\n",
      "Average Total reward: 335.85\n",
      "Running reward of episode 800/2000: 97.59170765350223\n",
      "Average Total reward: 474.75\n",
      "Running reward of episode 900/2000: 92.97206822071841\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 95.01416784366324\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 96.92506116702563\n",
      "Average Total reward: 403.5\n",
      "Running reward of episode 1200/2000: 95.61721605405036\n",
      "Average Total reward: 491.85\n",
      "Running reward of episode 1300/2000: 96.00941206145929\n",
      "Average Total reward: 478.95\n",
      "Running reward of episode 1400/2000: 98.26222471339906\n",
      "Average Total reward: 350.9\n",
      "Running reward of episode 1500/2000: 90.70865059997631\n",
      "Average Total reward: 476.55\n",
      "Running reward of episode 1600/2000: 94.40457869944454\n",
      "Average Total reward: 473.2\n",
      "Running reward of episode 1700/2000: 97.07393222137223\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.32693967819064\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34939173192818\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34952465996838\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 391.0045\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ptzs01hv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▄▅▇█▆███▇██▆██████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▄▅▇█▆███▇██▆██████</td></tr><tr><td>episode_length</td><td>▁▁▁▂▂▄▅▇▆▄▇▇███▇▇▄▇▄█▇▇██▆███▇█▄▇█▇█████</td></tr><tr><td>loss_policy</td><td>▅█▁▆▄▅▃▆▇▄▆▅▅▅▅▇▂▂▆▄▆▃▅▅▄▆▇▆▅▄▅▅▄▇▆▆▅▆▄▆</td></tr><tr><td>running_reward</td><td>▁▂▂▃▄▆▇█▇▇███████▇█▇█████▇███▇█▇████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>391.0045</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.00021</td></tr><tr><td>running_reward</td><td>98.34952</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard and temperature 10 </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ptzs01hv' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ptzs01hv</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_183022-ptzs01hv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ptzs01hv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_184021-fend34zo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/fend34zo' target=\"_blank\">Standard and temperature 10 </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/fend34zo' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/fend34zo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 20.354387225086743\n",
      "Average Total reward: 21.25\n",
      "Running reward of episode 200/2000: 34.538424925053995\n",
      "Average Total reward: 56.3\n",
      "Running reward of episode 300/2000: 82.26923967252634\n",
      "Average Total reward: 300.6\n",
      "Running reward of episode 400/2000: 92.77205541514661\n",
      "Average Total reward: 457.7\n",
      "Running reward of episode 500/2000: 96.81175716839374\n",
      "Average Total reward: 441.65\n",
      "Running reward of episode 600/2000: 97.73970607537439\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 700/2000: 98.24179671261828\n",
      "Average Total reward: 474.35\n",
      "Running reward of episode 800/2000: 96.50784829359428\n",
      "Average Total reward: 441.6\n",
      "Running reward of episode 900/2000: 98.32422065570984\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 98.2022674479129\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.11677271625855\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 97.21286433468465\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.2745573611487\n",
      "Average Total reward: 488.3\n",
      "Running reward of episode 1400/2000: 98.09141097916557\n",
      "Average Total reward: 485.45\n",
      "Running reward of episode 1500/2000: 98.06742696283465\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.1404562690665\n",
      "Average Total reward: 477.45\n",
      "Running reward of episode 1700/2000: 96.47050329015633\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.25754979759064\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.09930482746505\n",
      "Average Total reward: 480.75\n",
      "Running reward of episode 2000/2000: 97.38790359304687\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 414.981\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fend34zo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▂▅▇▇██▇████████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▂▅▇▇██▇████████████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▂▃▅▆▇▇▇████▇█████████████████▇▇█████</td></tr><tr><td>loss_policy</td><td>▆▆▆▁▆▂▆▇▅▆▄▆▇▇▅▆▆▆▆▅▆▅▇█▆▆▆▇▆▆█▆▅▄▆▆▆█▇▇</td></tr><tr><td>running_reward</td><td>▁▂▂▂▃▅▇█████████████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>414.981</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.00284</td></tr><tr><td>running_reward</td><td>97.3879</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard and temperature 10 </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/fend34zo' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/fend34zo</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_184021-fend34zo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fend34zo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_185103-ehofp8w8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ehofp8w8' target=\"_blank\">Standard and temperature 10 </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ehofp8w8' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ehofp8w8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 21.625619198686906\n",
      "Average Total reward: 24.8\n",
      "Running reward of episode 200/2000: 30.18874240659956\n",
      "Average Total reward: 44.25\n",
      "Running reward of episode 300/2000: 57.30448430834697\n",
      "Average Total reward: 109.55\n",
      "Running reward of episode 400/2000: 82.85195281131075\n",
      "Average Total reward: 131.45\n",
      "Running reward of episode 500/2000: 87.18454569224969\n",
      "Average Total reward: 221.15\n",
      "Running reward of episode 600/2000: 93.18461871015343\n",
      "Average Total reward: 231.55\n",
      "Running reward of episode 700/2000: 97.02780971509172\n",
      "Average Total reward: 479.1\n",
      "Running reward of episode 800/2000: 96.91635901242573\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 94.43567864952625\n",
      "Average Total reward: 483.4\n",
      "Running reward of episode 1000/2000: 98.19639645566181\n",
      "Average Total reward: 483.3\n",
      "Running reward of episode 1100/2000: 98.34861884696464\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.04214180364336\n",
      "Average Total reward: 410.15\n",
      "Running reward of episode 1300/2000: 97.99311440855871\n",
      "Average Total reward: 498.4\n",
      "Running reward of episode 1400/2000: 98.258310686518\n",
      "Average Total reward: 490.65\n",
      "Running reward of episode 1500/2000: 98.29379932769454\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.26784720378761\n",
      "Average Total reward: 495.0\n",
      "Running reward of episode 1700/2000: 94.25511439518674\n",
      "Average Total reward: 487.8\n",
      "Running reward of episode 1800/2000: 97.78513079064948\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 96.16528625428387\n",
      "Average Total reward: 465.4\n",
      "Running reward of episode 2000/2000: 98.30638037600501\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 376.759\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard and temperature 10 \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 10,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e2878-19d2-410b-bb75-e468082ad22b",
   "metadata": {},
   "source": [
    "## Standard run and lower gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41e338ef-b678-433f-8404-3ab28b34b287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ehofp8w8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▃▄▄█████▇██████▇█</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▂▃▄▄█████▇██████▇█</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▁▃▆▃▅▄▆▅████████████▇███████▇███▇██</td></tr><tr><td>loss_policy</td><td>▄▆▁▃█▅▃▄▄▆▁▃▃▄▄▃▅▅▄▄▅▅▄▄▅▂▅▅▆▅▅▅▄▂▅▃▄▅▅▅</td></tr><tr><td>running_reward</td><td>▁▂▂▂▃▄▅▇▆▇▇█▇██▇████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>376.759</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.00401</td></tr><tr><td>running_reward</td><td>98.30638</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard and temperature 10 </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ehofp8w8' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ehofp8w8</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_185103-ehofp8w8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ehofp8w8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_190141-keukao78</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/keukao78' target=\"_blank\">Standard and gamma 0.9 </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/keukao78' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/keukao78</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 7.994547748675325\n",
      "Average Total reward: 33.1\n",
      "Running reward of episode 200/2000: 8.607860377723597\n",
      "Average Total reward: 61.0\n",
      "Running reward of episode 300/2000: 8.923415512879773\n",
      "Average Total reward: 59.9\n",
      "Running reward of episode 400/2000: 8.983670079746867\n",
      "Average Total reward: 106.75\n",
      "Running reward of episode 500/2000: 8.988613969397555\n",
      "Average Total reward: 108.15\n",
      "Running reward of episode 600/2000: 8.999476674461715\n",
      "Average Total reward: 251.4\n",
      "Running reward of episode 700/2000: 8.999936398746492\n",
      "Average Total reward: 360.45\n",
      "Running reward of episode 800/2000: 8.999999622998956\n",
      "Average Total reward: 467.25\n",
      "Running reward of episode 900/2000: 8.998725503422172\n",
      "Average Total reward: 407.3\n",
      "Running reward of episode 1000/2000: 8.993508044397183\n",
      "Average Total reward: 346.5\n",
      "Running reward of episode 1100/2000: 8.999961371523598\n",
      "Average Total reward: 480.25\n",
      "Running reward of episode 1200/2000: 8.99968454111362\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 8.999998132316422\n",
      "Average Total reward: 343.75\n",
      "Running reward of episode 1400/2000: 8.998969062518439\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 8.994180729782984\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 8.999965546840615\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 8.999999796019042\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 8.999999998792303\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 8.999999999992825\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 8.999999714210952\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 340.3505\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:keukao78) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▁▂▂▄▆█▇▆██▆███████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▁▂▂▄▆█▇▆██▆███████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▂▂▁▂▂▃▃▃▅▆██▇▄▆▃██▇▇███▆████████████</td></tr><tr><td>loss_policy</td><td>▆▆▅▄█▁▄▅▇▆▅▆▆▆▇▇▇▃▄▅▆▆▃▆▇▆▆▇▄▆█▆█▇▇▆█▇▅▇</td></tr><tr><td>running_reward</td><td>▁▆▆▇████████████████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>340.3505</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.00283</td></tr><tr><td>running_reward</td><td>9.0</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard and gamma 0.9 </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/keukao78' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/keukao78</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_190141-keukao78/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:keukao78). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_191113-rdawnuuu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/rdawnuuu' target=\"_blank\">Standard and gamma 0.9 </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/rdawnuuu' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/rdawnuuu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 8.610700356876613\n",
      "Average Total reward: 53.5\n",
      "Running reward of episode 200/2000: 8.93692639304487\n",
      "Average Total reward: 70.75\n",
      "Running reward of episode 300/2000: 8.99012332073504\n",
      "Average Total reward: 163.6\n",
      "Running reward of episode 400/2000: 8.999940542148764\n",
      "Average Total reward: 371.9\n",
      "Running reward of episode 500/2000: 8.924227289187757\n",
      "Average Total reward: 146.5\n",
      "Running reward of episode 600/2000: 8.98158204562783\n",
      "Average Total reward: 417.7\n",
      "Running reward of episode 700/2000: 8.952451993457252\n",
      "Average Total reward: 498.35\n",
      "Running reward of episode 800/2000: 8.999718475491187\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 8.999998333225896\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 8.999999988882266\n",
      "Average Total reward: 482.55\n",
      "Running reward of episode 1100/2000: 8.99831160481705\n",
      "Average Total reward: 395.85\n",
      "Running reward of episode 1200/2000: 8.999990003806957\n",
      "Average Total reward: 343.0\n",
      "Running reward of episode 1300/2000: 8.999999940817226\n",
      "Average Total reward: 494.6\n",
      "Running reward of episode 1400/2000: 8.999999999649583\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 8.992357130406031\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 8.999954750167218\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 8.99999973209702\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 8.999999998413854\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 8.999999999990587\n",
      "Average Total reward: 382.5\n",
      "Running reward of episode 2000/2000: 8.98798754758002\n",
      "Average Total reward: 213.95\n",
      "Average length of all episodes: 372.5655\n",
      "Testing the best policy\n",
      "Average Total reward: 207.78\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:rdawnuuu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▃▆▂▇████▆▆██████▆▄</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▃▆▂▇████▆▆██████▆▄</td></tr><tr><td>episode_length</td><td>▁▁▁▂▂▂▄▅▇▃▄▄▇█▇██████▆▇█▆███▄▄████████▄▄</td></tr><tr><td>loss_policy</td><td>▆▆▇▆▅▆▇▅▆▅▆▅▆▅█▆▇▆█▇▆▇▆▇▇▇▆▆▅▁▅▆▆▆▅▆▆▅▄▅</td></tr><tr><td>running_reward</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>213.95</td></tr><tr><td>average_lenght_all_episodes</td><td>372.5655</td></tr><tr><td>average_total_reward</td><td>213.95</td></tr><tr><td>episode_length</td><td>249.6</td></tr><tr><td>loss_policy</td><td>-0.01006</td></tr><tr><td>running_reward</td><td>8.98799</td></tr><tr><td>test_average_episode_length</td><td>207.78</td></tr><tr><td>test_average_total_reward</td><td>207.78</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard and gamma 0.9 </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/rdawnuuu' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/rdawnuuu</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_191113-rdawnuuu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:rdawnuuu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_191955-81tke6eu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/81tke6eu' target=\"_blank\">Standard and gamma 0.9 </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/81tke6eu' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/81tke6eu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 7.71236728667254\n",
      "Average Total reward: 29.85\n",
      "Running reward of episode 200/2000: 7.5045824692438705\n",
      "Average Total reward: 19.75\n",
      "Running reward of episode 300/2000: 8.844400446374683\n",
      "Average Total reward: 61.6\n",
      "Running reward of episode 400/2000: 8.976467059383236\n",
      "Average Total reward: 99.7\n",
      "Running reward of episode 500/2000: 8.972381102171436\n",
      "Average Total reward: 220.9\n",
      "Running reward of episode 600/2000: 8.99736446464825\n",
      "Average Total reward: 302.95\n",
      "Running reward of episode 700/2000: 8.999719314555634\n",
      "Average Total reward: 426.9\n",
      "Running reward of episode 800/2000: 8.953743257278715\n",
      "Average Total reward: 481.2\n",
      "Running reward of episode 900/2000: 8.999705806784347\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 8.999998249774524\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 8.999999378501848\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 8.999934588265658\n",
      "Average Total reward: 491.9\n",
      "Running reward of episode 1300/2000: 8.999999610293788\n",
      "Average Total reward: 496.1\n",
      "Running reward of episode 1400/2000: 8.99971997741657\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 8.999776192671606\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 8.999998674942153\n",
      "Average Total reward: 490.45\n",
      "Running reward of episode 1700/2000: 8.999999992154935\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 8.999999999953532\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 8.99999998006217\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 8.99990724201374\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 358.531\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:81tke6eu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▂▄▅▇█████████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▂▂▄▅▇█████████████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▁▂▂▂▂▃▄▇█▇▇████████████▇█████▇▇████</td></tr><tr><td>loss_policy</td><td>▆▇▃▁▄▆▆▇▇█▆█▆▇▇▆▇▆▆▇▆▇▇▇█▆▇▆▆▇▇▆▆▇▆▆▆▇▆▆</td></tr><tr><td>running_reward</td><td>▁▆▆▆▆▇██████████████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>358.531</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.00113</td></tr><tr><td>running_reward</td><td>8.99991</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard and gamma 0.9 </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/81tke6eu' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/81tke6eu</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_191955-81tke6eu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:81tke6eu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_192903-vuwpmh63</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/vuwpmh63' target=\"_blank\">Standard and gamma 0.9 </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/vuwpmh63' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/vuwpmh63</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 7.806123420852687\n",
      "Average Total reward: 24.85\n",
      "Running reward of episode 200/2000: 8.839288587842182\n",
      "Average Total reward: 66.45\n",
      "Running reward of episode 300/2000: 8.985362091762077\n",
      "Average Total reward: 215.35\n",
      "Running reward of episode 400/2000: 8.992356651845018\n",
      "Average Total reward: 235.9\n",
      "Running reward of episode 500/2000: 8.999753145151516\n",
      "Average Total reward: 299.2\n",
      "Running reward of episode 600/2000: 8.998333289476468\n",
      "Average Total reward: 320.35\n",
      "Running reward of episode 700/2000: 8.968956133204532\n",
      "Average Total reward: 413.95\n",
      "Running reward of episode 800/2000: 8.990612746779243\n",
      "Average Total reward: 455.5\n",
      "Running reward of episode 900/2000: 8.99539400955497\n",
      "Average Total reward: 489.6\n",
      "Running reward of episode 1000/2000: 8.999954090911041\n",
      "Average Total reward: 467.45\n",
      "Running reward of episode 1100/2000: 8.999791028781559\n",
      "Average Total reward: 472.75\n",
      "Running reward of episode 1200/2000: 8.999998709643604\n",
      "Average Total reward: 269.1\n",
      "Running reward of episode 1300/2000: 8.998815999233226\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 8.999992990088842\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 8.999999958497595\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 8.99999999975426\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 8.999994871287992\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 8.999999969635285\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 8.997909773358195\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 8.999987624752071\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 374.3055\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vuwpmh63) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▂▄▄▅▅▇▇███▅████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▂▄▄▅▅▇▇███▅████████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▂▂▃▃▆█▄▄▆▆▇▇▇█▇█▇▇█▅▆███████████████</td></tr><tr><td>loss_policy</td><td>▅▆▁▂▅▇▆██▅▇▅▅▆▅▆▅▇▅▅▅▆▆▆▅▆▆▆▆▆▅▇▆▆▅▆▅▅▆▆</td></tr><tr><td>running_reward</td><td>▁▆▆▇████████████████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>374.3055</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.0042</td></tr><tr><td>running_reward</td><td>8.99999</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard and gamma 0.9 </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/vuwpmh63' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/vuwpmh63</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_192903-vuwpmh63/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vuwpmh63). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_193749-jdb5ilz6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/jdb5ilz6' target=\"_blank\">Standard and gamma 0.9 </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/jdb5ilz6' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/jdb5ilz6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/2000: 8.37756749381354\n",
      "Average Total reward: 32.35\n",
      "Running reward of episode 200/2000: 8.714103535021597\n",
      "Average Total reward: 69.7\n",
      "Running reward of episode 300/2000: 8.972797166807794\n",
      "Average Total reward: 137.5\n",
      "Running reward of episode 400/2000: 8.991433354967826\n",
      "Average Total reward: 384.3\n",
      "Running reward of episode 500/2000: 8.994922394511539\n",
      "Average Total reward: 338.15\n",
      "Running reward of episode 600/2000: 8.972180263806955\n",
      "Average Total reward: 465.7\n",
      "Running reward of episode 700/2000: 8.999828892203816\n",
      "Average Total reward: 444.25\n",
      "Running reward of episode 800/2000: 8.999941640250848\n",
      "Average Total reward: 376.7\n",
      "Running reward of episode 900/2000: 8.99989967599757\n",
      "Average Total reward: 495.45\n",
      "Running reward of episode 1000/2000: 8.999999405699516\n",
      "Average Total reward: 490.9\n",
      "Running reward of episode 1100/2000: 8.999999996481405\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 8.999999926835704\n",
      "Average Total reward: 470.45\n",
      "Running reward of episode 1300/2000: 8.999999999566809\n",
      "Average Total reward: 488.3\n",
      "Running reward of episode 1400/2000: 8.99999934650454\n",
      "Average Total reward: 492.1\n",
      "Running reward of episode 1500/2000: 8.993825031635767\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 8.999963436184622\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 8.99999977784049\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 8.99997460523957\n",
      "Average Total reward: 240.9\n",
      "Running reward of episode 1900/2000: 8.999999838966087\n",
      "Average Total reward: 454.45\n",
      "Running reward of episode 2000/2000: 8.999999935545734\n",
      "Average Total reward: 499.15\n",
      "Average length of all episodes: 387.739\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Standard and gamma 0.9 \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.9,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5adad7-759b-4000-925b-701f41fe6e97",
   "metadata": {
    "id": "2a5adad7-759b-4000-925b-701f41fe6e97"
   },
   "source": [
    "-----\n",
    "## Exercise 2: `REINFORCE` with a Value Baseline (warm up)\n",
    "\n",
    "In this exercise we will augment my implementation (or your own) of `REINFORCE` to subtract a baseline from the target in the update equation in order to stabilize (and hopefully speed-up) convergence. For now we will stick to the Cartpole environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07",
   "metadata": {
    "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07"
   },
   "source": [
    "**First Things First**: Recall from the slides on Deep Reinforcement Learning that we can **subtract** any function that doesn't depend on the current action from the q-value without changing the (maximum of our) objecttive function $J$:  \n",
    "\n",
    "$$ \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_{s} \\mu(s) \\sum_a \\left( q_{\\pi}(s, a) - b(s) \\right) \\nabla \\pi(a \\mid s, \\boldsymbol{\\theta}) $$\n",
    "\n",
    "In `REINFORCE` this means we can subtract from our target $G_t$:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - b(S_t)) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
    "\n",
    "Since we are only interested in the **maximum** of our objective, we can also **rescale** our target by any function that also doesn't depend on the action. A **simple baseline** which is even independent of the state -- that is, it is **constant** for each episode -- is to just **standardize rewards within the episode**. So, we **subtract** the average return and **divide** by the variance of returns:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha \\left(\\frac{G_t - \\bar{G}}{\\sigma_G}\\right) \\nabla  \\pi(A_t \\mid s, \\boldsymbol{\\theta}) $$\n",
    "\n",
    "This baseline is **already** implemented in my implementation of `REINFORCE`. Experiment with and without this standardization baseline and compare the performance. We are going to do something more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888d7e7-57f7-424a-98b1-21717b266bdb",
   "metadata": {},
   "source": [
    "## Without the baseline\n",
    "Since I already experimented with Reinforce with the standard baseline the next runs will not cointain a baseline of any kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faf5b245-9369-4ed9-b5f4-3a52bb975d3f",
   "metadata": {
    "id": "faf5b245-9369-4ed9-b5f4-3a52bb975d3f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jdb5ilz6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▂▃▆▆▇▇▆█████████▄▇█</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▂▃▆▆▇▇▆█████████▄▇█</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▂▃▄▆▅▆▆█▆▇▇▆█████▇█▇█▇██▇██▇█▇████▇</td></tr><tr><td>loss_policy</td><td>▁▆▁▂▆▅▄▅▄▅▃▃▄█▄▃▃▅▄▄▅▅▄▄▅▆▅▅▅▅▃▄▅▇▆▅▄▄▄▄</td></tr><tr><td>running_reward</td><td>▁▆▇█▇███████████████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>499.15</td></tr><tr><td>average_lenght_all_episodes</td><td>387.739</td></tr><tr><td>average_total_reward</td><td>499.15</td></tr><tr><td>episode_length</td><td>459.7</td></tr><tr><td>loss_policy</td><td>0.00077</td></tr><tr><td>running_reward</td><td>9.0</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Standard and gamma 0.9 </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/jdb5ilz6' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/jdb5ilz6</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_193749-jdb5ilz6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jdb5ilz6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_194729-4ngffdb4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/4ngffdb4' target=\"_blank\">Without the baseline </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/4ngffdb4' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/4ngffdb4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with no baseline.\n",
      "Running reward of episode 100/2000: 20.367188207392463\n",
      "Average Total reward: 23.6\n",
      "Running reward of episode 200/2000: 28.356962689132324\n",
      "Average Total reward: 32.65\n",
      "Running reward of episode 300/2000: 50.196384699792084\n",
      "Average Total reward: 97.05\n",
      "Running reward of episode 400/2000: 77.38302344667393\n",
      "Average Total reward: 164.2\n",
      "Running reward of episode 500/2000: 89.90828410855038\n",
      "Average Total reward: 358.75\n",
      "Running reward of episode 600/2000: 73.04345409524544\n",
      "Average Total reward: 270.0\n",
      "Running reward of episode 700/2000: 66.43086691635207\n",
      "Average Total reward: 185.9\n",
      "Running reward of episode 800/2000: 75.47564028859684\n",
      "Average Total reward: 209.3\n",
      "Running reward of episode 900/2000: 94.2267822808414\n",
      "Average Total reward: 342.45\n",
      "Running reward of episode 1000/2000: 92.01660900812905\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.3102797721196\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.34929309646748\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 92.68345565634851\n",
      "Average Total reward: 376.1\n",
      "Running reward of episode 1400/2000: 89.95125500848417\n",
      "Average Total reward: 295.8\n",
      "Running reward of episode 1500/2000: 88.3216693759167\n",
      "Average Total reward: 482.7\n",
      "Running reward of episode 1600/2000: 97.39291710428218\n",
      "Average Total reward: 495.8\n",
      "Running reward of episode 1700/2000: 98.29145264909977\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.29507603656072\n",
      "Average Total reward: 441.65\n",
      "Running reward of episode 1900/2000: 98.34920308230684\n",
      "Average Total reward: 493.45\n",
      "Running reward of episode 2000/2000: 66.97932579294422\n",
      "Average Total reward: 283.15\n",
      "Average length of all episodes: 299.879\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4ngffdb4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▃▆▅▃▄▆███▆▅███▇█▅</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▂▃▆▅▃▄▆███▆▅███▇█▅</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▂▂▂▂▃▄▂▄▂▆▄▄▆▃▅██████▇█▂▅▇▇███████▃</td></tr><tr><td>loss_policy</td><td>▁▁▂▄▃▄▅▆▇▇▇▅▆▅▆█▇▆█▆▄▄▄▄▄▄▃▄▅▆▄▄▄▄▄▄▄▄▄▆</td></tr><tr><td>running_reward</td><td>▁▂▂▃▂▃▄▅▆▆▇▅▇▅▇▆▆▇▇▆██████▇█▅▇█████████▆</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>283.15</td></tr><tr><td>average_lenght_all_episodes</td><td>299.879</td></tr><tr><td>average_total_reward</td><td>283.15</td></tr><tr><td>episode_length</td><td>124.8</td></tr><tr><td>loss_policy</td><td>14.09194</td></tr><tr><td>running_reward</td><td>66.97933</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Without the baseline </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/4ngffdb4' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/4ngffdb4</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_194729-4ngffdb4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4ngffdb4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_195452-0xdd0r8b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/0xdd0r8b' target=\"_blank\">Without the baseline </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/0xdd0r8b' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/0xdd0r8b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with no baseline.\n",
      "Running reward of episode 100/2000: 19.94712459018216\n",
      "Average Total reward: 20.65\n",
      "Running reward of episode 200/2000: 19.52804798016683\n",
      "Average Total reward: 25.1\n",
      "Running reward of episode 300/2000: 18.746739144450807\n",
      "Average Total reward: 20.0\n",
      "Running reward of episode 400/2000: 51.74657560544224\n",
      "Average Total reward: 97.65\n",
      "Running reward of episode 500/2000: 74.99367015507983\n",
      "Average Total reward: 182.3\n",
      "Running reward of episode 600/2000: 79.68546876043298\n",
      "Average Total reward: 161.95\n",
      "Running reward of episode 700/2000: 85.04253764657325\n",
      "Average Total reward: 184.2\n",
      "Running reward of episode 800/2000: 94.52771599271692\n",
      "Average Total reward: 330.3\n",
      "Running reward of episode 900/2000: 96.24765622810492\n",
      "Average Total reward: 494.75\n",
      "Running reward of episode 1000/2000: 91.77900350005297\n",
      "Average Total reward: 252.25\n",
      "Running reward of episode 1100/2000: 83.04294433415521\n",
      "Average Total reward: 231.3\n",
      "Running reward of episode 1200/2000: 96.1195383919827\n",
      "Average Total reward: 450.1\n",
      "Running reward of episode 1300/2000: 69.45700395012219\n",
      "Average Total reward: 63.45\n",
      "Running reward of episode 1400/2000: 53.14884534951228\n",
      "Average Total reward: 164.9\n",
      "Running reward of episode 1500/2000: 17.896237316846186\n",
      "Average Total reward: 28.0\n",
      "Running reward of episode 1600/2000: 77.13120586470279\n",
      "Average Total reward: 115.85\n",
      "Running reward of episode 1700/2000: 86.0226805232538\n",
      "Average Total reward: 486.65\n",
      "Running reward of episode 1800/2000: 98.11724358984925\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 97.46603453713254\n",
      "Average Total reward: 453.1\n",
      "Running reward of episode 2000/2000: 58.004658712326204\n",
      "Average Total reward: 197.85\n",
      "Average length of all episodes: 250.1985\n",
      "Testing the best policy\n",
      "Average Total reward: 358.38\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:0xdd0r8b) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▁▂▃▃▃▆█▄▄▇▂▃▁▂██▇▄</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▁▂▃▃▃▆█▄▄▇▂▃▁▂██▇▄</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▁▁▂▂▄▃▄▂▄▄▆▅███▄▃▆▇▇▅▂▂▁▁▇▅▅▅██▇█▇▃</td></tr><tr><td>loss_policy</td><td>▃▃▃▂▂▂▅▆▆██▇▆██▇▇▅▅▅█▇▆▅▆▆▄▅▂▁▅▇▆▄▅▅▅▅▅▃</td></tr><tr><td>running_reward</td><td>▁▂▂▂▂▁▂▄▅▆▆▆▆▇▇▇████▇▇▇███▅▄▃▂▆▇▆▆█████▅</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>197.85</td></tr><tr><td>average_lenght_all_episodes</td><td>250.1985</td></tr><tr><td>average_total_reward</td><td>197.85</td></tr><tr><td>episode_length</td><td>131.2</td></tr><tr><td>loss_policy</td><td>7.18892</td></tr><tr><td>running_reward</td><td>58.00466</td></tr><tr><td>test_average_episode_length</td><td>358.38</td></tr><tr><td>test_average_total_reward</td><td>358.38</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Without the baseline </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/0xdd0r8b' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/0xdd0r8b</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_195452-0xdd0r8b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:0xdd0r8b). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_200009-5iaavndr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/5iaavndr' target=\"_blank\">Without the baseline </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/5iaavndr' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/5iaavndr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with no baseline.\n",
      "Running reward of episode 100/2000: 22.890720422596\n",
      "Average Total reward: 33.55\n",
      "Running reward of episode 200/2000: 35.291455188592074\n",
      "Average Total reward: 44.65\n",
      "Running reward of episode 300/2000: 53.717657421400745\n",
      "Average Total reward: 86.35\n",
      "Running reward of episode 400/2000: 61.880180400354455\n",
      "Average Total reward: 132.35\n",
      "Running reward of episode 500/2000: 35.89277275919185\n",
      "Average Total reward: 53.5\n",
      "Running reward of episode 600/2000: 50.69716957968764\n",
      "Average Total reward: 60.85\n",
      "Running reward of episode 700/2000: 48.53642207868032\n",
      "Average Total reward: 73.75\n",
      "Running reward of episode 800/2000: 80.04372163275795\n",
      "Average Total reward: 213.85\n",
      "Running reward of episode 900/2000: 98.15710645086948\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 98.34838622934323\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.34951870686092\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.3188037686289\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.19454959831131\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.34860791259177\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.34952001934309\n",
      "Average Total reward: 482.15\n",
      "Running reward of episode 1600/2000: 97.96093786732729\n",
      "Average Total reward: 490.95\n",
      "Running reward of episode 1700/2000: 98.34722480751225\n",
      "Average Total reward: 475.25\n",
      "Running reward of episode 1800/2000: 97.76764777797591\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34608042789024\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.1775506011132\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 329.1145\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:5iaavndr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▂▁▁▂▄████████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▂▂▁▁▂▄████████████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▂▂▂▂▂▄▃▂▂▂▃▅██████████████████▇████</td></tr><tr><td>loss_policy</td><td>▁▁▂▄▄▆▆▆▆▄▆▅▄▅▅█▇▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▄▃▄▄</td></tr><tr><td>running_reward</td><td>▁▂▂▂▃▄▄▅▅▄▄▆▄▆▄▅▇███████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>329.1145</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>487.5</td></tr><tr><td>loss_policy</td><td>10.36318</td></tr><tr><td>running_reward</td><td>98.17755</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Without the baseline </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/5iaavndr' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/5iaavndr</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_200009-5iaavndr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:5iaavndr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_200742-e3rnv7x0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/e3rnv7x0' target=\"_blank\">Without the baseline </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/e3rnv7x0' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/e3rnv7x0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with no baseline.\n",
      "Running reward of episode 100/2000: 21.736855232360256\n",
      "Average Total reward: 30.6\n",
      "Running reward of episode 200/2000: 27.39902428751464\n",
      "Average Total reward: 36.75\n",
      "Running reward of episode 300/2000: 41.56982564925054\n",
      "Average Total reward: 76.8\n",
      "Running reward of episode 400/2000: 65.0520883257497\n",
      "Average Total reward: 133.85\n",
      "Running reward of episode 500/2000: 51.80863923915272\n",
      "Average Total reward: 79.9\n",
      "Running reward of episode 600/2000: 95.16295773472105\n",
      "Average Total reward: 478.1\n",
      "Running reward of episode 700/2000: 88.90569476059035\n",
      "Average Total reward: 225.45\n",
      "Running reward of episode 800/2000: 77.05751689646934\n",
      "Average Total reward: 259.7\n",
      "Running reward of episode 900/2000: 53.12773204415513\n",
      "Average Total reward: 68.2\n",
      "Running reward of episode 1000/2000: 31.86528201448646\n",
      "Average Total reward: 94.45\n",
      "Running reward of episode 1100/2000: 68.95688293451661\n",
      "Average Total reward: 102.45\n",
      "Running reward of episode 1200/2000: 77.46725857204423\n",
      "Average Total reward: 161.75\n",
      "Running reward of episode 1300/2000: 79.72680182288524\n",
      "Average Total reward: 161.45\n",
      "Running reward of episode 1400/2000: 84.09958072379192\n",
      "Average Total reward: 151.1\n",
      "Running reward of episode 1500/2000: 68.55403268157372\n",
      "Average Total reward: 122.8\n",
      "Running reward of episode 1600/2000: 71.71346301647915\n",
      "Average Total reward: 127.5\n",
      "Running reward of episode 1700/2000: 55.21488543783315\n",
      "Average Total reward: 76.25\n",
      "Running reward of episode 1800/2000: 56.607369740237495\n",
      "Average Total reward: 73.7\n",
      "Running reward of episode 1900/2000: 64.19322126991837\n",
      "Average Total reward: 105.9\n",
      "Running reward of episode 2000/2000: 66.05285039403569\n",
      "Average Total reward: 104.6\n",
      "Average length of all episodes: 134.5575\n",
      "Testing the best policy\n",
      "Average Total reward: 110.66\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:e3rnv7x0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▃▂█▄▅▂▂▂▃▃▃▂▃▂▂▂▂</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▂▃▂█▄▅▂▂▂▃▃▃▂▃▂▂▂▂</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▂▁▂▂▄▃▆▇█▃▃▇▂▂▁▃▃▃▃▃▃▄▄▃▂▃▃▃▂▂▂▂▂▂▂</td></tr><tr><td>loss_policy</td><td>▁▃▃▃▂▅▄▇▇██▆▆▅█▇▅▄▄▁▆▇▇▆▇▇▆▇▇▅▇▆▅▄▃▃▄▄▄▄</td></tr><tr><td>running_reward</td><td>▁▂▂▂▃▃▄▄▅▆▅▇██▇▆▇▅▄▂▅▆▆▆▆▇▇█▇▆▆▆▆▅▄▅▅▅▅▆</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>104.6</td></tr><tr><td>average_lenght_all_episodes</td><td>134.5575</td></tr><tr><td>average_total_reward</td><td>104.6</td></tr><tr><td>episode_length</td><td>109.9</td></tr><tr><td>loss_policy</td><td>10.44287</td></tr><tr><td>running_reward</td><td>66.05285</td></tr><tr><td>test_average_episode_length</td><td>110.66</td></tr><tr><td>test_average_total_reward</td><td>110.66</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Without the baseline </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/e3rnv7x0' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/e3rnv7x0</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_200742-e3rnv7x0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:e3rnv7x0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_201036-k2vn31cc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/k2vn31cc' target=\"_blank\">Without the baseline </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/k2vn31cc' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/k2vn31cc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with no baseline.\n",
      "Running reward of episode 100/2000: 21.247652941300803\n",
      "Average Total reward: 27.85\n",
      "Running reward of episode 200/2000: 38.12038895776915\n",
      "Average Total reward: 44.25\n",
      "Running reward of episode 300/2000: 50.365668498263894\n",
      "Average Total reward: 91.3\n",
      "Running reward of episode 400/2000: 83.9404025617387\n",
      "Average Total reward: 246.55\n",
      "Running reward of episode 500/2000: 77.85885357293179\n",
      "Average Total reward: 268.5\n",
      "Running reward of episode 600/2000: 76.79211822645209\n",
      "Average Total reward: 198.6\n",
      "Running reward of episode 700/2000: 80.21160043222355\n",
      "Average Total reward: 134.95\n",
      "Running reward of episode 800/2000: 57.933488226697996\n",
      "Average Total reward: 114.0\n",
      "Running reward of episode 900/2000: 97.16289646088337\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 59.330597550833154\n",
      "Average Total reward: 94.25\n",
      "Running reward of episode 1100/2000: 14.857272275588404\n",
      "Average Total reward: 26.6\n",
      "Running reward of episode 1200/2000: 32.35128633998752\n",
      "Average Total reward: 63.6\n",
      "Running reward of episode 1300/2000: 96.58073869466823\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.33905329798064\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.34946345096809\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34952508458305\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.34952544948669\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.3495254516471\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 278.86\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Without the baseline \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": None,\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df",
   "metadata": {
    "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df"
   },
   "source": [
    "**The Real Exercise**: Standard practice is to use the state-value function $v(s)$ as a baseline. This is intuitively appealing -- we are more interested in updating out policy for returns that estimate the current **value** worse. Our new update becomes:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - \\tilde{v}(S_t \\mid \\mathbf{w})) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
    "\n",
    "where $\\tilde{v}(s \\mid \\mathbf{w})$ is a **deep neural network** with parameters $w$ that estimates $v_\\pi(s)$. What neural network? Typically, we use the **same** network architecture as that of the Policy.\n",
    "\n",
    "**Your Task**: Modify your implementation to fit a second, baseline network to estimate the value function and use it as **baseline**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ad67f-180b-4009-a6e7-752fe1e4bc58",
   "metadata": {},
   "source": [
    "## With Baseline network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6187b4f3-568a-49a5-93f6-cc7cccb8cc2f",
   "metadata": {
    "id": "6187b4f3-568a-49a5-93f6-cc7cccb8cc2f"
   },
   "outputs": [],
   "source": [
    "# A simple Net with the same architecture as the Policy Net that has to learn to estimate the value function\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, env, hidden_layers=32):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], hidden_layers)\n",
    "        self.fc2 = nn.Linear(hidden_layers, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = self.fc2(s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f",
   "metadata": {
    "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:k2vn31cc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▄▅▄▃▂█▂▁▂████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▂▄▅▄▃▂█▂▁▂████████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▂▂▃▃▃▂▂▂▅▃▃▂▃██▇▁▁▁▂▃███████████████</td></tr><tr><td>loss_policy</td><td>▃▃▃▅▅▆██▆▆▆▅▆▇▇▅▇▅▅▅▁▁▃▄▆▅▅▅▅▅▄▄▅▄▄▄▄▄▄▄</td></tr><tr><td>running_reward</td><td>▁▂▂▂▃▄▅▆▆▄▆▄▇▇▆▄▆██▇▂▁▂▃▅███████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>278.86</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>9.52762</td></tr><tr><td>running_reward</td><td>98.34953</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Without the baseline </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/k2vn31cc' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/k2vn31cc</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_201036-k2vn31cc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:k2vn31cc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_201758-ujn7oaj0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ujn7oaj0' target=\"_blank\">Baseline Network </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ujn7oaj0' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ujn7oaj0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 18.385507946235805\n",
      "Average Total reward: 24.8\n",
      "Running reward of episode 200/2000: 27.337891315924985\n",
      "Average Total reward: 40.3\n",
      "Running reward of episode 300/2000: 48.94506384717461\n",
      "Average Total reward: 94.2\n",
      "Running reward of episode 400/2000: 87.7221432319426\n",
      "Average Total reward: 358.5\n",
      "Running reward of episode 500/2000: 96.1324490482994\n",
      "Average Total reward: 473.25\n",
      "Running reward of episode 600/2000: 98.33093439318282\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 700/2000: 98.25359321689321\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 800/2000: 94.15650484549951\n",
      "Average Total reward: 225.55\n",
      "Running reward of episode 900/2000: 97.45551554852823\n",
      "Average Total reward: 472.75\n",
      "Running reward of episode 1000/2000: 97.06465916173579\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.07286247669177\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.34682825063238\n",
      "Average Total reward: 478.55\n",
      "Running reward of episode 1300/2000: 98.34950948280247\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.34952535711585\n",
      "Average Total reward: 481.25\n",
      "Running reward of episode 1500/2000: 98.3495254511002\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34952545165665\n",
      "Average Total reward: 476.55\n",
      "Running reward of episode 1700/2000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.34887133491937\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34952157894269\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34952542873143\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 398.129\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ujn7oaj0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▆███▄████████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▂▆███▄████████████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▁▂▃▇▃█████▇▄██▇████████████████████</td></tr><tr><td>loss_policy</td><td>▅▇▆▅▇▇▇█▆█▃▂▂▂▂▂▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_value</td><td>▁▃▂▁▃▃▅▇▇█▆▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>running_reward</td><td>▁▂▂▂▃▃▄▆▇▇██████▇███████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>398.129</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.11073</td></tr><tr><td>loss_value</td><td>598.27344</td></tr><tr><td>running_reward</td><td>98.34953</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ujn7oaj0' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ujn7oaj0</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_201758-ujn7oaj0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ujn7oaj0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_202939-g55ig2la</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/g55ig2la' target=\"_blank\">Baseline Network </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/g55ig2la' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/g55ig2la</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 16.83578667352332\n",
      "Average Total reward: 17.7\n",
      "Running reward of episode 200/2000: 22.965908322185395\n",
      "Average Total reward: 28.9\n",
      "Running reward of episode 300/2000: 55.90019131914127\n",
      "Average Total reward: 128.55\n",
      "Running reward of episode 400/2000: 87.75277591504432\n",
      "Average Total reward: 234.25\n",
      "Running reward of episode 500/2000: 92.53298793620372\n",
      "Average Total reward: 239.55\n",
      "Running reward of episode 600/2000: 96.16116664149013\n",
      "Average Total reward: 488.55\n",
      "Running reward of episode 700/2000: 96.46826864501728\n",
      "Average Total reward: 490.9\n",
      "Running reward of episode 800/2000: 70.62004861917573\n",
      "Average Total reward: 103.15\n",
      "Running reward of episode 900/2000: 78.2246071553638\n",
      "Average Total reward: 158.25\n",
      "Running reward of episode 1000/2000: 95.39791992764842\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.33205038490816\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.34942199001661\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.34952483911228\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.0243949462877\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 97.14402008911124\n",
      "Average Total reward: 308.35\n",
      "Running reward of episode 1600/2000: 95.46214532009387\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 97.6092015460747\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.34514234234442\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34445001519887\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34949540239008\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 350.9945\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:g55ig2la) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▃▄▄██▂▃█████▅█████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▃▄▄██▂▃█████▅█████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▂▃▅▄▅▆█▇▇▇▆▂▃▃▇██████████▅▄▄███████</td></tr><tr><td>loss_policy</td><td>▄▅▅▅▇▇█▇▇▆▄▃▂▂▂▃▃▄▃▁▂▂▂▂▂▂▂▂▂▂▂▃▃▁▂▂▂▂▂▂</td></tr><tr><td>loss_value</td><td>▁▁▁▁▃▄███▇▆▅▅▅▅▅▃▅▅▅▅▅▅▅▅▅▅▅▅▅▆▅▆▅▅▅▅▅▅▅</td></tr><tr><td>running_reward</td><td>▁▂▂▂▂▄▆▇▇▇▇█████▆▆▇███████████▇█████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>350.9945</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.07509</td></tr><tr><td>loss_value</td><td>598.49774</td></tr><tr><td>running_reward</td><td>98.3495</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/g55ig2la' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/g55ig2la</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_202939-g55ig2la/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:g55ig2la). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_203947-pie194yo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/pie194yo' target=\"_blank\">Baseline Network </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/pie194yo' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/pie194yo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 22.281848419537802\n",
      "Average Total reward: 28.0\n",
      "Running reward of episode 200/2000: 53.62815686467122\n",
      "Average Total reward: 115.1\n",
      "Running reward of episode 300/2000: 83.76246841043545\n",
      "Average Total reward: 268.9\n",
      "Running reward of episode 400/2000: 63.74858045082211\n",
      "Average Total reward: 102.6\n",
      "Running reward of episode 500/2000: 87.97375980632762\n",
      "Average Total reward: 336.95\n",
      "Running reward of episode 600/2000: 97.6647314586418\n",
      "Average Total reward: 481.65\n",
      "Running reward of episode 700/2000: 97.65936320372809\n",
      "Average Total reward: 493.55\n",
      "Running reward of episode 800/2000: 98.28467008855114\n",
      "Average Total reward: 495.15\n",
      "Running reward of episode 900/2000: 97.7117098206728\n",
      "Average Total reward: 488.8\n",
      "Running reward of episode 1000/2000: 98.01391223908752\n",
      "Average Total reward: 488.7\n",
      "Running reward of episode 1100/2000: 86.66527912355751\n",
      "Average Total reward: 269.25\n",
      "Running reward of episode 1200/2000: 98.24028467323993\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.34887868843924\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.34952162247941\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 95.03711018618743\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.15382586586624\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.34836680654386\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 96.24567875509851\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 97.22901073650642\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34289141154706\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 400.676\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pie194yo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▂▅▂▆█████▅█████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▂▅▂▆█████▅█████████</td></tr><tr><td>episode_length</td><td>▁▁▁▂▃▄▄▅▄█▆▇█████▇██▇███████▇███████▅▇██</td></tr><tr><td>loss_policy</td><td>▄▄▅▇██▇▆▅▃▃▂▄▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▁▂▂</td></tr><tr><td>loss_value</td><td>▁▁▂▄▇█▇▇▆▅▅▅▅▅▅▅▅▅▅▅▄▅▅▅▅▅▅▅▄▅▅▅▅▅▅▅▅▄▅▅</td></tr><tr><td>running_reward</td><td>▁▂▂▃▄▆▇▇▆█▇███████████▇█████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>400.676</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.15962</td></tr><tr><td>loss_value</td><td>598.25964</td></tr><tr><td>running_reward</td><td>98.34289</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/pie194yo' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/pie194yo</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_203947-pie194yo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pie194yo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_205022-7q6dvowr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/7q6dvowr' target=\"_blank\">Baseline Network </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/7q6dvowr' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/7q6dvowr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 17.44650506993677\n",
      "Average Total reward: 21.3\n",
      "Running reward of episode 200/2000: 25.488443079499636\n",
      "Average Total reward: 32.15\n",
      "Running reward of episode 300/2000: 41.63601094207207\n",
      "Average Total reward: 88.9\n",
      "Running reward of episode 400/2000: 87.6099692984882\n",
      "Average Total reward: 226.35\n",
      "Running reward of episode 500/2000: 85.55186051180435\n",
      "Average Total reward: 127.85\n",
      "Running reward of episode 600/2000: 95.44625588271197\n",
      "Average Total reward: 370.9\n",
      "Running reward of episode 700/2000: 98.15177528843111\n",
      "Average Total reward: 487.85\n",
      "Running reward of episode 800/2000: 98.3110262229343\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 98.27358853264109\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 98.25829019228898\n",
      "Average Total reward: 497.4\n",
      "Running reward of episode 1100/2000: 98.27441217162068\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 97.18155263092343\n",
      "Average Total reward: 480.4\n",
      "Running reward of episode 1300/2000: 98.34261043444623\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.33016125519278\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.33164992619021\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 97.52887172257327\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.34466674727713\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.34949668555869\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 94.72731729514955\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.32808006242718\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 399.6445\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7q6dvowr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▄▃▆██████████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▂▄▃▆██████████████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▂▃▆▆█▃▇▆██████████▆████████████████</td></tr><tr><td>loss_policy</td><td>▅▅▄▅▅██▆▅▄▅▃▃▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_value</td><td>▁▂▁▃▂▆█▇▇▆▆▆▆▅▅▅▅▅▅▅▅▅▅▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>running_reward</td><td>▁▂▁▂▂▃▅▇▇█▆█████████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>399.6445</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.22547</td></tr><tr><td>loss_value</td><td>598.12701</td></tr><tr><td>running_reward</td><td>98.32808</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/7q6dvowr' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/7q6dvowr</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_205022-7q6dvowr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7q6dvowr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_210206-8hwbgh4e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8hwbgh4e' target=\"_blank\">Baseline Network </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8hwbgh4e' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8hwbgh4e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 21.38267050170289\n",
      "Average Total reward: 28.0\n",
      "Running reward of episode 200/2000: 23.217362229512833\n",
      "Average Total reward: 35.8\n",
      "Running reward of episode 300/2000: 36.94217072842432\n",
      "Average Total reward: 39.45\n",
      "Running reward of episode 400/2000: 62.832406715066156\n",
      "Average Total reward: 110.65\n",
      "Running reward of episode 500/2000: 93.43508517402675\n",
      "Average Total reward: 285.9\n",
      "Running reward of episode 600/2000: 95.65822266827351\n",
      "Average Total reward: 418.4\n",
      "Running reward of episode 700/2000: 95.69248309437596\n",
      "Average Total reward: 487.45\n",
      "Running reward of episode 800/2000: 98.08902420205092\n",
      "Average Total reward: 456.25\n",
      "Running reward of episode 900/2000: 97.94376891505759\n",
      "Average Total reward: 412.05\n",
      "Running reward of episode 1000/2000: 93.72715203393149\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.32215855477298\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.32225553746267\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 97.20177257026548\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 93.09085280096075\n",
      "Average Total reward: 359.45\n",
      "Running reward of episode 1500/2000: 95.54722565756727\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.3329343538449\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.34942722358053\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.34952487009775\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.19329980734726\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.3432102314906\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 362.4385\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Baseline Network \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612164bd-ad19-4b7c-815f-79e397ac08d7",
   "metadata": {},
   "source": [
    "## With Baseline Network and higher temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "943c27ab-9561-4edf-91a4-6d4e67d2b1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8hwbgh4e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▁▂▅▇█▇▇████▆██████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▁▂▅▇█▇▇████▆██████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▁▁▂▃▄▆▆▇▆████▃▆██████▆▇▇███████████</td></tr><tr><td>loss_policy</td><td>▇▆▅▅▇█▆▇█▇▅▄▂▄▃▃▂▂▅▂▁▂▂▂▂▂▃▂▁▁▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>loss_value</td><td>▂▁▁▁▃▄▂▄██▆▆▅▆▅▅▅▅▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>running_reward</td><td>▁▂▂▂▂▃▃▄▅▇▇▇█▇████▇▇████████▇███████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>362.4385</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>498.4</td></tr><tr><td>loss_policy</td><td>-0.07096</td></tr><tr><td>loss_value</td><td>599.25696</td></tr><tr><td>running_reward</td><td>98.34321</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8hwbgh4e' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8hwbgh4e</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_210206-8hwbgh4e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8hwbgh4e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_211150-1eypie3l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/1eypie3l' target=\"_blank\">Baseline Network and temp 10</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/1eypie3l' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/1eypie3l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 19.183506459172378\n",
      "Average Total reward: 20.75\n",
      "Running reward of episode 200/2000: 30.100428325060776\n",
      "Average Total reward: 42.7\n",
      "Running reward of episode 300/2000: 53.24085431990675\n",
      "Average Total reward: 156.2\n",
      "Running reward of episode 400/2000: 87.17611378731436\n",
      "Average Total reward: 274.25\n",
      "Running reward of episode 500/2000: 93.78258029846293\n",
      "Average Total reward: 429.45\n",
      "Running reward of episode 600/2000: 78.16152860895221\n",
      "Average Total reward: 256.0\n",
      "Running reward of episode 700/2000: 97.28110043913716\n",
      "Average Total reward: 478.35\n",
      "Running reward of episode 800/2000: 97.17399631955786\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 92.42908476865685\n",
      "Average Total reward: 383.45\n",
      "Running reward of episode 1000/2000: 97.25006782471317\n",
      "Average Total reward: 480.45\n",
      "Running reward of episode 1100/2000: 97.33184293619298\n",
      "Average Total reward: 462.95\n",
      "Running reward of episode 1200/2000: 97.81609175989298\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 97.76243397069076\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.33841622464995\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.08991800880762\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34303871630684\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.34948704675375\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 97.86846648314557\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34663347711195\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.3495083296401\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 383.9965\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1eypie3l) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▃▅▇▄██▆█▇█████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▃▅▇▄██▆█▇█████████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▁▄▄▄▄▇▅█▇█▇█▆▃▇▇▇███▇██████████████</td></tr><tr><td>loss_policy</td><td>▅▅▆▅▆▆██▇▆▃▄▁▁▁▂▁▂▃▁▁▂▁▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_value</td><td>▁▁▂▂▃▃██▆▇▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>running_reward</td><td>▁▂▂▂▃▃▆▇▇▇██▇█████▇█████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>383.9965</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.13483</td></tr><tr><td>loss_value</td><td>598.22485</td></tr><tr><td>running_reward</td><td>98.34951</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network and temp 10</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/1eypie3l' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/1eypie3l</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_211150-1eypie3l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1eypie3l). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_212241-i7tef33h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/i7tef33h' target=\"_blank\">Baseline Network and temp 10</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/i7tef33h' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/i7tef33h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 16.115211315668343\n",
      "Average Total reward: 26.8\n",
      "Running reward of episode 200/2000: 21.81889781319484\n",
      "Average Total reward: 29.4\n",
      "Running reward of episode 300/2000: 25.638065698205068\n",
      "Average Total reward: 32.6\n",
      "Running reward of episode 400/2000: 51.82225212538218\n",
      "Average Total reward: 149.25\n",
      "Running reward of episode 500/2000: 79.54618308575843\n",
      "Average Total reward: 218.7\n",
      "Running reward of episode 600/2000: 94.63875154463302\n",
      "Average Total reward: 481.9\n",
      "Running reward of episode 700/2000: 95.08126311887636\n",
      "Average Total reward: 484.35\n",
      "Running reward of episode 800/2000: 97.50197131607042\n",
      "Average Total reward: 467.0\n",
      "Running reward of episode 900/2000: 97.40831860307311\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 98.34395300901052\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 94.89274654804902\n",
      "Average Total reward: 269.05\n",
      "Running reward of episode 1200/2000: 98.22545901313909\n",
      "Average Total reward: 466.25\n",
      "Running reward of episode 1300/2000: 96.02346280902047\n",
      "Average Total reward: 492.2\n",
      "Running reward of episode 1400/2000: 96.13537297516179\n",
      "Average Total reward: 460.35\n",
      "Running reward of episode 1500/2000: 98.2779391235332\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.3491016227125\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.34952294236828\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.34952543680359\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.349525451572\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.12195673564335\n",
      "Average Total reward: 491.35\n",
      "Average length of all episodes: 371.8035\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:i7tef33h) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▁▃▄█████▅██▇██████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▁▃▄█████▅██▇██████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▁▁▂▃▃▃▇▇▅▇█▇████▇███▇█▇████████████</td></tr><tr><td>loss_policy</td><td>▅▅▅▅▆▅▆▇██▆▃▂▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>loss_value</td><td>▁▁▁▁▂▁▂▄▇█▇▆▆▇▆▆▆▆▆▆▆▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>running_reward</td><td>▁▂▂▂▂▂▃▄▅▆▇▇█▇██████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>491.35</td></tr><tr><td>average_lenght_all_episodes</td><td>371.8035</td></tr><tr><td>average_total_reward</td><td>491.35</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.1577</td></tr><tr><td>loss_value</td><td>598.24377</td></tr><tr><td>running_reward</td><td>98.12196</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network and temp 10</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/i7tef33h' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/i7tef33h</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_212241-i7tef33h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:i7tef33h). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_213305-7b7onsrn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/7b7onsrn' target=\"_blank\">Baseline Network and temp 10</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/7b7onsrn' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/7b7onsrn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 18.866451379219736\n",
      "Average Total reward: 20.65\n",
      "Running reward of episode 200/2000: 26.69573728326189\n",
      "Average Total reward: 39.75\n",
      "Running reward of episode 300/2000: 54.860592637949644\n",
      "Average Total reward: 77.9\n",
      "Running reward of episode 400/2000: 85.2991098779191\n",
      "Average Total reward: 321.1\n",
      "Running reward of episode 500/2000: 95.85501774082272\n",
      "Average Total reward: 440.05\n",
      "Running reward of episode 600/2000: 97.4902415571414\n",
      "Average Total reward: 492.05\n",
      "Running reward of episode 700/2000: 96.89804264012697\n",
      "Average Total reward: 483.2\n",
      "Running reward of episode 800/2000: 94.64327280983144\n",
      "Average Total reward: 319.6\n",
      "Running reward of episode 900/2000: 96.95755066634251\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 98.16432356286433\n",
      "Average Total reward: 465.1\n",
      "Running reward of episode 1100/2000: 97.78299076953908\n",
      "Average Total reward: 493.3\n",
      "Running reward of episode 1200/2000: 97.62746771256893\n",
      "Average Total reward: 483.65\n",
      "Running reward of episode 1300/2000: 97.7260381266303\n",
      "Average Total reward: 491.5\n",
      "Running reward of episode 1400/2000: 98.07494328500465\n",
      "Average Total reward: 497.4\n",
      "Running reward of episode 1500/2000: 98.14962926721932\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34207038541128\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.3494813137224\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.33414104461798\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 97.11090883791464\n",
      "Average Total reward: 391.15\n",
      "Running reward of episode 2000/2000: 97.47807785158317\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 387.4625\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7b7onsrn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▅▇██▅█▇████████▆█</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▂▅▇██▅█▇████████▆█</td></tr><tr><td>episode_length</td><td>▁▁▁▁▂▂▃▄▃▅▇██▇█▇▇███▇▇▇█▇█████████████▄█</td></tr><tr><td>loss_policy</td><td>▅▄▅▄▇▇█▇▇▅▃▃▃▂▂▂▂▂▁▁▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>loss_value</td><td>▁▁▂▁▄▅█▇▇▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▅</td></tr><tr><td>running_reward</td><td>▁▂▂▂▃▄▅▆▇▇████████████████████████████▇█</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>387.4625</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>487.6</td></tr><tr><td>loss_policy</td><td>-0.69755</td></tr><tr><td>loss_value</td><td>606.10754</td></tr><tr><td>running_reward</td><td>97.47808</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network and temp 10</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/7b7onsrn' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/7b7onsrn</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_213305-7b7onsrn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7b7onsrn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_214317-lr30vn0y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/lr30vn0y' target=\"_blank\">Baseline Network and temp 10</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/lr30vn0y' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/lr30vn0y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 22.137660930667803\n",
      "Average Total reward: 24.45\n",
      "Running reward of episode 200/2000: 34.35531586944902\n",
      "Average Total reward: 39.15\n",
      "Running reward of episode 300/2000: 69.72722328299251\n",
      "Average Total reward: 179.85\n",
      "Running reward of episode 400/2000: 83.35566354013048\n",
      "Average Total reward: 139.45\n",
      "Running reward of episode 500/2000: 92.64241645580998\n",
      "Average Total reward: 461.15\n",
      "Running reward of episode 600/2000: 92.39572814183286\n",
      "Average Total reward: 297.15\n",
      "Running reward of episode 700/2000: 94.4515992615833\n",
      "Average Total reward: 463.7\n",
      "Running reward of episode 800/2000: 95.71002139925483\n",
      "Average Total reward: 493.6\n",
      "Running reward of episode 900/2000: 81.9532053384586\n",
      "Average Total reward: 352.65\n",
      "Running reward of episode 1000/2000: 96.15248828408872\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 95.81721568027436\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.29921840933484\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.20151905854863\n",
      "Average Total reward: 475.85\n",
      "Running reward of episode 1400/2000: 97.23883791773981\n",
      "Average Total reward: 444.7\n",
      "Running reward of episode 1500/2000: 74.75987026762236\n",
      "Average Total reward: 173.9\n",
      "Running reward of episode 1600/2000: 94.20278582481616\n",
      "Average Total reward: 480.95\n",
      "Running reward of episode 1700/2000: 85.0821380461576\n",
      "Average Total reward: 240.7\n",
      "Running reward of episode 1800/2000: 98.24317288194722\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 96.67521915369382\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 95.3384700037684\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 336.287\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lr30vn0y) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▃▃▇▅▇█▆████▇▃█▄███</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▃▃▇▅▇█▆████▇▃█▄███</td></tr><tr><td>episode_length</td><td>▁▁▁▂▁▂▃▄▂▅▇▇▅▇▆▆▇▅▇█▇▇██████▂▂██▄▄██▅▇██</td></tr><tr><td>loss_policy</td><td>▅▆▆█▇██▇▆▅▄▄▅▃▃▂▂▃▂▂▂▁▂▂▂▂▂▂▃▂▁▁▃▃▁▂▃▁▂▂</td></tr><tr><td>loss_value</td><td>▁▁▂▄▃▆██▅▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▃▃▅▅▅▅▅▅▆▅▅▅</td></tr><tr><td>running_reward</td><td>▁▂▂▃▃▄▆▇▆▇██▇██▇█▇██████████▆▆▇██▇██████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>336.287</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.25145</td></tr><tr><td>loss_value</td><td>599.67987</td></tr><tr><td>running_reward</td><td>95.33847</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network and temp 10</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/lr30vn0y' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/lr30vn0y</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_214317-lr30vn0y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lr30vn0y). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_215233-qqa67eeh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/qqa67eeh' target=\"_blank\">Baseline Network and temp 10</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/qqa67eeh' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/qqa67eeh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 19.84480796851617\n",
      "Average Total reward: 22.2\n",
      "Running reward of episode 200/2000: 22.66890234889799\n",
      "Average Total reward: 28.5\n",
      "Running reward of episode 300/2000: 37.388597683732826\n",
      "Average Total reward: 52.85\n",
      "Running reward of episode 400/2000: 54.868267098938446\n",
      "Average Total reward: 103.75\n",
      "Running reward of episode 500/2000: 82.56332495435332\n",
      "Average Total reward: 199.25\n",
      "Running reward of episode 600/2000: 86.82295108422903\n",
      "Average Total reward: 416.75\n",
      "Running reward of episode 700/2000: 96.6440508326321\n",
      "Average Total reward: 395.55\n",
      "Running reward of episode 800/2000: 96.7059063122603\n",
      "Average Total reward: 385.7\n",
      "Running reward of episode 900/2000: 91.15909213629051\n",
      "Average Total reward: 163.45\n",
      "Running reward of episode 1000/2000: 90.42343576993066\n",
      "Average Total reward: 246.75\n",
      "Running reward of episode 1100/2000: 92.73186541422275\n",
      "Average Total reward: 284.3\n",
      "Running reward of episode 1200/2000: 97.05024269371694\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.09982921018158\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.14885237719015\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.34778773552974\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 97.07490726955018\n",
      "Average Total reward: 440.3\n",
      "Running reward of episode 1700/2000: 96.80268863762828\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 96.85210126028464\n",
      "Average Total reward: 273.3\n",
      "Running reward of episode 1900/2000: 96.69609266826575\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.339736254552\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 330.12\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Baseline Network and temp 10\",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 10,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c732c58-824d-447a-a520-9880d936704b",
   "metadata": {},
   "source": [
    "## With Baseline Network, temperature 10 and 128 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a1a4dad-f5b9-4a32-99bb-18ad45a2d20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:qqa67eeh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▁▂▄▇▆▆▃▄▅████▇█▅██</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▁▂▄▇▆▆▃▄▅████▇█▅██</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▂▂▂▂▃▅▅▇█▆█▇▇▃▅▃▅▅███▇███▇▇█▇███▇██</td></tr><tr><td>loss_policy</td><td>▅▆▅▅▆▇▇▇█▇▅▅▃▃▃▃▃▂▄▄▄▃▂▁▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>loss_value</td><td>▁▂▁▁▂▄▅▄██▇▇▆▆▇▆▆▆▆▇▅▆▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>running_reward</td><td>▁▂▂▂▂▃▃▄▅▆▇▇▇█████▇▇▇███████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>330.12</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.35661</td></tr><tr><td>loss_value</td><td>598.45764</td></tr><tr><td>running_reward</td><td>98.33974</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network and temp 10</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/qqa67eeh' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/qqa67eeh</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_215233-qqa67eeh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:qqa67eeh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_220253-bmxm8ok0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/bmxm8ok0' target=\"_blank\">Baseline Network 128 layers</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/bmxm8ok0' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/bmxm8ok0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 32.0094782451846\n",
      "Average Total reward: 54.6\n",
      "Running reward of episode 200/2000: 53.71425610165732\n",
      "Average Total reward: 105.75\n",
      "Running reward of episode 300/2000: 93.38760826609865\n",
      "Average Total reward: 345.7\n",
      "Running reward of episode 400/2000: 90.9043200569277\n",
      "Average Total reward: 338.7\n",
      "Running reward of episode 500/2000: 98.15367064571203\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 600/2000: 93.59512871292686\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 700/2000: 98.04483675426094\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 800/2000: 95.99248338343018\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 93.32621489131296\n",
      "Average Total reward: 207.95\n",
      "Running reward of episode 1000/2000: 85.33033286073864\n",
      "Average Total reward: 228.35\n",
      "Running reward of episode 1100/2000: 97.6791894842405\n",
      "Average Total reward: 426.1\n",
      "Running reward of episode 1200/2000: 94.66290064327141\n",
      "Average Total reward: 344.15\n",
      "Running reward of episode 1300/2000: 98.32151429902193\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 97.72802958873324\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.34400637608476\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34949277581171\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.34952525820162\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.34952545051456\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34952545165315\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 403.6785\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bmxm8ok0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▂▆▅████▃▄▇▆████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▂▆▅████▃▄▇▆████████</td></tr><tr><td>episode_length</td><td>▁▁▂▂▄▅▇▅▆█▇▇██████▃▃▄▇█▅████████████████</td></tr><tr><td>loss_policy</td><td>▆▆██▇▅▂▃▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▃▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>loss_value</td><td>▁▁▄▅█▆▆▅▆▅▅▅▅▅▅▅▅▆▅▄▆▆▅▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>running_reward</td><td>▁▂▃▄▅▇█▇▇█████████▇▇▇███████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>403.6785</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.03259</td></tr><tr><td>loss_value</td><td>598.48242</td></tr><tr><td>running_reward</td><td>98.34953</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network 128 layers</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/bmxm8ok0' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/bmxm8ok0</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_220253-bmxm8ok0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bmxm8ok0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_221328-s3b2uyoq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/s3b2uyoq' target=\"_blank\">Baseline Network 128 layers</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/s3b2uyoq' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/s3b2uyoq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 21.82209116101363\n",
      "Average Total reward: 42.25\n",
      "Running reward of episode 200/2000: 71.86424379036274\n",
      "Average Total reward: 176.95\n",
      "Running reward of episode 300/2000: 91.56399804929937\n",
      "Average Total reward: 461.5\n",
      "Running reward of episode 400/2000: 96.46635909649882\n",
      "Average Total reward: 461.9\n",
      "Running reward of episode 500/2000: 69.45056391289467\n",
      "Average Total reward: 119.4\n",
      "Running reward of episode 600/2000: 96.84742894461216\n",
      "Average Total reward: 490.0\n",
      "Running reward of episode 700/2000: 96.94150844818222\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 800/2000: 98.33302265259974\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 96.60796007612899\n",
      "Average Total reward: 470.35\n",
      "Running reward of episode 1000/2000: 97.83744630024387\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.18714837127611\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 80.59467058201587\n",
      "Average Total reward: 122.95\n",
      "Running reward of episode 1300/2000: 97.96709759161939\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.34726127633989\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.3233064059758\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34937022103387\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 87.98353929020932\n",
      "Average Total reward: 213.7\n",
      "Running reward of episode 1800/2000: 95.16618891568798\n",
      "Average Total reward: 342.2\n",
      "Running reward of episode 1900/2000: 98.284749641298\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.18483407243112\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 390.1525\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:s3b2uyoq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▃▇▇▂██████▂████▄▆██</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▃▇▇▂██████▂████▄▆██</td></tr><tr><td>episode_length</td><td>▁▁▁▂▅▅█▇▇▃▄▇█▇█████████▇▃███████▇▇▄▆██▇█</td></tr><tr><td>loss_policy</td><td>▆▆▇█▆▅▃▄▃▄▃▁▂▂▂▂▃▃▂▂▃▂▃▃▃▂▃▂▂▂▃▂▃▂▂▁▂▂▂▂</td></tr><tr><td>loss_value</td><td>▁▁▃▆█▆▆▆▆▅▅▇▆▆▆▆▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆▇▆▆▅▆</td></tr><tr><td>running_reward</td><td>▁▂▂▄▆▆███▇▆▇████████████▇████████▇▇█████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>390.1525</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.34646</td></tr><tr><td>loss_value</td><td>598.45117</td></tr><tr><td>running_reward</td><td>98.18483</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network 128 layers</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/s3b2uyoq' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/s3b2uyoq</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_221328-s3b2uyoq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:s3b2uyoq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_222423-95gp4xyw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/95gp4xyw' target=\"_blank\">Baseline Network 128 layers</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/95gp4xyw' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/95gp4xyw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 32.87386553492955\n",
      "Average Total reward: 62.1\n",
      "Running reward of episode 200/2000: 71.7765475513662\n",
      "Average Total reward: 307.4\n",
      "Running reward of episode 300/2000: 77.32814666953193\n",
      "Average Total reward: 207.25\n",
      "Running reward of episode 400/2000: 97.2856897095752\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 500/2000: 98.25577237769019\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 600/2000: 98.34897038384602\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 700/2000: 98.2959884278717\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 800/2000: 96.54816612731405\n",
      "Average Total reward: 473.35\n",
      "Running reward of episode 900/2000: 98.29286194993297\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 98.34918997374223\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.30676929946814\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 96.93669745527039\n",
      "Average Total reward: 473.75\n",
      "Running reward of episode 1300/2000: 97.77295292871008\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 94.65029017228507\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.31959166702833\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.07877980263075\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.34792249413358\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 97.57112601454477\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34491691504745\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34949816668428\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 428.02\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:95gp4xyw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▅▃█████████████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▅▃█████████████████</td></tr><tr><td>episode_length</td><td>▁▁▂▂▃▄▆████████▆▇███████▇██▅███████▇████</td></tr><tr><td>loss_policy</td><td>▆▆█▇▇▆▃▂▂▂▂▂▁▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>loss_value</td><td>▁▁▆▅██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇</td></tr><tr><td>running_reward</td><td>▁▂▃▄▆▇▇████████████████████▇████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>428.02</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.09432</td></tr><tr><td>loss_value</td><td>598.31781</td></tr><tr><td>running_reward</td><td>98.3495</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network 128 layers</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/95gp4xyw' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/95gp4xyw</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_222423-95gp4xyw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:95gp4xyw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_223613-81kn9vbc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/81kn9vbc' target=\"_blank\">Baseline Network 128 layers</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/81kn9vbc' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/81kn9vbc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 26.444223843940154\n",
      "Average Total reward: 43.85\n",
      "Running reward of episode 200/2000: 43.736361093433004\n",
      "Average Total reward: 109.45\n",
      "Running reward of episode 300/2000: 86.90578161826217\n",
      "Average Total reward: 248.1\n",
      "Running reward of episode 400/2000: 88.41146762258899\n",
      "Average Total reward: 202.4\n",
      "Running reward of episode 500/2000: 93.32662692067937\n",
      "Average Total reward: 419.8\n",
      "Running reward of episode 600/2000: 75.0133115672801\n",
      "Average Total reward: 142.8\n",
      "Running reward of episode 700/2000: 88.50847363514167\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 800/2000: 98.29126121682144\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 98.13336392430519\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 70.39035060759554\n",
      "Average Total reward: 127.85\n",
      "Running reward of episode 1100/2000: 97.9296358656161\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.34703948309645\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.23765252262099\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.34886310471458\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.3495215302155\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34952542844292\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.34952545152251\n",
      "Average Total reward: 494.7\n",
      "Running reward of episode 1800/2000: 95.19695961234845\n",
      "Average Total reward: 317.65\n",
      "Running reward of episode 1900/2000: 87.14636698801185\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 96.74297531309217\n",
      "Average Total reward: 268.7\n",
      "Average length of all episodes: 357.0285\n",
      "Testing the best policy\n",
      "Average Total reward: 328.87\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:81kn9vbc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▂▄▃▇▃███▂███████▅█▄</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▂▄▃▇▃███▂███████▅█▄</td></tr><tr><td>episode_length</td><td>▁▁▁▂▃▄▅▅▇█▆▃▂▃████▆▃▃██████████████▄▄▃█▇</td></tr><tr><td>loss_policy</td><td>▇▅▇▇█▆▄▅▁▂▂▃▁▃▁▂▂▂▃▂▃▂▂▁▁▂▂▂▂▂▂▂▂▂▂▃▃▂▂▂</td></tr><tr><td>loss_value</td><td>▂▁▄▄██▇▇▇▆▆▄▂▆▆▆▆▆▆▄▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▄▆▆</td></tr><tr><td>running_reward</td><td>▁▂▂▃▄▆▇█▇▇█▆▅▅████▇▆▆███████████████▇▆██</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>268.7</td></tr><tr><td>average_lenght_all_episodes</td><td>357.0285</td></tr><tr><td>average_total_reward</td><td>268.7</td></tr><tr><td>episode_length</td><td>442.9</td></tr><tr><td>loss_policy</td><td>0.03808</td></tr><tr><td>loss_value</td><td>595.13416</td></tr><tr><td>running_reward</td><td>96.74298</td></tr><tr><td>test_average_episode_length</td><td>328.87</td></tr><tr><td>test_average_total_reward</td><td>328.87</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network 128 layers</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/81kn9vbc' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/81kn9vbc</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_223613-81kn9vbc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:81kn9vbc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240509_224422-9lkge6zl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/9lkge6zl' target=\"_blank\">Baseline Network 128 layers</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/9lkge6zl' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/9lkge6zl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 27.32490215760632\n",
      "Average Total reward: 46.85\n",
      "Running reward of episode 200/2000: 50.07565717645881\n",
      "Average Total reward: 143.55\n",
      "Running reward of episode 300/2000: 94.660531243787\n",
      "Average Total reward: 389.35\n",
      "Running reward of episode 400/2000: 95.69167556889424\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 500/2000: 95.73467453263497\n",
      "Average Total reward: 430.1\n",
      "Running reward of episode 600/2000: 98.07848238386033\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 700/2000: 95.5742230383279\n",
      "Average Total reward: 457.6\n",
      "Running reward of episode 800/2000: 98.25599593002525\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 94.8329517703557\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 97.20180244361106\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.34273032405396\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.34948522090842\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.34952521347262\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.34952545024977\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.32758480286834\n",
      "Average Total reward: 471.1\n",
      "Running reward of episode 1600/2000: 97.7589792027083\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 96.42798253163966\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.27190802585741\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.13872295442307\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34827738931531\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 432.9675\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Baseline Network 128 layers\",\n",
    "          config={\n",
    "              \"hidden_layers\": 128,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 10,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f186e88-9437-4453-b4f8-27c0d3c8e92f",
   "metadata": {},
   "source": [
    "## With baseline, 128 hidden layer and temperature 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22488e84-fadd-44e7-b9a8-4d3ba72f0819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_111900-bmyxnwz1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/bmyxnwz1' target=\"_blank\">Baseline Network 128 layers temp 5</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/bmyxnwz1' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/bmyxnwz1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 35.797448217256715\n",
      "Average Total reward: 67.25\n",
      "Running reward of episode 200/2000: 79.6415294325236\n",
      "Average Total reward: 137.4\n",
      "Running reward of episode 300/2000: 97.29908716932266\n",
      "Average Total reward: 480.3\n",
      "Running reward of episode 400/2000: 98.34330630111522\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 500/2000: 98.34948863099743\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 600/2000: 92.47221855127512\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 700/2000: 98.31472868441936\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 800/2000: 98.34931943638274\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 98.31785339862901\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 98.3493379363445\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.34952434147003\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 95.36918384522541\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.33188025209247\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.34942098274033\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.34426159667939\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.3494942868527\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.3495252671478\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.23805113381104\n",
      "Average Total reward: 416.65\n",
      "Running reward of episode 1900/2000: 98.33387642645506\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 96.53180580685412\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 439.1475\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bmyxnwz1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▂███████████████▇██</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▂███████████████▇██</td></tr><tr><td>episode_length</td><td>▁▁▂▃▂█████▇▃███████████████████████████▇</td></tr><tr><td>loss_policy</td><td>▅▆██▄▃▃▂▂▂▂▂▁▂▂▂▁▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂</td></tr><tr><td>loss_value</td><td>▁▂▅█▃▅▅▅▅▅▅▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>running_reward</td><td>▁▂▃▅▆▇█████▇████████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>439.1475</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>436.9</td></tr><tr><td>loss_policy</td><td>0.42298</td></tr><tr><td>loss_value</td><td>581.49048</td></tr><tr><td>running_reward</td><td>96.53181</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network 128 layers temp 5</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/bmyxnwz1' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/bmyxnwz1</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_111900-bmyxnwz1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bmyxnwz1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_113047-vkteicus</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/vkteicus' target=\"_blank\">Baseline Network 128 layers temp 5</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/vkteicus' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/vkteicus</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 20.841262645814915\n",
      "Average Total reward: 29.65\n",
      "Running reward of episode 200/2000: 71.9193425366524\n",
      "Average Total reward: 144.95\n",
      "Running reward of episode 300/2000: 95.45127602692487\n",
      "Average Total reward: 469.2\n",
      "Running reward of episode 400/2000: 94.12219114288489\n",
      "Average Total reward: 257.5\n",
      "Running reward of episode 500/2000: 98.18456820757844\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 600/2000: 96.43755045398139\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 700/2000: 98.32011739779324\n",
      "Average Total reward: 478.8\n",
      "Running reward of episode 800/2000: 98.34527953749006\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 98.3475551331099\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 96.79266993178605\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 95.1890426758746\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.33081372103555\n",
      "Average Total reward: 491.65\n",
      "Running reward of episode 1300/2000: 97.7065068889819\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.34571844147041\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.34950291214489\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34952531821409\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.31245312816749\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.34930596388544\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 97.15186649235244\n",
      "Average Total reward: 441.45\n",
      "Running reward of episode 2000/2000: 98.34207903760867\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 443.4365\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vkteicus) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▃█▄██████████████▇█</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▃█▄██████████████▇█</td></tr><tr><td>episode_length</td><td>▁▁▁▂▆▅█▇█▇███████████▇███▇███████████▇██</td></tr><tr><td>loss_policy</td><td>▇▄▇█▆▇▂▂▁▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>loss_value</td><td>▂▁▄▅▇█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>running_reward</td><td>▁▂▂▄▆▇██████████████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>443.4365</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.29222</td></tr><tr><td>loss_value</td><td>598.1026</td></tr><tr><td>running_reward</td><td>98.34208</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network 128 layers temp 5</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/vkteicus' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/vkteicus</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_113047-vkteicus/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vkteicus). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_114209-qzn3ugr5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/qzn3ugr5' target=\"_blank\">Baseline Network 128 layers temp 5</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/qzn3ugr5' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/qzn3ugr5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 45.045412820910265\n",
      "Average Total reward: 82.55\n",
      "Running reward of episode 200/2000: 80.85112253017265\n",
      "Average Total reward: 120.7\n",
      "Running reward of episode 300/2000: 97.90786057883304\n",
      "Average Total reward: 490.5\n",
      "Running reward of episode 400/2000: 97.63810325223278\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 500/2000: 95.34413878063337\n",
      "Average Total reward: 494.0\n",
      "Running reward of episode 600/2000: 97.80194689958434\n",
      "Average Total reward: 488.75\n",
      "Running reward of episode 700/2000: 96.33997477460002\n",
      "Average Total reward: 372.45\n",
      "Running reward of episode 800/2000: 73.92080141748143\n",
      "Average Total reward: 300.15\n",
      "Running reward of episode 900/2000: 96.35612166099278\n",
      "Average Total reward: 422.5\n",
      "Running reward of episode 1000/2000: 98.16600390349913\n",
      "Average Total reward: 424.9\n",
      "Running reward of episode 1100/2000: 98.04089997865516\n",
      "Average Total reward: 482.6\n",
      "Running reward of episode 1200/2000: 98.20492934301527\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 97.73794884269913\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 92.29668929425789\n",
      "Average Total reward: 197.2\n",
      "Running reward of episode 1500/2000: 87.074340337908\n",
      "Average Total reward: 175.25\n",
      "Running reward of episode 1600/2000: 83.49813974651339\n",
      "Average Total reward: 181.2\n",
      "Running reward of episode 1700/2000: 86.29653175546879\n",
      "Average Total reward: 330.3\n",
      "Running reward of episode 1800/2000: 94.98032831667346\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 90.28195745092417\n",
      "Average Total reward: 472.2\n",
      "Running reward of episode 2000/2000: 97.7237495501605\n",
      "Average Total reward: 345.65\n",
      "Average length of all episodes: 367.0285\n",
      "Testing the best policy\n",
      "Average Total reward: 348.29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:qzn3ugr5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▂████▆▅▇▇███▃▃▃▅██▅</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▂████▆▅▇▇███▃▃▃▅██▅</td></tr><tr><td>episode_length</td><td>▁▁▂▃▄██▅▇██▆██▅▂▄█▆██████▇█▄▅▅▅▄▄▃█▅▇▃██</td></tr><tr><td>loss_policy</td><td>▅▆▇█▇▃▃▃▂▂▂▃▂▂▃▂▃▁▂▂▂▂▂▂▂▂▂▄▂▂▂▂▂▃▁▃▂▃▂▂</td></tr><tr><td>loss_value</td><td>▁▂▅██▆▆▆▆▆▆▆▆▆▆▂▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▆▆▆▅▆▆</td></tr><tr><td>running_reward</td><td>▁▂▄▅▆██████████▅▇███████████▇▇▇▇▇▆███▇██</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>345.65</td></tr><tr><td>average_lenght_all_episodes</td><td>367.0285</td></tr><tr><td>average_total_reward</td><td>345.65</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.58856</td></tr><tr><td>loss_value</td><td>601.22131</td></tr><tr><td>running_reward</td><td>97.72375</td></tr><tr><td>test_average_episode_length</td><td>348.29</td></tr><tr><td>test_average_total_reward</td><td>348.29</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network 128 layers temp 5</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/qzn3ugr5' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/qzn3ugr5</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_114209-qzn3ugr5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:qzn3ugr5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_115125-lgyqb224</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/lgyqb224' target=\"_blank\">Baseline Network 128 layers temp 5</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/lgyqb224' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/lgyqb224</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 28.502079069726147\n",
      "Average Total reward: 59.9\n",
      "Running reward of episode 200/2000: 54.19332413032455\n",
      "Average Total reward: 96.0\n",
      "Running reward of episode 300/2000: 87.12481725002105\n",
      "Average Total reward: 488.1\n",
      "Running reward of episode 400/2000: 79.59950670360128\n",
      "Average Total reward: 247.1\n",
      "Running reward of episode 500/2000: 97.2750826406726\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 600/2000: 97.66652157414896\n",
      "Average Total reward: 494.1\n",
      "Running reward of episode 700/2000: 98.12435511492698\n",
      "Average Total reward: 495.7\n",
      "Running reward of episode 800/2000: 98.34062218908198\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 98.34947273963373\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 98.34952513957684\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.32981761882733\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.3494087708598\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 91.66191252170232\n",
      "Average Total reward: 478.8\n",
      "Running reward of episode 1400/2000: 94.13240169562427\n",
      "Average Total reward: 461.4\n",
      "Running reward of episode 1500/2000: 98.32449197265733\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34937724021604\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.34952457416978\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.34952544646477\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 97.82643825671401\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.3464284986375\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 414.4455\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lgyqb224) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▂█▄█████████▇██████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▂█▄█████████▇██████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▂▂▆▃▄██▆██▇██████████▇██████████▆███</td></tr><tr><td>loss_policy</td><td>▆▆▆▇█▆▄▄▄▁▂▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>loss_value</td><td>▁▂▂▃█▅▆▆▇▇▆▆▇▆▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>running_reward</td><td>▁▂▃▃▅▆▇▇▇███████████████████████████▇███</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>414.4455</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.17439</td></tr><tr><td>loss_value</td><td>598.13141</td></tr><tr><td>running_reward</td><td>98.34643</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network 128 layers temp 5</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/lgyqb224' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/lgyqb224</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_115125-lgyqb224/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lgyqb224). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_120210-dsty5od2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/dsty5od2' target=\"_blank\">Baseline Network 128 layers temp 5</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/dsty5od2' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/dsty5od2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 41.683340314953234\n",
      "Average Total reward: 70.75\n",
      "Running reward of episode 200/2000: 87.95169738306129\n",
      "Average Total reward: 385.95\n",
      "Running reward of episode 300/2000: 87.71938741314705\n",
      "Average Total reward: 179.05\n",
      "Running reward of episode 400/2000: 97.84658594168975\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 500/2000: 97.6137482486996\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 600/2000: 93.79955925892523\n",
      "Average Total reward: 287.6\n",
      "Running reward of episode 700/2000: 97.86824121423288\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 800/2000: 98.34667599426898\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 900/2000: 98.34950858136422\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/2000: 98.34952535177885\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.34952545106859\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.34952545165648\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 94.83707642926711\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.32872989458794\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34940233095665\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.34952472272023\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.34952544734425\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34952545163439\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 429.5965\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Baseline Network 128 layers temp 5\",\n",
    "          config={\n",
    "              \"hidden_layers\": 128,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac2ae4a-f4e8-432d-aa66-fa50cc891a8a",
   "metadata": {},
   "source": [
    "## With baseline, 16 hidden layer and temperature 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac0652b1-5cbf-4cc1-8e67-9d80ab45be18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:5f2givem) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▂▂▃█</td></tr><tr><td>average_total_reward</td><td>▁▁▂▂▂▃█</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▄▅█▇▇▇▆▃█</td></tr><tr><td>loss_policy</td><td>▆▅▅▄▅▅▆▅▆▆▆▆▅▆▇▇▇█▇▇▇████▇▇██▇▆▅▃▃▃▁▂▂▃▁</td></tr><tr><td>loss_value</td><td>▂▁▁▁▂▁▂▂▂▂▂▂▂▂▄▄▃▄▄▄▅▅▆▆▆▅▆██▇▆▆▅▆▅▅▅▅▅▅</td></tr><tr><td>running_reward</td><td>▁▁▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▆▇▇▇▇▇█████▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>467.5</td></tr><tr><td>average_total_reward</td><td>467.5</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.09622</td></tr><tr><td>loss_value</td><td>604.20288</td></tr><tr><td>running_reward</td><td>92.11111</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network 128 layers temp 5</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/5f2givem' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/5f2givem</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_122312-5f2givem/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:5f2givem). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_122456-pna48n13</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/pna48n13' target=\"_blank\">Baseline Network 16 layers temp 5</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/pna48n13' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/pna48n13</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 21.198236506876636\n",
      "Average Total reward: 30.55\n",
      "Running reward of episode 200/2000: 31.232564610192572\n",
      "Average Total reward: 44.6\n",
      "Running reward of episode 300/2000: 44.692480121087954\n",
      "Average Total reward: 68.05\n",
      "Running reward of episode 400/2000: 66.62110245511452\n",
      "Average Total reward: 180.4\n",
      "Running reward of episode 500/2000: 88.40182042486495\n",
      "Average Total reward: 332.6\n",
      "Running reward of episode 600/2000: 83.32056016023441\n",
      "Average Total reward: 157.65\n",
      "Running reward of episode 700/2000: 91.81500888867096\n",
      "Average Total reward: 406.55\n",
      "Running reward of episode 800/2000: 96.77226491538515\n",
      "Average Total reward: 454.4\n",
      "Running reward of episode 900/2000: 97.48401223321831\n",
      "Average Total reward: 476.15\n",
      "Running reward of episode 1000/2000: 97.66977940926452\n",
      "Average Total reward: 450.95\n",
      "Running reward of episode 1100/2000: 97.60032193149338\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.12390117785313\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.34716432389259\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.27061893526657\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.02575645902446\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34760856787842\n",
      "Average Total reward: 488.8\n",
      "Running reward of episode 1700/2000: 98.3495141026935\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 97.00211866157204\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.21485714570144\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.34872814401946\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 374.909\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pna48n13) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▃▆▃▇▇█▇██████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▂▃▆▃▇▇█▇██████████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▁▂▂▃▇▆▄▃▇█▇██████▇███████████▇█████</td></tr><tr><td>loss_policy</td><td>▅▆▆▇▇▇▇██▇▇▇▇▄▃▄▃▃▂▃▃▂▁▂▂▃▂▂▁▂▂▁▂▂▂▂▂▂▂▂</td></tr><tr><td>loss_value</td><td>▁▂▂▃▃▃▄▅▇██▇▇▆▆▅▆▆▅▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>running_reward</td><td>▁▁▂▂▃▃▄▄▆▇▇▇▇▇██████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>374.909</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.22988</td></tr><tr><td>loss_value</td><td>598.46936</td></tr><tr><td>running_reward</td><td>98.34873</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network 16 layers temp 5</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/pna48n13' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/pna48n13</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_122456-pna48n13/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pna48n13). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_123525-5ttq251s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/5ttq251s' target=\"_blank\">Baseline Network 16 layers temp 5</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/5ttq251s' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/5ttq251s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 17.617126928474022\n",
      "Average Total reward: 18.95\n",
      "Running reward of episode 200/2000: 21.983712912468164\n",
      "Average Total reward: 24.15\n",
      "Running reward of episode 300/2000: 32.688632384872285\n",
      "Average Total reward: 53.8\n",
      "Running reward of episode 400/2000: 46.311647207216595\n",
      "Average Total reward: 86.35\n",
      "Running reward of episode 500/2000: 55.40946724598233\n",
      "Average Total reward: 85.45\n",
      "Running reward of episode 600/2000: 77.91658142260447\n",
      "Average Total reward: 176.95\n",
      "Running reward of episode 700/2000: 91.96008081729212\n",
      "Average Total reward: 467.5\n",
      "Running reward of episode 800/2000: 92.64993100950878\n",
      "Average Total reward: 487.55\n",
      "Running reward of episode 900/2000: 93.43870505846611\n",
      "Average Total reward: 458.7\n",
      "Running reward of episode 1000/2000: 98.17758115866054\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/2000: 98.348507450449\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 98.34951942455405\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 98.231848918837\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.26801381498599\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.33539474936698\n",
      "Average Total reward: 482.75\n",
      "Running reward of episode 1600/2000: 98.32030882397845\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.29886501774828\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 97.94142971264782\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34710930891244\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 98.3495111468162\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 353.0045\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:5ttq251s) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▂▂▃██▇███████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▂▂▂▃██▇███████████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▁▁▂▂▂▃▃▄█▆▇█▆██████████████████████</td></tr><tr><td>loss_policy</td><td>▅▅▆▇▆▆▇▇████▆▃▃▃▂▃▂▂▂▃▃▂▂▃▃▂▃▃▂▂▁▂▂▂▂▂▂▂</td></tr><tr><td>loss_value</td><td>▁▁▂▂▂▂▄▄▆▆██▇▆▅▅▅▅▅▅▅▅▅▅▅▅▆▅▆▆▅▅▆▅▅▅▅▅▅▅</td></tr><tr><td>running_reward</td><td>▁▁▂▂▂▂▃▃▄▅▅▆▇█▇▇████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>353.0045</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.01722</td></tr><tr><td>loss_value</td><td>598.54407</td></tr><tr><td>running_reward</td><td>98.34951</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network 16 layers temp 5</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/5ttq251s' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/5ttq251s</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_123525-5ttq251s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:5ttq251s). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_124504-4gxizu57</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/4gxizu57' target=\"_blank\">Baseline Network 16 layers temp 5</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/4gxizu57' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/4gxizu57</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 22.16306466148011\n",
      "Average Total reward: 26.85\n",
      "Running reward of episode 200/2000: 28.886228809847072\n",
      "Average Total reward: 45.65\n",
      "Running reward of episode 300/2000: 40.843385781872165\n",
      "Average Total reward: 66.8\n",
      "Running reward of episode 400/2000: 53.74016651778328\n",
      "Average Total reward: 84.25\n",
      "Running reward of episode 500/2000: 72.45635442557415\n",
      "Average Total reward: 136.8\n",
      "Running reward of episode 600/2000: 76.60291503680668\n",
      "Average Total reward: 179.9\n",
      "Running reward of episode 700/2000: 89.11852919517449\n",
      "Average Total reward: 205.15\n",
      "Running reward of episode 800/2000: 93.80659716912147\n",
      "Average Total reward: 380.6\n",
      "Running reward of episode 900/2000: 94.89933485951958\n",
      "Average Total reward: 360.3\n",
      "Running reward of episode 1000/2000: 96.63808651317343\n",
      "Average Total reward: 474.2\n",
      "Running reward of episode 1100/2000: 71.61713226011538\n",
      "Average Total reward: 123.05\n",
      "Running reward of episode 1200/2000: 95.22382979780787\n",
      "Average Total reward: 466.7\n",
      "Running reward of episode 1300/2000: 95.91594315899572\n",
      "Average Total reward: 380.55\n",
      "Running reward of episode 1400/2000: 97.31557264720499\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.2698443322096\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.34905369726395\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 97.9608038294285\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.25907116059527\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34568264219335\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 94.85032843320938\n",
      "Average Total reward: 341.55\n",
      "Average length of all episodes: 318.242\n",
      "Testing the best policy\n",
      "Average Total reward: 248.67\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4gxizu57) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▂▃▃▄▆▆█▂█▆██████▆</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▂▂▃▃▄▆▆█▂█▆██████▆</td></tr><tr><td>episode_length</td><td>▁▁▁▁▂▂▂▂▂▃▅▃▄▄▅▇▆▇▅▆▆▃▅▇▅▅█████████████▆</td></tr><tr><td>loss_policy</td><td>▅▅▆▆█▇██▇█▆▆▆▅▄▃▄▄▅▂▃▄▂▂▃▃▁▁▁▁▂▂▂▂▂▂▂▂▂▃</td></tr><tr><td>loss_value</td><td>▁▁▂▂▄▅▆▇▅██▇█▇▇▇▆▇▇▆▆▅▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>running_reward</td><td>▁▂▂▂▃▃▄▅▅▆▆▆▇▇▇▇█████▇▇█▇███████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>341.55</td></tr><tr><td>average_lenght_all_episodes</td><td>318.242</td></tr><tr><td>average_total_reward</td><td>341.55</td></tr><tr><td>episode_length</td><td>345.8</td></tr><tr><td>loss_policy</td><td>2.31151</td></tr><tr><td>loss_value</td><td>637.19031</td></tr><tr><td>running_reward</td><td>94.85033</td></tr><tr><td>test_average_episode_length</td><td>248.67</td></tr><tr><td>test_average_total_reward</td><td>248.67</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network 16 layers temp 5</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/4gxizu57' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/4gxizu57</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_124504-4gxizu57/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4gxizu57). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_125317-rzz2tto3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/rzz2tto3' target=\"_blank\">Baseline Network 16 layers temp 5</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/rzz2tto3' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/rzz2tto3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 22.154159010378276\n",
      "Average Total reward: 23.6\n",
      "Running reward of episode 200/2000: 42.640429757797364\n",
      "Average Total reward: 60.85\n",
      "Running reward of episode 300/2000: 58.28422419342057\n",
      "Average Total reward: 149.9\n",
      "Running reward of episode 400/2000: 70.13383411703677\n",
      "Average Total reward: 243.9\n",
      "Running reward of episode 500/2000: 93.65025409918812\n",
      "Average Total reward: 400.7\n",
      "Running reward of episode 600/2000: 97.21920257720252\n",
      "Average Total reward: 458.45\n",
      "Running reward of episode 700/2000: 96.36180964935616\n",
      "Average Total reward: 490.35\n",
      "Running reward of episode 800/2000: 98.21942163017954\n",
      "Average Total reward: 468.2\n",
      "Running reward of episode 900/2000: 98.29505146390808\n",
      "Average Total reward: 477.15\n",
      "Running reward of episode 1000/2000: 81.07692798090767\n",
      "Average Total reward: 180.8\n",
      "Running reward of episode 1100/2000: 95.57818115147941\n",
      "Average Total reward: 454.75\n",
      "Running reward of episode 1200/2000: 94.65561841689339\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/2000: 96.99980151584307\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.25317035156071\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 98.34895497847427\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 97.8527202386137\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 98.33989343168383\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/2000: 98.3494684250042\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.34952511403199\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward of episode 2000/2000: 97.57133208039082\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 368.136\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:rzz2tto3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▂▃▄▇▇███▃▇█████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▂▃▄▇▇███▃▇█████████</td></tr><tr><td>episode_length</td><td>▁▁▁▁▂▂▃▄▃▅▆█▇▇████▃▃▃██▇███████▇██████▆█</td></tr><tr><td>loss_policy</td><td>▆▆▅█▇██▇▆▅▄▃▂▄▃▃▃▄▇▆▅▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁</td></tr><tr><td>loss_value</td><td>▁▁▁▄▄▇██▅▅▅▄▄▅▄▅▄▅▇▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▄</td></tr><tr><td>running_reward</td><td>▁▂▂▃▄▅▆▆▆▇▇███████▇▆▆███████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>368.136</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.95553</td></tr><tr><td>loss_value</td><td>600.33997</td></tr><tr><td>running_reward</td><td>97.57133</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline Network 16 layers temp 5</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/rzz2tto3' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/rzz2tto3</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_125317-rzz2tto3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:rzz2tto3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_130828-gugqe96c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/gugqe96c' target=\"_blank\">Baseline Network 16 layers temp 5</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/gugqe96c' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/gugqe96c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/2000: 20.81301291478362\n",
      "Average Total reward: 20.85\n",
      "Running reward of episode 200/2000: 22.536055472466934\n",
      "Average Total reward: 25.85\n",
      "Running reward of episode 300/2000: 38.754879320110874\n",
      "Average Total reward: 49.8\n",
      "Running reward of episode 400/2000: 46.98872133705153\n",
      "Average Total reward: 72.1\n",
      "Running reward of episode 500/2000: 71.14876500291152\n",
      "Average Total reward: 170.45\n",
      "Running reward of episode 600/2000: 84.52754520848063\n",
      "Average Total reward: 373.3\n",
      "Running reward of episode 700/2000: 95.90888860307658\n",
      "Average Total reward: 446.6\n",
      "Running reward of episode 800/2000: 96.88433790762633\n",
      "Average Total reward: 488.65\n",
      "Running reward of episode 900/2000: 98.15760126368826\n",
      "Average Total reward: 489.35\n",
      "Running reward of episode 1000/2000: 96.86923938458297\n",
      "Average Total reward: 463.85\n",
      "Running reward of episode 1100/2000: 97.92858538331704\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/2000: 97.43464217564056\n",
      "Average Total reward: 496.25\n",
      "Running reward of episode 1300/2000: 98.04559152240903\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/2000: 98.3465619032148\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/2000: 97.9957572773852\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/2000: 98.18202558507042\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/2000: 97.08182828275858\n",
      "Average Total reward: 498.5\n",
      "Running reward of episode 1800/2000: 93.97941734683715\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/2000: 98.32365209892933\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/2000: 97.9347733174998\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 361.4645\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Baseline Network 16 layers temp 5\",\n",
    "          config={\n",
    "              \"hidden_layers\": 16,\n",
    "              \"num_episodes\": 2000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 20,\n",
    "              \"test_episodes\" : 200,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc5eec-9ebc-4867-b698-cfefbe6387d8",
   "metadata": {},
   "source": [
    " # Longer runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a52700c-e8cd-4048-b567-bb885b9eea04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_172044-6j08k3hv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/6j08k3hv' target=\"_blank\">Long Standard</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/6j08k3hv' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/6j08k3hv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/5000: 25.929437324089378\n",
      "Average Total reward: 34.88\n",
      "Running reward of episode 200/5000: 66.22344520865079\n",
      "Average Total reward: 220.42\n",
      "Running reward of episode 300/5000: 93.13194041076233\n",
      "Average Total reward: 446.58\n",
      "Running reward of episode 400/5000: 97.69084853801253\n",
      "Average Total reward: 497.64\n",
      "Running reward of episode 500/5000: 91.6378924596298\n",
      "Average Total reward: 445.74\n",
      "Running reward of episode 600/5000: 97.87126826870261\n",
      "Average Total reward: 496.68\n",
      "Running reward of episode 700/5000: 98.23325474611671\n",
      "Average Total reward: 487.94\n",
      "Running reward of episode 800/5000: 97.9213996811954\n",
      "Average Total reward: 496.42\n",
      "Running reward of episode 900/5000: 97.29732643216911\n",
      "Average Total reward: 464.68\n",
      "Running reward of episode 1000/5000: 96.43985168253451\n",
      "Average Total reward: 465.9\n",
      "Running reward of episode 1100/5000: 95.6770866155626\n",
      "Average Total reward: 485.62\n",
      "Running reward of episode 1200/5000: 96.45996353983907\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/5000: 98.33833824514737\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/5000: 97.81372249677094\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/5000: 98.34635321460921\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/5000: 98.34950667033779\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/5000: 98.34952534046457\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/5000: 98.34952545100163\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/5000: 98.34952545165606\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2100/5000: 98.34926338495842\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2200/5000: 98.3495239000864\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2300/5000: 98.34952544247382\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2400/5000: 98.34952545160556\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2500/5000: 98.34952545165962\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2600/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2700/5000: 94.54130338671894\n",
      "Average Total reward: 469.9\n",
      "Running reward of episode 2800/5000: 70.3336157211271\n",
      "Average Total reward: 123.12\n",
      "Running reward of episode 2900/5000: 59.22917163968234\n",
      "Average Total reward: 105.34\n",
      "Running reward of episode 3000/5000: 79.93188230846587\n",
      "Average Total reward: 218.86\n",
      "Running reward of episode 3100/5000: 97.60459785776405\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3200/5000: 97.50452859537756\n",
      "Average Total reward: 385.14\n",
      "Running reward of episode 3300/5000: 98.02503413786158\n",
      "Average Total reward: 496.48\n",
      "Running reward of episode 3400/5000: 98.34760429135487\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3500/5000: 94.54359205920031\n",
      "Average Total reward: 343.0\n",
      "Running reward of episode 3600/5000: 98.3239741859001\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3700/5000: 98.34937417464442\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3800/5000: 98.34952455601996\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3900/5000: 97.95625361996674\n",
      "Average Total reward: 327.58\n",
      "Running reward of episode 4000/5000: 98.32395289617769\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4100/5000: 98.34937404859795\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4200/5000: 98.34952455527369\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4300/5000: 98.34952544635289\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4400/5000: 98.34952545162854\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4500/5000: 98.34626561865768\n",
      "Average Total reward: 498.44\n",
      "Running reward of episode 4600/5000: 98.31525708714544\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4700/5000: 98.34932256480651\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4800/5000: 98.34952425046241\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4900/5000: 98.34952544454823\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 5000/5000: 98.34952545161784\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 445.5856\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6j08k3hv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▄▇████▇██████████████▂▂█▆█████▅████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▄▇████▇██████████████▂▂█▆█████▅████████</td></tr><tr><td>episode_length</td><td>▁▂▇███████████████████▂▂▆▆██████████████</td></tr><tr><td>loss_policy</td><td>▇▂▇▆▇▇▇▇▇▅▆▇▆▆▅▆▅▅▆▇▅▅▆▁▆▇▄▄▇▇▃▅▆█▇▄█▆█▇</td></tr><tr><td>running_reward</td><td>▁▁▇███████████████████▅▅████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>445.5856</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.00373</td></tr><tr><td>running_reward</td><td>98.34953</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Long Standard</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/6j08k3hv' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/6j08k3hv</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_172044-6j08k3hv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6j08k3hv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_174923-9peeveyg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/9peeveyg' target=\"_blank\">Long Standard</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/9peeveyg' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/9peeveyg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/5000: 23.881031570633947\n",
      "Average Total reward: 43.96\n",
      "Running reward of episode 200/5000: 61.357614959251784\n",
      "Average Total reward: 137.44\n",
      "Running reward of episode 300/5000: 90.70079348025416\n",
      "Average Total reward: 293.94\n",
      "Running reward of episode 400/5000: 97.04527767035847\n",
      "Average Total reward: 449.1\n",
      "Running reward of episode 500/5000: 88.00431507793684\n",
      "Average Total reward: 324.28\n",
      "Running reward of episode 600/5000: 90.81557652559184\n",
      "Average Total reward: 342.0\n",
      "Running reward of episode 700/5000: 98.26493084813076\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 800/5000: 98.34902460683786\n",
      "Average Total reward: 492.16\n",
      "Running reward of episode 900/5000: 98.33670367946526\n",
      "Average Total reward: 498.64\n",
      "Running reward of episode 1000/5000: 98.24892454394835\n",
      "Average Total reward: 478.48\n",
      "Running reward of episode 1100/5000: 93.7856017900028\n",
      "Average Total reward: 305.54\n",
      "Running reward of episode 1200/5000: 98.09485637274152\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/5000: 98.3480176759367\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/5000: 98.34951652482972\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/5000: 98.3495253988084\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/5000: 98.22219433470464\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/5000: 98.34877158406135\n",
      "Average Total reward: 490.5\n",
      "Running reward of episode 1800/5000: 97.69242949967061\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/5000: 98.34563509587564\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/5000: 98.34950241869487\n",
      "Average Total reward: 498.32\n",
      "Running reward of episode 2100/5000: 98.3495253152926\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2200/5000: 98.34952545085258\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2300/5000: 98.3495254516552\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2400/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2500/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2600/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2700/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2800/5000: 98.34952545165982\n",
      "Average Total reward: 471.16\n",
      "Running reward of episode 2900/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3000/5000: 98.31349494381386\n",
      "Average Total reward: 480.74\n",
      "Running reward of episode 3100/5000: 98.28716752595909\n",
      "Average Total reward: 449.4\n",
      "Running reward of episode 3200/5000: 97.69637409600342\n",
      "Average Total reward: 494.92\n",
      "Running reward of episode 3300/5000: 97.3651585382089\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3400/5000: 98.34369747858536\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3500/5000: 98.34949094697507\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3600/5000: 98.34952524737395\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3700/5000: 98.34952545045046\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3800/5000: 98.3495254516528\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3900/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4000/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4100/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4200/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4300/5000: 98.34952545165982\n",
      "Average Total reward: 491.96\n",
      "Running reward of episode 4400/5000: 98.13455435951145\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4500/5000: 98.3351862890515\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4600/5000: 98.34815551064948\n",
      "Average Total reward: 494.68\n",
      "Running reward of episode 4700/5000: 98.34951734088416\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4800/5000: 98.34952540363987\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4900/5000: 98.34952545137567\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 5000/5000: 98.34952545165827\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 470.0716\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9peeveyg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▂▅▇▆███▅███████████████▇███████████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▂▅▇▆███▅███████████████▇███████████████</td></tr><tr><td>episode_length</td><td>▁▁▅█▇███▆███████████████████████████████</td></tr><tr><td>loss_policy</td><td>▄▂▅▂▁▃█▃▄▃▅▅▅▁▃▅▄▂▅▄▅▆▃▃▁▅▃▃▄▅▆▄▃▃▄▂▆▃▇▄</td></tr><tr><td>running_reward</td><td>▁▂▇█▇███████████████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>470.0716</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.00897</td></tr><tr><td>running_reward</td><td>98.34953</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Long Standard</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/9peeveyg' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/9peeveyg</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_174923-9peeveyg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9peeveyg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_181843-zsj1ijuk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/zsj1ijuk' target=\"_blank\">Long Standard</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/zsj1ijuk' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/zsj1ijuk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/5000: 24.97650669296172\n",
      "Average Total reward: 31.3\n",
      "Running reward of episode 200/5000: 45.921708582733174\n",
      "Average Total reward: 86.4\n",
      "Running reward of episode 300/5000: 87.27037965547721\n",
      "Average Total reward: 324.7\n",
      "Running reward of episode 400/5000: 92.80076757091595\n",
      "Average Total reward: 444.28\n",
      "Running reward of episode 500/5000: 97.74359558527486\n",
      "Average Total reward: 474.4\n",
      "Running reward of episode 600/5000: 96.04065731338116\n",
      "Average Total reward: 457.82\n",
      "Running reward of episode 700/5000: 97.14494061084474\n",
      "Average Total reward: 486.8\n",
      "Running reward of episode 800/5000: 97.57150191439311\n",
      "Average Total reward: 485.28\n",
      "Running reward of episode 900/5000: 98.3438696457183\n",
      "Average Total reward: 480.34\n",
      "Running reward of episode 1000/5000: 98.08748301236918\n",
      "Average Total reward: 487.18\n",
      "Running reward of episode 1100/5000: 97.68392732577747\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/5000: 80.79372477534717\n",
      "Average Total reward: 85.36\n",
      "Running reward of episode 1300/5000: 98.05309581282656\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/5000: 98.34777043132148\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/5000: 98.34951506101075\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/5000: 98.34952539014179\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/5000: 97.9814227015189\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/5000: 98.3356236380409\n",
      "Average Total reward: 489.84\n",
      "Running reward of episode 1900/5000: 98.05162381400778\n",
      "Average Total reward: 485.56\n",
      "Running reward of episode 2000/5000: 98.34748895094441\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2100/5000: 98.34951339449796\n",
      "Average Total reward: 492.42\n",
      "Running reward of episode 2200/5000: 98.26358763773116\n",
      "Average Total reward: 457.9\n",
      "Running reward of episode 2300/5000: 96.10061261787932\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2400/5000: 98.33621069751358\n",
      "Average Total reward: 499.94\n",
      "Running reward of episode 2500/5000: 98.34848095688064\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2600/5000: 98.34951926769807\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2700/5000: 98.34952541504762\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2800/5000: 98.3495254514432\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2900/5000: 98.34761198881937\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3000/5000: 97.68940157079548\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3100/5000: 98.34561716893428\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3200/5000: 98.34950231255789\n",
      "Average Total reward: 499.18\n",
      "Running reward of episode 3300/5000: 98.28174296478302\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3400/5000: 98.34912414346577\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3500/5000: 98.34952307570308\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3600/5000: 98.34952543759303\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3700/5000: 98.34952545157668\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3800/5000: 98.34952545165946\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3900/5000: 98.34952545165982\n",
      "Average Total reward: 499.5\n",
      "Running reward of episode 4000/5000: 97.17830985570963\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4100/5000: 98.08047227132988\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4200/5000: 98.13813533091798\n",
      "Average Total reward: 485.38\n",
      "Running reward of episode 4300/5000: 98.32406597457664\n",
      "Average Total reward: 497.9\n",
      "Running reward of episode 4400/5000: 94.6639482631399\n",
      "Average Total reward: 253.62\n",
      "Running reward of episode 4500/5000: 71.70248248542688\n",
      "Average Total reward: 131.78\n",
      "Running reward of episode 4600/5000: 95.51242023194604\n",
      "Average Total reward: 485.64\n",
      "Running reward of episode 4700/5000: 97.65288705859359\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4800/5000: 98.03382247589732\n",
      "Average Total reward: 447.2\n",
      "Running reward of episode 4900/5000: 98.31763104899252\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 5000/5000: 98.34933661991703\n",
      "Average Total reward: 499.08\n",
      "Average length of all episodes: 446.8126\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zsj1ijuk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▂▅▇▇████▂███████▇█████████████████▄██▇█</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▂▅▇▇████▂███████▇█████████████████▄██▇█</td></tr><tr><td>episode_length</td><td>▁▁▄▇██▇█▄██████████████▅████████▇██▃████</td></tr><tr><td>loss_policy</td><td>█▁▆▄▆▃▅▆▃▄▅▅▅▅▄▅▆▄▅▅▄▃▂▂▃▃▃█▅▃▅▃▆▄▅▄▄▅▅█</td></tr><tr><td>running_reward</td><td>▁▁▅▇███████████████████████████████▅████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>499.08</td></tr><tr><td>average_lenght_all_episodes</td><td>446.8126</td></tr><tr><td>average_total_reward</td><td>499.08</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.00377</td></tr><tr><td>running_reward</td><td>98.34934</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Long Standard</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/zsj1ijuk' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/zsj1ijuk</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_181843-zsj1ijuk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zsj1ijuk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_184514-hrv2yanx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/hrv2yanx' target=\"_blank\">Long Standard</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/hrv2yanx' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/hrv2yanx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/5000: 23.553795438268196\n",
      "Average Total reward: 36.92\n",
      "Running reward of episode 200/5000: 48.850442055002205\n",
      "Average Total reward: 120.02\n",
      "Running reward of episode 300/5000: 87.3899667134479\n",
      "Average Total reward: 381.84\n",
      "Running reward of episode 400/5000: 96.9750300420671\n",
      "Average Total reward: 405.3\n",
      "Running reward of episode 500/5000: 94.92428300482032\n",
      "Average Total reward: 424.54\n",
      "Running reward of episode 600/5000: 95.3577596200834\n",
      "Average Total reward: 375.52\n",
      "Running reward of episode 700/5000: 96.93442734276559\n",
      "Average Total reward: 353.04\n",
      "Running reward of episode 800/5000: 96.63340597882762\n",
      "Average Total reward: 480.56\n",
      "Running reward of episode 900/5000: 93.35433063628705\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1000/5000: 98.31995125479428\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/5000: 97.41970080271187\n",
      "Average Total reward: 479.3\n",
      "Running reward of episode 1200/5000: 97.80065968394203\n",
      "Average Total reward: 460.22\n",
      "Running reward of episode 1300/5000: 97.29289693520056\n",
      "Average Total reward: 492.3\n",
      "Running reward of episode 1400/5000: 98.28423752283175\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/5000: 97.22370521222277\n",
      "Average Total reward: 490.52\n",
      "Running reward of episode 1600/5000: 98.13870112268881\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/5000: 98.34827726005993\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/5000: 98.34951806170513\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/5000: 98.34952540790749\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/5000: 98.34952545140091\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2100/5000: 98.15923730438682\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2200/5000: 98.27089710049182\n",
      "Average Total reward: 490.46\n",
      "Running reward of episode 2300/5000: 97.44775902918568\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2400/5000: 97.41444419772492\n",
      "Average Total reward: 460.68\n",
      "Running reward of episode 2500/5000: 96.63649834426172\n",
      "Average Total reward: 491.82\n",
      "Running reward of episode 2600/5000: 96.54680702701432\n",
      "Average Total reward: 492.8\n",
      "Running reward of episode 2700/5000: 96.85186863210703\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2800/5000: 95.77694212801217\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2900/5000: 98.33429439692055\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3000/5000: 98.34943527575531\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3100/5000: 98.34952491777088\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3200/5000: 98.34952544849905\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3300/5000: 98.34952545164123\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3400/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3500/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3600/5000: 98.34952545165982\n",
      "Average Total reward: 494.66\n",
      "Running reward of episode 3700/5000: 98.34229281834226\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3800/5000: 98.34948263064307\n",
      "Average Total reward: 481.14\n",
      "Running reward of episode 3900/5000: 97.67981991486783\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4000/5000: 98.06405787715384\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4100/5000: 98.34783533254362\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4200/5000: 98.34951544526031\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4300/5000: 98.34952539241677\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4400/5000: 98.3495254513092\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4500/5000: 98.34952545165788\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4600/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4700/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4800/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4900/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 5000/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 461.3368\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hrv2yanx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▂▆▇▆▆███▇█████████▇████████████████████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▂▆▇▆▆███▇█████████▇████████████████████</td></tr><tr><td>episode_length</td><td>▁▂▅▇▆███████▇███▇███▇▇████████▆█████████</td></tr><tr><td>loss_policy</td><td>▅█▃▃▁▅▆▆▅▅▂▅▂▅▆▆▅▄▄▅▂▃▄▄▄▃▅▇▄▄▂▃▅▆▅█▆▄▇▆</td></tr><tr><td>running_reward</td><td>▁▂▆███▇▇████████████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>461.3368</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.00442</td></tr><tr><td>running_reward</td><td>98.34953</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Long Standard</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/hrv2yanx' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/hrv2yanx</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_184514-hrv2yanx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:hrv2yanx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_191343-042n8u7m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/042n8u7m' target=\"_blank\">Long Standard</a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/042n8u7m' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/042n8u7m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with standardization baseline.\n",
      "Running reward of episode 100/5000: 26.83923487458664\n",
      "Average Total reward: 45.16\n",
      "Running reward of episode 200/5000: 71.07414355893434\n",
      "Average Total reward: 291.66\n",
      "Running reward of episode 300/5000: 75.59806452324817\n",
      "Average Total reward: 120.5\n",
      "Running reward of episode 400/5000: 64.91610301583613\n",
      "Average Total reward: 167.26\n",
      "Running reward of episode 500/5000: 60.49810698208862\n",
      "Average Total reward: 166.26\n",
      "Running reward of episode 600/5000: 83.81780691453102\n",
      "Average Total reward: 180.28\n",
      "Running reward of episode 700/5000: 84.25354414056797\n",
      "Average Total reward: 187.3\n",
      "Running reward of episode 800/5000: 89.31049440453363\n",
      "Average Total reward: 270.28\n",
      "Running reward of episode 900/5000: 87.40660873201648\n",
      "Average Total reward: 254.5\n",
      "Running reward of episode 1000/5000: 91.23645942247299\n",
      "Average Total reward: 299.2\n",
      "Running reward of episode 1100/5000: 94.95247169705188\n",
      "Average Total reward: 403.0\n",
      "Running reward of episode 1200/5000: 97.71454417256952\n",
      "Average Total reward: 495.96\n",
      "Running reward of episode 1300/5000: 97.73314267177034\n",
      "Average Total reward: 450.66\n",
      "Running reward of episode 1400/5000: 97.75502336023142\n",
      "Average Total reward: 488.0\n",
      "Running reward of episode 1500/5000: 98.03383809286734\n",
      "Average Total reward: 453.0\n",
      "Running reward of episode 1600/5000: 98.32955281710298\n",
      "Average Total reward: 475.74\n",
      "Running reward of episode 1700/5000: 98.16027703063934\n",
      "Average Total reward: 496.1\n",
      "Running reward of episode 1800/5000: 98.33094020841216\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/5000: 97.31002492899007\n",
      "Average Total reward: 287.22\n",
      "Running reward of episode 2000/5000: 70.09232309451198\n",
      "Average Total reward: 102.36\n",
      "Running reward of episode 2100/5000: 44.012018214286805\n",
      "Average Total reward: 60.7\n",
      "Running reward of episode 2200/5000: 85.90237479107815\n",
      "Average Total reward: 216.26\n",
      "Running reward of episode 2300/5000: 87.1866130856949\n",
      "Average Total reward: 325.36\n",
      "Running reward of episode 2400/5000: 89.668908185264\n",
      "Average Total reward: 296.12\n",
      "Running reward of episode 2500/5000: 79.30945990823484\n",
      "Average Total reward: 159.48\n",
      "Running reward of episode 2600/5000: 83.31821817365591\n",
      "Average Total reward: 196.84\n",
      "Running reward of episode 2700/5000: 80.60755684030558\n",
      "Average Total reward: 158.26\n",
      "Running reward of episode 2800/5000: 84.71235636431079\n",
      "Average Total reward: 194.74\n",
      "Running reward of episode 2900/5000: 95.54390960888816\n",
      "Average Total reward: 408.78\n",
      "Running reward of episode 3000/5000: 96.51912053682699\n",
      "Average Total reward: 366.54\n",
      "Running reward of episode 3100/5000: 84.24532110338004\n",
      "Average Total reward: 171.12\n",
      "Running reward of episode 3200/5000: 95.98211693656677\n",
      "Average Total reward: 410.84\n",
      "Running reward of episode 3300/5000: 89.80081305625109\n",
      "Average Total reward: 286.72\n",
      "Running reward of episode 3400/5000: 98.27762472896816\n",
      "Average Total reward: 496.3\n",
      "Running reward of episode 3500/5000: 98.33500667036928\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3600/5000: 97.98571703043372\n",
      "Average Total reward: 464.56\n",
      "Running reward of episode 3700/5000: 83.69988123999197\n",
      "Average Total reward: 168.12\n",
      "Running reward of episode 3800/5000: 89.50429022162652\n",
      "Average Total reward: 241.64\n",
      "Running reward of episode 3900/5000: 96.40369309752482\n",
      "Average Total reward: 490.5\n",
      "Running reward of episode 4000/5000: 95.87793388950872\n",
      "Average Total reward: 292.4\n",
      "Running reward of episode 4100/5000: 98.27487927642017\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4200/5000: 97.40601843960944\n",
      "Average Total reward: 387.28\n",
      "Running reward of episode 4300/5000: 97.69403544000504\n",
      "Average Total reward: 499.38\n",
      "Running reward of episode 4400/5000: 94.96440609239653\n",
      "Average Total reward: 330.32\n",
      "Running reward of episode 4500/5000: 94.84458308846366\n",
      "Average Total reward: 246.68\n",
      "Running reward of episode 4600/5000: 96.2146585376961\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4700/5000: 98.3368859097143\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4800/5000: 98.34945061888254\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4900/5000: 97.71724100650323\n",
      "Average Total reward: 498.54\n",
      "Running reward of episode 5000/5000: 98.08131648985317\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 324.567\n",
      "Testing the best policy\n",
      "Average Total reward: 473.328\n"
     ]
    }
   ],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Long Standard\",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 5000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'std',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 50,\n",
    "              \"test_episodes\" : 500,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= config.baseline, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf6d680-b49d-4b82-81e8-fb431e225e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:042n8u7m) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▅▂▃▃▃▄▄▇█▇████▅▁▄▅▅▃▃▃▇▃▇▅█▇▃▄██▆█▅████</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▅▂▃▃▃▄▄▇█▇████▅▁▄▅▅▃▃▃▇▃▇▅█▇▃▄██▆█▅████</td></tr><tr><td>episode_length</td><td>▁▂▃▃▄▃▄▅▆▇█████▅▂▄▄▆▃▃▃▇▄▇▇█▇▃▄▇█▇▅▅████</td></tr><tr><td>loss_policy</td><td>▅▁▄▇█▅█▄▄▅▅▅▅▆▆▄▂█▇▆▃▇▇▆▄▆▄▇▅▆▃▅▅▃▆▄█▅▄▆</td></tr><tr><td>running_reward</td><td>▁▁▆▄▅▆▇▇▇███████▄▆▇▇▇▇▇█▇████▆▇█████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>324.567</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.01108</td></tr><tr><td>running_reward</td><td>98.08132</td></tr><tr><td>test_average_episode_length</td><td>473.328</td></tr><tr><td>test_average_total_reward</td><td>473.328</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Long Standard</strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/042n8u7m' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/042n8u7m</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_191343-042n8u7m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:042n8u7m). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_193404-tnzjt0t8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/tnzjt0t8' target=\"_blank\">Long Baseline Network </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/tnzjt0t8' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/tnzjt0t8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/5000: 18.385507946235805\n",
      "Average Total reward: 25.22\n",
      "Running reward of episode 200/5000: 27.72983752575506\n",
      "Average Total reward: 32.12\n",
      "Running reward of episode 300/5000: 64.22468848056272\n",
      "Average Total reward: 155.14\n",
      "Running reward of episode 400/5000: 89.10697015616128\n",
      "Average Total reward: 335.62\n",
      "Running reward of episode 500/5000: 95.63168623651953\n",
      "Average Total reward: 415.32\n",
      "Running reward of episode 600/5000: 93.84854663320502\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 700/5000: 98.25783903420837\n",
      "Average Total reward: 473.08\n",
      "Running reward of episode 800/5000: 97.15665590218028\n",
      "Average Total reward: 467.8\n",
      "Running reward of episode 900/5000: 98.2840907545666\n",
      "Average Total reward: 465.64\n",
      "Running reward of episode 1000/5000: 98.18950170364234\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/5000: 98.34615702986025\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/5000: 98.34950550882024\n",
      "Average Total reward: 496.38\n",
      "Running reward of episode 1300/5000: 98.34952533358778\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/5000: 98.3495254509609\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/5000: 98.34952545165581\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/5000: 98.32962924616493\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/5000: 98.25898769537997\n",
      "Average Total reward: 481.8\n",
      "Running reward of episode 2000/5000: 98.24574295025072\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2100/5000: 98.34891100432779\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2200/5000: 98.34952181380658\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2300/5000: 98.34952543012191\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2400/5000: 98.34952545153241\n",
      "Average Total reward: 494.54\n",
      "Running reward of episode 2500/5000: 97.95563913978995\n",
      "Average Total reward: 479.24\n",
      "Running reward of episode 2600/5000: 87.40089557407588\n",
      "Average Total reward: 384.0\n",
      "Running reward of episode 2700/5000: 82.51188800846302\n",
      "Average Total reward: 131.08\n",
      "Running reward of episode 2800/5000: 98.21828193370371\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2900/5000: 98.23216923570604\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3000/5000: 95.48930466661595\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3100/5000: 97.43096781025399\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3200/5000: 98.34408710430344\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3300/5000: 98.34949325376553\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3400/5000: 98.34952526103139\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3500/5000: 98.34952545053135\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3600/5000: 98.06269606644418\n",
      "Average Total reward: 461.58\n",
      "Running reward of episode 3700/5000: 98.34533057736606\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3800/5000: 98.34950061578414\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3900/5000: 98.34952530461842\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4000/5000: 98.3495254507894\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4100/5000: 98.34952545165477\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4200/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4300/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4400/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4500/5000: 98.34952545165982\n",
      "Average Total reward: 411.64\n",
      "Running reward of episode 4600/5000: 96.15900826386725\n",
      "Average Total reward: 355.34\n",
      "Running reward of episode 4700/5000: 87.94187016892681\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4800/5000: 98.28790662444338\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4900/5000: 98.34916063559288\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 5000/5000: 98.34952329175577\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 450.5462\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:tnzjt0t8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▃▆███▇████████████▆▃██████▇███████▆███</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▃▆███▇████████████▆▃██████▇███████▆███</td></tr><tr><td>episode_length</td><td>▁▁▂▅▇████████████████▇█▅████████████▇███</td></tr><tr><td>loss_policy</td><td>▅▅█▆▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>loss_value</td><td>▁▁▇█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>running_reward</td><td>▁▁▃▇▇████████████████▇█▇████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>450.5462</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>-0.31685</td></tr><tr><td>loss_value</td><td>598.28729</td></tr><tr><td>running_reward</td><td>98.34952</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Long Baseline Network </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/tnzjt0t8' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/tnzjt0t8</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_193404-tnzjt0t8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:tnzjt0t8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_200106-ekv1ybj9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ekv1ybj9' target=\"_blank\">Long Baseline Network </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ekv1ybj9' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ekv1ybj9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/5000: 16.83578667352332\n",
      "Average Total reward: 20.38\n",
      "Running reward of episode 200/5000: 20.976225015836192\n",
      "Average Total reward: 30.86\n",
      "Running reward of episode 300/5000: 36.95819186369441\n",
      "Average Total reward: 70.76\n",
      "Running reward of episode 400/5000: 38.819241672584\n",
      "Average Total reward: 81.48\n",
      "Running reward of episode 500/5000: 96.0395940629453\n",
      "Average Total reward: 492.72\n",
      "Running reward of episode 600/5000: 97.31956066094345\n",
      "Average Total reward: 450.82\n",
      "Running reward of episode 700/5000: 97.61191316214983\n",
      "Average Total reward: 478.96\n",
      "Running reward of episode 800/5000: 96.33065113235315\n",
      "Average Total reward: 481.86\n",
      "Running reward of episode 900/5000: 96.68219995549433\n",
      "Average Total reward: 491.32\n",
      "Running reward of episode 1000/5000: 97.21477078743885\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/5000: 98.29640428899066\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/5000: 98.07151779140217\n",
      "Average Total reward: 491.4\n",
      "Running reward of episode 1300/5000: 98.33526550150425\n",
      "Average Total reward: 495.42\n",
      "Running reward of episode 1400/5000: 98.32446524398132\n",
      "Average Total reward: 468.36\n",
      "Running reward of episode 1500/5000: 96.29201937835882\n",
      "Average Total reward: 411.42\n",
      "Running reward of episode 1600/5000: 98.2809618200613\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/5000: 95.77807170765614\n",
      "Average Total reward: 490.74\n",
      "Running reward of episode 1800/5000: 98.3250806500368\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/5000: 98.33924674272453\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/5000: 98.34946459626335\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2100/5000: 98.34952509136377\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2200/5000: 95.9995883310386\n",
      "Average Total reward: 490.88\n",
      "Running reward of episode 2300/5000: 98.30499698153739\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2400/5000: 98.34926181955144\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2500/5000: 98.34952389081835\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2600/5000: 98.34952544241895\n",
      "Average Total reward: 497.16\n",
      "Running reward of episode 2700/5000: 98.34952545160523\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2800/5000: 98.34952545165962\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2900/5000: 94.80466300914986\n",
      "Average Total reward: 189.5\n",
      "Running reward of episode 3000/5000: 54.228522099410554\n",
      "Average Total reward: 81.3\n",
      "Running reward of episode 3100/5000: 81.52865947426778\n",
      "Average Total reward: 209.78\n",
      "Running reward of episode 3200/5000: 82.72711422059895\n",
      "Average Total reward: 212.6\n",
      "Running reward of episode 3300/5000: 96.50064212023351\n",
      "Average Total reward: 380.0\n",
      "Running reward of episode 3400/5000: 97.4535176702615\n",
      "Average Total reward: 333.82\n",
      "Running reward of episode 3500/5000: 94.68593963117803\n",
      "Average Total reward: 329.48\n",
      "Running reward of episode 3600/5000: 98.31601464868778\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3700/5000: 98.34932704997176\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3800/5000: 95.98844843835354\n",
      "Average Total reward: 263.16\n",
      "Running reward of episode 3900/5000: 80.152485842973\n",
      "Average Total reward: 165.84\n",
      "Running reward of episode 4000/5000: 79.01644772053935\n",
      "Average Total reward: 162.56\n",
      "Running reward of episode 4100/5000: 67.05531275398128\n",
      "Average Total reward: 101.06\n",
      "Running reward of episode 4200/5000: 79.47472307978131\n",
      "Average Total reward: 172.68\n",
      "Running reward of episode 4300/5000: 70.36449691324371\n",
      "Average Total reward: 133.14\n",
      "Running reward of episode 4400/5000: 69.43049644423262\n",
      "Average Total reward: 115.68\n",
      "Running reward of episode 4500/5000: 51.54146219850931\n",
      "Average Total reward: 73.1\n",
      "Running reward of episode 4600/5000: 56.9426027894827\n",
      "Average Total reward: 76.8\n",
      "Running reward of episode 4700/5000: 55.09665020924784\n",
      "Average Total reward: 68.78\n",
      "Running reward of episode 4800/5000: 61.335540957590545\n",
      "Average Total reward: 114.28\n",
      "Running reward of episode 4900/5000: 74.54874903430787\n",
      "Average Total reward: 143.4\n",
      "Running reward of episode 5000/5000: 74.947294868518\n",
      "Average Total reward: 143.86\n",
      "Average length of all episodes: 333.0886\n",
      "Testing the best policy\n",
      "Average Total reward: 148.828\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ekv1ybj9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▂▇██████████████████▃▄▄▆▆██▅▃▂▃▃▂▂▂▂▃</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▂▂▇██████████████████▃▄▄▆▆██▅▃▂▃▃▂▂▂▂▃</td></tr><tr><td>episode_length</td><td>▁▁▁▂█▇███████▇█████████▂▃▄█▆██▃▃▂▃▃▂▂▂▂▃</td></tr><tr><td>loss_policy</td><td>▇▆██▄▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▁▂▂▂▃▃▂▂▂▁▂▂▂▂</td></tr><tr><td>loss_value</td><td>▂▁▃▅███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▃▇▇██▇▇▇▅▅▇▅▂▃▂▅▆</td></tr><tr><td>running_reward</td><td>▁▁▂▂█████████▇█████████▅▆▇████▇▆▅▆▆▄▄▄▆▆</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>143.86</td></tr><tr><td>average_lenght_all_episodes</td><td>333.0886</td></tr><tr><td>average_total_reward</td><td>143.86</td></tr><tr><td>episode_length</td><td>145.9</td></tr><tr><td>loss_policy</td><td>0.51675</td></tr><tr><td>loss_value</td><td>471.48407</td></tr><tr><td>running_reward</td><td>74.94729</td></tr><tr><td>test_average_episode_length</td><td>148.828</td></tr><tr><td>test_average_total_reward</td><td>148.828</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Long Baseline Network </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ekv1ybj9' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/ekv1ybj9</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_200106-ekv1ybj9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ekv1ybj9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_201952-fq8t2ghe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/fq8t2ghe' target=\"_blank\">Long Baseline Network </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/fq8t2ghe' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/fq8t2ghe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/5000: 22.281848419537802\n",
      "Average Total reward: 30.88\n",
      "Running reward of episode 200/5000: 41.25732420820014\n",
      "Average Total reward: 53.4\n",
      "Running reward of episode 300/5000: 69.02338710732116\n",
      "Average Total reward: 169.32\n",
      "Running reward of episode 400/5000: 83.35198132907558\n",
      "Average Total reward: 194.24\n",
      "Running reward of episode 500/5000: 83.6997560802067\n",
      "Average Total reward: 310.32\n",
      "Running reward of episode 600/5000: 72.3976967477512\n",
      "Average Total reward: 103.26\n",
      "Running reward of episode 700/5000: 70.58062011509753\n",
      "Average Total reward: 171.04\n",
      "Running reward of episode 800/5000: 73.07261246170619\n",
      "Average Total reward: 127.74\n",
      "Running reward of episode 900/5000: 70.02051382243351\n",
      "Average Total reward: 154.12\n",
      "Running reward of episode 1000/5000: 90.96148124459195\n",
      "Average Total reward: 303.32\n",
      "Running reward of episode 1100/5000: 74.28175608240997\n",
      "Average Total reward: 140.32\n",
      "Running reward of episode 1200/5000: 97.1970325686927\n",
      "Average Total reward: 499.5\n",
      "Running reward of episode 1300/5000: 98.16427713088015\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/5000: 98.34842868356377\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/5000: 98.29929477024265\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/5000: 97.5481392311015\n",
      "Average Total reward: 314.96\n",
      "Running reward of episode 1700/5000: 94.9976532556428\n",
      "Average Total reward: 299.12\n",
      "Running reward of episode 1800/5000: 93.88051169595965\n",
      "Average Total reward: 269.04\n",
      "Running reward of episode 1900/5000: 91.92387086156353\n",
      "Average Total reward: 219.84\n",
      "Running reward of episode 2000/5000: 98.15508200541183\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2100/5000: 98.34837424355472\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2200/5000: 98.34951863589872\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2300/5000: 98.34952541130703\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2400/5000: 94.12476034140516\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2500/5000: 94.67068860665196\n",
      "Average Total reward: 245.98\n",
      "Running reward of episode 2600/5000: 98.05321017048504\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2700/5000: 97.78952834415506\n",
      "Average Total reward: 496.12\n",
      "Running reward of episode 2800/5000: 98.34620997242168\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2900/5000: 98.21188650562266\n",
      "Average Total reward: 454.76\n",
      "Running reward of episode 3000/5000: 98.34818786347554\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3100/5000: 98.34694688086289\n",
      "Average Total reward: 499.84\n",
      "Running reward of episode 3200/5000: 90.36020349776815\n",
      "Average Total reward: 213.06\n",
      "Running reward of episode 3300/5000: 95.53675281370151\n",
      "Average Total reward: 419.5\n",
      "Running reward of episode 3400/5000: 96.25595453716797\n",
      "Average Total reward: 310.92\n",
      "Running reward of episode 3500/5000: 98.13752675589816\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3600/5000: 98.34827030718702\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3700/5000: 98.34153880315226\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3800/5000: 98.34947816647409\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3900/5000: 98.34952517170662\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4000/5000: 98.33673259372112\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4100/5000: 98.34944971117072\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4200/5000: 98.34952500323615\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4300/5000: 98.33168390314638\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4400/5000: 98.31464908122263\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4500/5000: 93.3580358434808\n",
      "Average Total reward: 209.74\n",
      "Running reward of episode 4600/5000: 96.83423826379067\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4700/5000: 98.34055414958698\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4800/5000: 83.33932786259412\n",
      "Average Total reward: 164.86\n",
      "Running reward of episode 4900/5000: 66.37596837576675\n",
      "Average Total reward: 140.1\n",
      "Running reward of episode 5000/5000: 87.41060361375463\n",
      "Average Total reward: 260.12\n",
      "Average length of all episodes: 372.0146\n",
      "Testing the best policy\n",
      "Average Total reward: 296.92\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fq8t2ghe) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▃▃▂▃▂▃▃███▅▅▅▄███████▇█▄▇▅██████████▃▄</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▃▃▂▃▂▃▃███▅▅▅▄███████▇█▄▇▅██████████▃▄</td></tr><tr><td>episode_length</td><td>▁▁▂▄▅▂▃▃▃▆███▆▄▄█████████▄█▅███████▆██▂▃</td></tr><tr><td>loss_policy</td><td>▅▇█▇▄▃▂▃▃▂▂▂▂▂▃▃▂▂▂▂▁▂▂▂▂▃▁▂▂▂▂▂▂▂▂▃▂▂▂▂</td></tr><tr><td>loss_value</td><td>▁▄▇█▅▃▄▅▅▆▅▅▅▆▆▅▅▅▅▅▅▅▅▅▅▅▅▆▅▅▅▅▅▅▅▆▅▅▃▅</td></tr><tr><td>running_reward</td><td>▁▂▄▇▆▅▇▅▇▇█████▇█████████▇████████████▄▇</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>260.12</td></tr><tr><td>average_lenght_all_episodes</td><td>372.0146</td></tr><tr><td>average_total_reward</td><td>260.12</td></tr><tr><td>episode_length</td><td>241.3</td></tr><tr><td>loss_policy</td><td>-0.5874</td></tr><tr><td>loss_value</td><td>625.31879</td></tr><tr><td>running_reward</td><td>87.4106</td></tr><tr><td>test_average_episode_length</td><td>296.92</td></tr><tr><td>test_average_total_reward</td><td>296.92</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Long Baseline Network </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/fq8t2ghe' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/fq8t2ghe</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_201952-fq8t2ghe/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fq8t2ghe). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_204103-id0tebve</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/id0tebve' target=\"_blank\">Long Baseline Network </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/id0tebve' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/id0tebve</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/5000: 17.44650506993677\n",
      "Average Total reward: 19.88\n",
      "Running reward of episode 200/5000: 31.832088364179725\n",
      "Average Total reward: 35.78\n",
      "Running reward of episode 300/5000: 49.58172597979848\n",
      "Average Total reward: 77.24\n",
      "Running reward of episode 400/5000: 86.35361267541263\n",
      "Average Total reward: 318.74\n",
      "Running reward of episode 500/5000: 90.25031077442189\n",
      "Average Total reward: 419.4\n",
      "Running reward of episode 600/5000: 75.66654488330619\n",
      "Average Total reward: 169.3\n",
      "Running reward of episode 700/5000: 86.40439981796591\n",
      "Average Total reward: 173.6\n",
      "Running reward of episode 800/5000: 78.92960674017783\n",
      "Average Total reward: 156.38\n",
      "Running reward of episode 900/5000: 90.62637731209816\n",
      "Average Total reward: 366.28\n",
      "Running reward of episode 1000/5000: 98.27393842759136\n",
      "Average Total reward: 499.92\n",
      "Running reward of episode 1100/5000: 98.29516696862272\n",
      "Average Total reward: 498.1\n",
      "Running reward of episode 1200/5000: 98.34902815379931\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1300/5000: 98.34952250739346\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/5000: 98.34952543422833\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1500/5000: 97.2248442115441\n",
      "Average Total reward: 379.66\n",
      "Running reward of episode 1600/5000: 93.19862537540637\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/5000: 98.31902939724743\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/5000: 98.34934489887868\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/5000: 98.34952438269192\n",
      "Average Total reward: 499.94\n",
      "Running reward of episode 2000/5000: 97.15218590939992\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2100/5000: 98.31358389950259\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2200/5000: 98.02259108577421\n",
      "Average Total reward: 478.5\n",
      "Running reward of episode 2300/5000: 97.62873712538938\n",
      "Average Total reward: 460.24\n",
      "Running reward of episode 2400/5000: 98.34451177276158\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2500/5000: 98.34949576802755\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2600/5000: 98.34952527591713\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2700/5000: 98.34952545061945\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2800/5000: 98.34952545165379\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2900/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3000/5000: 98.34952545165982\n",
      "Average Total reward: 490.1\n",
      "Running reward of episode 3100/5000: 97.71263261270265\n",
      "Average Total reward: 484.84\n",
      "Running reward of episode 3200/5000: 98.33955818864078\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3300/5000: 98.349466440188\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3400/5000: 98.3495251022808\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3500/5000: 98.34952544959144\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3600/5000: 98.34952545164771\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3700/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3800/5000: 81.71428801589016\n",
      "Average Total reward: 263.14\n",
      "Running reward of episode 3900/5000: 85.67552331387458\n",
      "Average Total reward: 393.38\n",
      "Running reward of episode 4000/5000: 97.30404255055925\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4100/5000: 98.19920450626934\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4200/5000: 93.48749932893419\n",
      "Average Total reward: 447.28\n",
      "Running reward of episode 4300/5000: 98.26482545719114\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4400/5000: 98.34902398286772\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4500/5000: 98.3495224826993\n",
      "Average Total reward: 490.52\n",
      "Running reward of episode 4600/5000: 97.08203255061724\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4700/5000: 98.2508147058956\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4800/5000: 83.80402498217622\n",
      "Average Total reward: 214.94\n",
      "Running reward of episode 4900/5000: 97.74542218063289\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 5000/5000: 98.34594884059175\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 423.678\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:id0tebve) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▁▂▅▃▃▃▆██████████▇███████████▅▆█▇████▄█</td></tr><tr><td>average_lenght_all_episodes</td><td>▁</td></tr><tr><td>average_total_reward</td><td>▁▁▂▅▃▃▃▆██████████▇███████████▅▆█▇████▄█</td></tr><tr><td>episode_length</td><td>▁▁▂▅█▃▃█▆███▄███████████▇████████▇██████</td></tr><tr><td>loss_policy</td><td>▆██▇▃▆▃▁▃▃▃▃▃▃▃▃▂▃▂▃▃▃▃▃▃▃▃▃▃▃▃▂▃▃▃▃▃▃▂▃</td></tr><tr><td>loss_value</td><td>▁▂▅█▅▆▅▆▆▅▅▅▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>running_reward</td><td>▁▁▃▆▇▆▆▇████████████████████████████████</td></tr><tr><td>test_average_episode_length</td><td>▁</td></tr><tr><td>test_average_total_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>500.0</td></tr><tr><td>average_lenght_all_episodes</td><td>423.678</td></tr><tr><td>average_total_reward</td><td>500.0</td></tr><tr><td>episode_length</td><td>500.0</td></tr><tr><td>loss_policy</td><td>0.21494</td></tr><tr><td>loss_value</td><td>598.36804</td></tr><tr><td>running_reward</td><td>98.34595</td></tr><tr><td>test_average_episode_length</td><td>500.0</td></tr><tr><td>test_average_total_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Long Baseline Network </strong> at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/id0tebve' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/id0tebve</a><br/> View project at: <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_204103-id0tebve/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:id0tebve). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Marco/JupyterNotebooks/wandb/run-20240510_210546-8jcr6gf2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8jcr6gf2' target=\"_blank\">Long Baseline Network </a></strong> to <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8jcr6gf2' target=\"_blank\">https://wandb.ai/marcouni/Lab3-DRL-warmups/runs/8jcr6gf2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n",
      "Running reward of episode 100/5000: 21.38267050170289\n",
      "Average Total reward: 25.68\n",
      "Running reward of episode 200/5000: 32.62097073353295\n",
      "Average Total reward: 49.18\n",
      "Running reward of episode 300/5000: 57.47753396836479\n",
      "Average Total reward: 129.76\n",
      "Running reward of episode 400/5000: 80.79634629702414\n",
      "Average Total reward: 298.72\n",
      "Running reward of episode 500/5000: 91.93705105788192\n",
      "Average Total reward: 422.34\n",
      "Running reward of episode 600/5000: 95.66825502449966\n",
      "Average Total reward: 433.68\n",
      "Running reward of episode 700/5000: 97.71020506950825\n",
      "Average Total reward: 463.3\n",
      "Running reward of episode 800/5000: 98.04008323667588\n",
      "Average Total reward: 424.3\n",
      "Running reward of episode 900/5000: 90.69683889750817\n",
      "Average Total reward: 490.7\n",
      "Running reward of episode 1000/5000: 98.2942283719292\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1100/5000: 98.32808925030336\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1200/5000: 98.34939853800343\n",
      "Average Total reward: 496.46\n",
      "Running reward of episode 1300/5000: 98.34952470026394\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1400/5000: 98.34952544721125\n",
      "Average Total reward: 494.48\n",
      "Running reward of episode 1500/5000: 98.18177739741844\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1600/5000: 98.34853229440317\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1700/5000: 98.34951957164337\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1800/5000: 98.34952541684713\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 1900/5000: 98.34952545145384\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2000/5000: 98.34837752000301\n",
      "Average Total reward: 498.06\n",
      "Running reward of episode 2100/5000: 98.28896937165646\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2200/5000: 97.60588417697674\n",
      "Average Total reward: 491.88\n",
      "Running reward of episode 2300/5000: 98.34399078918324\n",
      "Average Total reward: 478.96\n",
      "Running reward of episode 2400/5000: 98.34808989856242\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2500/5000: 98.3495169524259\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2600/5000: 98.34952540133997\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2700/5000: 98.34952545136204\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2800/5000: 98.34952545165818\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 2900/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3000/5000: 98.34952545165982\n",
      "Average Total reward: 485.26\n",
      "Running reward of episode 3100/5000: 97.6983149912146\n",
      "Average Total reward: 475.62\n",
      "Running reward of episode 3200/5000: 86.00670684530517\n",
      "Average Total reward: 161.72\n",
      "Running reward of episode 3300/5000: 98.18501155869808\n",
      "Average Total reward: 480.9\n",
      "Running reward of episode 3400/5000: 98.3473177536894\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3500/5000: 98.34951238091959\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3600/5000: 98.34952537427425\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3700/5000: 98.34952545120177\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3800/5000: 98.34952545165726\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 3900/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4000/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4100/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4200/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4300/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4400/5000: 98.06323962582994\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4500/5000: 98.34783048806274\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4600/5000: 98.34951541657846\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4700/5000: 98.34952539224695\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4800/5000: 98.3495254513082\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 4900/5000: 98.34952545165787\n",
      "Average Total reward: 500.0\n",
      "Running reward of episode 5000/5000: 98.34952545165982\n",
      "Average Total reward: 500.0\n",
      "Average length of all episodes: 445.7678\n",
      "Testing the best policy\n",
      "Average Total reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "#random seeds, to reproduce the same results\n",
    "seeds = [1, 11, 111, 1111, 11111]\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    # Training and arhitecture hyperparameters, initialise a wandb run\n",
    "    run=wandb.init(\n",
    "          project=\"Lab3-DRL-warmups\",\n",
    "          name = \"Long Baseline Network \",\n",
    "          config={\n",
    "              \"hidden_layers\": 32,\n",
    "              \"num_episodes\": 5000,\n",
    "              \"gamma\": 0.99,\n",
    "              \"baseline\": 'net',\n",
    "              \"eval_every\":100,\n",
    "              \"eval_episodes\": 50,\n",
    "              \"test_episodes\" : 500,\n",
    "              \"temperature\" : 5,\n",
    "              \"lr\" : 1e-2,\n",
    "              \"lr_baseline\" : 1e-3\n",
    "              })\n",
    "    \n",
    "    # Copy the configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    #Instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "    # down), and another that does not animate.\n",
    "    env = gymnasium.make('CartPole-v1')\n",
    "    env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    #set the seed\n",
    "    torch.manual_seed(seeds[i])\n",
    "    env.reset(seed = seeds[i])\n",
    "    env_render.reset(seed = seeds[i])\n",
    "    \n",
    "    # Make a policy network.\n",
    "    policy = PolicyNet(env, config.hidden_layers, config.temperature)\n",
    "\n",
    "    # Make a value network\n",
    "    value = ValueNet(env, config.hidden_layers)\n",
    "    \n",
    "    # Create episode_runner\n",
    "    episode_runner= Episode_runner(env, policy)\n",
    "    episode_runner_rend= Episode_runner(env_render, policy)\n",
    "    \n",
    "    # Train the agent\n",
    "    best_model_state_dict = reinforce(episode_runner, run, episode_runner_rend, gamma=config.gamma, num_episodes=config.num_episodes,\n",
    "              baseline= value, display=False, eval_every=config.eval_every,\n",
    "              eval_episodes=config.eval_episodes, lr= config.lr, lr_baseline = config.lr_baseline )\n",
    "    \n",
    "    # Load the best policy on the determinist episode runner to test it\n",
    "    episode_runner.policy.load_state_dict(best_model_state_dict)\n",
    "    det_ep_runner = Determinist_Test_Episode_runner(episode_runner, episode_runner_rend )\n",
    "    det_ep_runner.test(test_episodes=config.test_episodes)\n",
    "    \n",
    "    # Close up everything\n",
    "    env_render.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b3194",
   "metadata": {},
   "source": [
    "## Video recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0459791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "env_render = gymnasium.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "# wrap the env in the record video\n",
    "recorder = gymnasium.wrappers.RecordVideo(env=env_render, video_folder=\"/data01/dl24marchi/DLAProjects/videos\", name_prefix=\"test-video\", episode_trigger=lambda x: x%10 == 0)\n",
    "\n",
    "# env reset for a fresh start\n",
    "observation, info = recorder.reset()\n",
    "\n",
    "###\n",
    "# Start the recorder\n",
    "recorder.start_video_recorder()\n",
    "\n",
    "# And run the final agent for a few episodes.\n",
    "for _ in range(100):\n",
    "    run_episode(recorder, policy)\n",
    "\n",
    "####\n",
    "# Don't forget to close the video recorder before the env!\n",
    "recorder.close_video_recorder()\n",
    "\n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf1447-d222-4b24-a357-5b7f9824390c",
   "metadata": {
    "id": "64bf1447-d222-4b24-a357-5b7f9824390c"
   },
   "source": [
    "-----\n",
    "## Exercise 3: Going Deeper\n",
    "\n",
    "As usual, pick **AT LEAST ONE** of the following exercises to complete.\n",
    "\n",
    "### Exercise 3.1: Solving Lunar Lander with `REINFORCE` (easy)\n",
    "\n",
    "Use my (or even better, improve on my) implementation of `REINFORCE` to solve the [Lunar Lander Environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/). This environment is a little bit harder than Cartpole, but not much. Make sure you perform the same types of analyses we did during the lab session to quantify and qualify the performance of your agents.\n",
    "\n",
    "### Exercise 3.2: Solving Cartpole and Lunar Lander with `Deep Q-Learning` (harder)\n",
    "\n",
    "On policy Deep Reinforcement Learning tends to be **very unstable**. Write an implementation (or adapt an existing one) of `Deep Q-Learning` to solve our two environments (Cartpole and Lunar Lander). To do this you will need to implement a **Replay Buffer** and use a second, slow-moving **target Q-Network** to stabilize learning.\n",
    "\n",
    "### Exercise 3.3: Solving the OpenAI CarRacing environment (hardest)\n",
    "\n",
    "Use `Deep Q-Learning` -- or even better, an off-the-shelf implementation of **Proximal Policy Optimization (PPO)** -- to train an agent to solve the [OpenAI CarRacing](https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN) environment. This will be the most *fun*, but also the most *difficult*. Some tips:\n",
    "\n",
    "1. Make sure you use the `continuous=False` argument to the environment constructor. This ensures that the action space is **discrete** (we haven't seen how to work with continuous action spaces).\n",
    "2. Your Q-Network will need to be a CNN. A simple one should do, with two convolutional + maxpool layers, folowed by a two dense layers. You will **definitely** want to use a GPU to train your agents.\n",
    "3. The observation space of the environment is a single **color image** (a single frame of the game). Most implementations stack multiple frames (e.g. 3) after converting them to grayscale images as an observation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab3562d-9e4a-4ac1-9667-a9ea36412a24",
   "metadata": {
    "id": "6ab3562d-9e4a-4ac1-9667-a9ea36412a24"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
